{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context-aware text analysis\n",
    "\n",
    "Or, feature engineering beyond the token level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "from itertools import groupby\n",
    "from math import log\n",
    "from nltk import ne_chunk\n",
    "from nltk.chunk import tree2conlltags\n",
    "from nltk.chunk.regexp import RegexpParser\n",
    "from nltk.collocations import QuadgramCollocationFinder\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.metrics.association import QuadgramAssocMeasures\n",
    "from nltk.probability import ProbDistI, FreqDist, ConditionalFreqDist\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.util import ngrams\n",
    "from operator import itemgetter\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from unicodedata import category as unicat\n",
    "\n",
    "# Where are the corpus texts on your system\n",
    "text_dir = os.path.join('..', 'data', 'texts')\n",
    "pickle_dir = os.path.join('..', 'data', 'pickled')\n",
    "\n",
    "# Import our libraries\n",
    "sys.path.append(os.path.join('..', 'libraries'))\n",
    "from TMN import TMNCorpusReader, PickledCorpusReader\n",
    "\n",
    "# Set up corpus\n",
    "corpus = PickledCorpusReader(pickle_dir)\n",
    "txtcorpus = TMNCorpusReader(text_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `lambda`, `filter`, and `map`\n",
    "\n",
    "`lambda` functions are short pieces of code, usually defined inline, that are run over some input data. They're also known as anonymous functions, because they don't have a name to which you can refer out of the context of their execution. They're a convenience that saves you from needing to write a full-blown function to accomplish some simple, one-off task. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(13, -3), (4, 1), (1, 2), (9, 10)]\n"
     ]
    }
   ],
   "source": [
    "# Sort a list of tuples by the second element of each tuple\n",
    "a = [(1, 2), (4, 1), (9, 10), (13, -3)]\n",
    "a.sort(key=lambda x: x[1])\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`filter` selects those elements from a list that meet a defined (usually via a lambda function) criterion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-5, -4, -3, -2, -1]\n"
     ]
    }
   ],
   "source": [
    "number_list = range(-5, 5)\n",
    "less_than_zero = list(filter(lambda x: x < 0, number_list))\n",
    "print(less_than_zero)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`map` appplies a function (often a lambda function) to each element of a list: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 4, 9, 16, 25]\n"
     ]
    }
   ],
   "source": [
    "items = [1, 2, 3, 4, 5]\n",
    "squared = list(map(lambda x: x**2, items))\n",
    "print(squared)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keyphrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "GRAMMAR = r'KT: {(<JJ>* <NN.*>+ <IN>)? <JJ>* <NN.*>+}'\n",
    "#GOODTAGS = frozenset(['JJ','JJR','JJS','NN','NNP','NNS','NNPS'])\n",
    "\n",
    "class KeyphraseExtractor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Wraps a PickledCorpusReader consisting of pos-tagged documents.\n",
    "    \"\"\"\n",
    "    def __init__(self, grammar=GRAMMAR):\n",
    "        self.grammar = GRAMMAR\n",
    "        self.chunker = RegexpParser(self.grammar)\n",
    "\n",
    "    def normalize(self, sent):\n",
    "        \"\"\"\n",
    "        Removes punctuation from a tokenized/tagged sentence and\n",
    "        lowercases words.\n",
    "        \"\"\"\n",
    "        is_punct = lambda word: all(unicat(char).startswith('P') for char in word)\n",
    "        sent = filter(lambda t: not is_punct(t[0]), sent)\n",
    "        sent = map(lambda t: (t[0].lower(), t[1]), sent)\n",
    "        return list(sent)\n",
    "\n",
    "    def extract_keyphrases(self, document):\n",
    "        \"\"\"\n",
    "        For a document, parse sentences using our chunker created by\n",
    "        our grammar, converting the parse tree into a tagged sequence.\n",
    "        Yields extracted phrases.\n",
    "        \"\"\"\n",
    "        for sents in document:\n",
    "            for sent in sents:\n",
    "                sent = self.normalize(sent)\n",
    "                if not sent: continue\n",
    "                chunks = tree2conlltags(self.chunker.parse(sent))\n",
    "                phrases = [\n",
    "                    \" \".join(word for word, pos, chunk in group).lower()\n",
    "                    for key, group in groupby(\n",
    "                        chunks, lambda term: term[-1] != 'O'\n",
    "                    ) if key\n",
    "                ]\n",
    "                for phrase in phrases:\n",
    "                    yield phrase\n",
    "\n",
    "    def fit(self, documents, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, documents):\n",
    "        for document in documents:\n",
    "            yield list(self.extract_keyphrases(document))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 41.5 s, sys: 401 ms, total: 41.9 s\n",
      "Wall time: 42.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "docs = corpus.docs()\n",
    "\n",
    "phrase_extractor = KeyphraseExtractor()\n",
    "keyphrases = list(phrase_extractor.fit_transform(docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that it takes c. 45 seconds to process the full corpus (40 vols, 6.5M words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A-Alcott-Little_Women-1868-F.pickle : ['s fair', 'girls', 'plenty of pretty things']\n",
      "A-Cather-Antonia-1918-F.pickle : ['miles of ripe wheat', 'country towns', 'bright flowered pastures']\n",
      "A-Chesnutt-Marrow-1901-M.pickle : ['chamber', 'muslin', 'breath of air']\n",
      "A-Chopin-Awakening-1899-F.pickle : ['other side', 'door', 'fluty']\n",
      "A-Crane-Maggie-1893-M.pickle : ['small body', 'delivery of great crimson oaths', 'run jimmie run']\n",
      "A-Davis-Life_Iron_Mills-1861-F.pickle : ['breath of crowded human beings', 'window', 'rain']\n",
      "A-Dreiser-Sister_Carrie-1900-M.pickle : ['yellow leather snap purse', 'ticket', 'scrap of paper']\n",
      "A-Freeman-Pembroke-1894-F.pickle : ['barnabas', 'bedroom', 'kitchen']\n",
      "A-Gilman-Herland-1915-F.pickle : ['bird', 'eyes', 'cities']\n",
      "A-Harper-Iola_Leroy-1892-F.pickle : ['condition', 'market', 'robert johnson']\n",
      "A-Hawthorne-Scarlet_Letter-1850-M.pickle : ['first time', 'years', 'reader']\n",
      "A-Howells-Silas_Lapham-1885-M.pickle : ['door', 'room', 'desk']\n",
      "A-James-Golden_Bowl-1904-M.pickle : ['tiber', 'brought', 'legend']\n",
      "A-Jewett-Pointed_Firs-1896-F.pickle : ['ledges', 'landing', 'houses']\n",
      "A-London-Call_Wild-1903-M.pickle : ['muscle', 'warm long hair from puget sound', 'san diego']\n",
      "A-Melville-Moby_Dick-1851-M.pickle : ['world', 'old grammars', 'mortality']\n",
      "A-Norris-Pit-1903-M.pickle : ['women', 'dress', 'vestibule']\n",
      "A-Stowe-Uncle_Tom-1852-F.pickle : ['parlor', 'town of p', 'kentucky']\n",
      "A-Twain-Huck_Finn-1885-M.pickle : ['things', 'truth', 'nothing']\n",
      "A-Wharton-Age_Innocence-1920-F.pickle : ['costliness', 'splendour', 'great european capitals']\n",
      "B-Austen-Pride_Prejudice-1813-F.pickle : ['truth', 'minds', 'families']\n",
      "B-Bronte_C-Jane_Eyre-1847-F.pickle : ['door exercise', 'question', 'glad']\n",
      "B-Bronte_E-Wuthering_Heights-1847-F.pickle : ['suitable pair', 'desolation', 'capital fellow']\n",
      "B-Burney-Evelina-1778-F.pickle : ['letter from madame duval', 'loss', 'manner']\n",
      "B-Conrad-Heart_Darkness-1902-M.pickle : ['turn', 'tide', 'sea reach']\n",
      "B-Dickens-Bleak_House-1853-M.pickle : ['face', 'earth', 'megalosaurus forty feet']\n",
      "B-Disraeli-Sybil-1845-M.pickle : ['grey', 'member', 'jockey club']\n",
      "B-Eliot-Middlemarch-1869-F.pickle : ['hand', 'wrist', 'sleeves']\n",
      "B-Forster-Room_View-1908-M.pickle : ['long way', 'lucy', 'cockney']\n",
      "B-Gaskell-North_South-1855-F.pickle : ['room in harley street', 'white muslin', 'blue ribbons']\n",
      "B-Gissing-Grub_Street-1893-M.pickle : ['egg', 'cheerfulness', 'man']\n",
      "B-Hardy-Tess-1891-M.pickle : ['rickety', 'bias', 'gait']\n",
      "B-Mitford-Our_Village-1826-F.pickle : ['friend of mine calls', 'nondescript dwellings with inhabitants', 'flowers']\n",
      "B-Radcliffe-Mysteries_Udolpho-1794-F.pickle : ['garonne', 'province of gascony', 'year']\n",
      "B-Shelley-Frankenstein-1818-F.pickle : ['confidence', 'success', 'undertaking']\n",
      "B-Stevenson-Treasure_Island-1883-M.pickle : ['nothing', 'bearings', 'island']\n",
      "B-Thackeray-Vanity_Fair-1848-M.pickle : ['fat coachman', 'wig', 'rate']\n",
      "B-Trollope-Live_Now-1875-M.pickle : ['lady carbury', 'many hours', 'desk']\n",
      "B-Wells-Time_Machine-1895-M.pickle : ['chairs', 'patents', 'dinner']\n",
      "B-Woolf-Mrs_Dalloway-1925-F.pickle : ['lark', 'plunge', 'little squeak']\n"
     ]
    }
   ],
   "source": [
    "# Print keyphrases 10-12 in each book\n",
    "for i in range(len(keyphrases)):\n",
    "    print(corpus.fileids()[i], ':', keyphrases[i][10:13])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "GOODLABELS = frozenset(['PERSON', 'ORGANIZATION', 'FACILITY', 'GPE', 'GSP'])\n",
    "\n",
    "class EntityExtractor(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, labels=GOODLABELS, **kwargs):\n",
    "        self.labels = labels\n",
    "\n",
    "    def get_entities(self, document):\n",
    "        entities = []\n",
    "        for paragraph in document:\n",
    "            for sentence in paragraph:\n",
    "                trees = ne_chunk(sentence)\n",
    "                for tree in trees:\n",
    "                    if hasattr(tree, 'label'):\n",
    "                        if tree.label() in self.labels:\n",
    "                            entities.append(\n",
    "                                ' '.join([child[0].lower() for child in tree])\n",
    "                                )\n",
    "        return entities\n",
    "\n",
    "    def fit(self, documents, labels=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, documents):\n",
    "        for document in documents:\n",
    "            yield self.get_entities(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['john', 'virginia', 'wolfes', 'kirby', 'john', 'deborah', 'wolfes', 'welsh', 'cornish', 'cornish', 'wolfes', 'christ', 'good', 'deb', 'dah']\n",
      "CPU times: user 4.24 s, sys: 33.6 ms, total: 4.27 s\n",
      "Wall time: 4.29 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "davis = corpus.docs(fileids=['A-Davis-Life_Iron_Mills-1861-F.pickle'])\n",
    "\n",
    "entity_extractor = EntityExtractor()\n",
    "entities = list(entity_extractor.fit_transform(davis))\n",
    "print(entities[0][10:25])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NER is (much) slower than keyphrase extraction: about 30 seconds per novel-length document. NLTK is slower than other NER packages (e.g., SpaCy, Stanford CRF-NER (which is written in Java), etc.), but has the advantage of being nicely tied into Python language processing.\n",
    "\n",
    "### SpaCy\n",
    "\n",
    "Let's try the same thing with SpaCy (note that you may need to [install SpaCy](https://spacy.io/usage#installation) on your system)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SpaCy imports and setup\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "import en_core_web_sm\n",
    "from pprint import pprint\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.85 s, sys: 862 ms, total: 5.71 s\n",
      "Wall time: 3.12 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with open(os.path.join('..', 'data', 'texts', 'A-Davis-Life_Iron_Mills-1861-F.txt')) as f:\n",
    "    doc = nlp(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we just ran an **entire** NLP pipeline -- tokenization, PoS tagging, NER, dependancy parsing, etc. -- on a full novel from raw text in 3 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('thousands', 'CARDINAL'),\n",
      " ('thousands', 'CARDINAL'),\n",
      " ('centuries', 'DATE'),\n",
      " ('Egoist', 'ORG'),\n",
      " ('Pantheist', 'ORG'),\n",
      " ('Arminian', 'NORP'),\n",
      " ('Society', 'ORG'),\n",
      " ('the day', 'DATE'),\n",
      " ('one', 'CARDINAL'),\n",
      " ('one', 'CARDINAL'),\n",
      " (\"Kirby & John's\", 'ORG'),\n",
      " ('Hugh Wolfe', 'PERSON'),\n",
      " ('Virginia', 'GPE'),\n",
      " ('last winter', 'DATE'),\n",
      " ('about a thousand', 'CARDINAL')]\n"
     ]
    }
   ],
   "source": [
    "# Examine the first ten entities\n",
    "pprint([(X.text, X.label_) for X in doc.ents][10:25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('PERSON', 166),\n",
       " ('CARDINAL', 83),\n",
       " ('DATE', 71),\n",
       " ('GPE', 55),\n",
       " ('TIME', 48),\n",
       " ('ORG', 41),\n",
       " ('NORP', 17),\n",
       " ('ORDINAL', 8),\n",
       " ('WORK_OF_ART', 6),\n",
       " ('LOC', 5),\n",
       " ('PRODUCT', 3),\n",
       " ('QUANTITY', 2),\n",
       " ('FAC', 1),\n",
       " ('LANGUAGE', 1)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count the entities by type\n",
    "labels = [x.label_ for x in doc.ents]\n",
    "Counter(labels).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Deborah', 31),\n",
       " ('Mitchell', 26),\n",
       " ('Haley', 11),\n",
       " ('Kirby', 10),\n",
       " ('Hugh', 8),\n",
       " ('Wolfe', 7),\n",
       " ('Janey', 6),\n",
       " ('Hur', 6),\n",
       " ('May', 6),\n",
       " ('Joe', 6)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Most common PERSONs\n",
    "people = [x.text for x in doc.ents if x.label_=='PERSON']\n",
    "Counter(people).most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"entities\" style=\"line-height: 2.5\">I open the window, and, looking out, can scarcely see through the rain the grocer's shop opposite, where a crowd of drunken \n",
       "<mark class=\"entity\" style=\"background: #c887fb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Irishmen\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n",
       "</mark>\n",
       " are puffing \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Lynchburg\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " tobacco in their pipes.</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sentences = [x for x in doc.sents]\n",
    "displacy.render(nlp(str(sentences[7])), jupyter=True, style='ent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quadgram collocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_quadgrams(corpus, metric, path=None, fileids=corpus.fileids()):\n",
    "    \"\"\"\n",
    "    Find and rank quadgrams from the supplied corpus using the given\n",
    "    association metric. Write the quadgrams out to the given path if\n",
    "    supplied otherwise return the list in memory.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a collocation ranking utility from corpus words.\n",
    "    ngrams = QuadgramCollocationFinder.from_words(corpus.words(fileids=fileids))\n",
    "\n",
    "    # Rank collocations by an association metric\n",
    "    scored = ngrams.score_ngrams(metric)\n",
    "\n",
    "    if path:\n",
    "        with open(path, 'w') as f:\n",
    "            f.write(\"Collocation\\tScore ({})\\n\".format(metric.__name__))\n",
    "            for ngram, score in scored:\n",
    "                f.write(\"{}\\t{}\\n\".format(repr(ngram), score))\n",
    "    else:\n",
    "        return scored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 32.8 s, sys: 96.2 ms, total: 32.9 s\n",
      "Wall time: 32.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "scored = rank_quadgrams(\n",
    "    corpus, \n",
    "    QuadgramAssocMeasures.likelihood_ratio, \n",
    "    path=None, \n",
    "    fileids=['A-Twain-Huck_Finn-1885-M.pickle']\n",
    ")\n",
    "\n",
    "# Group quadgrams by first word\n",
    "prefixed = defaultdict(list)\n",
    "for key, score in scored:\n",
    "    prefixed[key[0]].append((key[1:], score))\n",
    "\n",
    "# Sort keyed quadgrams by strongest association\n",
    "for key in prefixed:\n",
    "    prefixed[key].sort(key=itemgetter(1), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('it', ',', 'and'), 20739.22200780236),\n",
       " ((',', 'and', 'they'), 20492.186387744914),\n",
       " ((',', 'and', 'you'), 20358.665902171422),\n",
       " (('nothing', ',', 'and'), 20358.64509694963),\n",
       " ((',', 'and', 'was'), 20300.004342382286),\n",
       " ((',', 'and', 'nobody'), 20257.994151180632),\n",
       " (('no', ',', 'and'), 20252.76265237206),\n",
       " ((',', 'and', 'cleared'), 20242.01759408574),\n",
       " ((',', 'and', 'that'), 20224.455755495605),\n",
       " (('howdy', ',', 'and'), 20199.952904882135)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prefixed['said'][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language models\n",
    "\n",
    "The idea in this case is that we can use observed probabilities of token collocation to predict what will come next in a sequence of tokens.\n",
    "\n",
    "Not going to do much with this, since it's outside the scope of the class. But including the textbook code here in case it's of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_ngrams(n, vocabulary, texts):\n",
    "    counter = NgramCounter(n, vocabulary)\n",
    "    counter.train_counts(texts)\n",
    "    return counter\n",
    "\n",
    "\n",
    "class NgramCounter(object):\n",
    "    \"\"\"\n",
    "    The NgramCounter class counts ngrams given a vocabulary and ngram size.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n, vocabulary, unknown=\"<UNK>\"):\n",
    "        \"\"\"\n",
    "        n is the size of the ngram\n",
    "        \"\"\"\n",
    "        if n < 1:\n",
    "            raise ValueError(\"ngram size must be greater than or equal to 1\")\n",
    "\n",
    "        self.n = n\n",
    "        self.unknown = unknown\n",
    "        self.padding = {\n",
    "            \"pad_left\": True,\n",
    "            \"pad_right\": True,\n",
    "            \"left_pad_symbol\": \"<s>\",\n",
    "            \"right_pad_symbol\": \"</s>\"\n",
    "        }\n",
    "\n",
    "        self.vocabulary = vocabulary\n",
    "        self.allgrams = defaultdict(ConditionalFreqDist)\n",
    "        self.ngrams = FreqDist()\n",
    "        self.unigrams = FreqDist()\n",
    "\n",
    "    def train_counts(self, training_text):\n",
    "        for sent in training_text:\n",
    "            checked_sent = (self.check_against_vocab(word) for word in sent)\n",
    "            sent_start = True\n",
    "            for ngram in self.to_ngrams(checked_sent):\n",
    "                self.ngrams[ngram] += 1\n",
    "                context, word = tuple(ngram[:-1]), ngram[-1]\n",
    "                if sent_start:\n",
    "                    for context_word in context:\n",
    "                        self.unigrams[context_word] += 1\n",
    "                    sent_start = False\n",
    "\n",
    "                for window, ngram_order in enumerate(range(self.n, 1, -1)):\n",
    "                    context = context[window:]\n",
    "                    self.allgrams[ngram_order][context][word] += 1\n",
    "                self.unigrams[word] += 1\n",
    "\n",
    "    def check_against_vocab(self, word):\n",
    "        if word in self.vocabulary:\n",
    "            return word\n",
    "        return self.unknown\n",
    "\n",
    "    def to_ngrams(self, sequence):\n",
    "        \"\"\"\n",
    "        Wrapper for NLTK ngrams method\n",
    "        \"\"\"\n",
    "        return ngrams(sequence, self.n, **self.padding)\n",
    "\n",
    "\n",
    "class BaseNgramModel(object):\n",
    "    \"\"\"\n",
    "    The BaseNgramModel creates an n-gram language model.\n",
    "    This base model is equivalent to a Maximum Likelihood Estimation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, ngram_counter):\n",
    "        \"\"\"\n",
    "        BaseNgramModel is initialized with an NgramCounter.\n",
    "        \"\"\"\n",
    "        self.n = ngram_counter.n\n",
    "        self.ngram_counter = ngram_counter\n",
    "        self.ngrams = ngram_counter.ngrams\n",
    "        self._check_against_vocab = self.ngram_counter.check_against_vocab\n",
    "\n",
    "    def check_context(self, context):\n",
    "        \"\"\"\n",
    "        Ensures that the context is not longer than or equal to the model's\n",
    "        n-gram order.\n",
    "\n",
    "        Returns the context as a tuple.\n",
    "        \"\"\"\n",
    "        if len(context) >= self.n:\n",
    "            raise ValueError(\"Context too long for this n-gram\")\n",
    "\n",
    "        return tuple(context)\n",
    "\n",
    "    def score(self, word, context):\n",
    "        \"\"\"\n",
    "        For a given string representation of a word, and a string word context,\n",
    "        returns the maximum likelihood score that the word will follow the\n",
    "        context.\n",
    "        \"\"\"\n",
    "        context = self.check_context(context)\n",
    "\n",
    "        return self.ngrams[context].freq(word)\n",
    "\n",
    "    def logscore(self, word, context):\n",
    "        \"\"\"\n",
    "        For a given string representation of a word, and a word context,\n",
    "        computes the log probability of this word in this context.\n",
    "        \"\"\"\n",
    "        score = self.score(word, context)\n",
    "        if score == 0.0:\n",
    "            return float(\"-inf\")\n",
    "\n",
    "        return log(score, 2)\n",
    "\n",
    "    def entropy(self, text):\n",
    "        \"\"\"\n",
    "        Calculate the approximate cross-entropy of the n-gram model for a\n",
    "        given text represented as a list of comma-separated strings.\n",
    "        This is the average log probability of each word in the text.\n",
    "        \"\"\"\n",
    "        normed_text = (self._check_against_vocab(word) for word in text)\n",
    "        entropy = 0.0\n",
    "        processed_ngrams = 0\n",
    "        for ngram in self.ngram_counter.to_ngrams(normed_text):\n",
    "            context, word = tuple(ngram[:-1]), ngram[-1]\n",
    "            entropy += self.logscore(word, context)\n",
    "            processed_ngrams += 1\n",
    "        return - (entropy / processed_ngrams)\n",
    "\n",
    "    def perplexity(self, text):\n",
    "        \"\"\"\n",
    "        Given list of comma-separated strings, calculates the perplexity\n",
    "        of the text.\n",
    "        \"\"\"\n",
    "        return pow(2.0, self.entropy(text))\n",
    "\n",
    "\n",
    "class AddKNgramModel(BaseNgramModel):\n",
    "    \"\"\"\n",
    "    Provides Add-k-smoothed scores.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, k, *args):\n",
    "        \"\"\"\n",
    "        Expects an input value, k, a number by which\n",
    "        to increment word counts during scoring.\n",
    "        \"\"\"\n",
    "        super(AddKNgramModel, self).__init__(*args)\n",
    "\n",
    "        self.k = k\n",
    "        self.k_norm = len(self.ngram_counter.vocabulary) * k\n",
    "\n",
    "    def score(self, word, context):\n",
    "        \"\"\"\n",
    "        With Add-k-smoothing, the score is normalized with\n",
    "        a k value.\n",
    "        \"\"\"\n",
    "        context = self.check_context(context)\n",
    "        context_freqdist = self.ngrams[context]\n",
    "        word_count = context_freqdist[word]\n",
    "        context_count = context_freqdist.N()\n",
    "        return (word_count + self.k) / \\\n",
    "               (context_count + self.k_norm)\n",
    "\n",
    "\n",
    "class LaplaceNgramModel(AddKNgramModel):\n",
    "    \"\"\"\n",
    "    Implements Laplace (add one) smoothing.\n",
    "    Laplace smoothing is the base case of Add-k smoothing,\n",
    "    with k set to 1.\n",
    "    \"\"\"\n",
    "    def __init__(self, *args):\n",
    "        super(LaplaceNgramModel, self).__init__(1, *args)\n",
    "\n",
    "\n",
    "class KneserNeyModel(BaseNgramModel):\n",
    "    \"\"\"\n",
    "    Implements Kneser-Ney smoothing\n",
    "    \"\"\"\n",
    "    def __init__(self, *args):\n",
    "        super(KneserNeyModel, self).__init__(*args)\n",
    "        self.model = nltk.KneserNeyProbDist(self.ngrams)\n",
    "\n",
    "    def score(self, word, context):\n",
    "        \"\"\"\n",
    "        Use KneserNeyProbDist from NLTK to get score\n",
    "        \"\"\"\n",
    "        trigram = tuple((context[0], context[1], word))\n",
    "        return self.model.prob(trigram)\n",
    "\n",
    "    def samples(self):\n",
    "        return self.model.samples()\n",
    "\n",
    "    def prob(self, sample):\n",
    "        return self.model.prob(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 27s, sys: 1.92 s, total: 1min 29s\n",
      "Wall time: 1min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "corpus = PickledCorpusReader(pickle_dir)\n",
    "tokens = [''.join(word) for word in corpus.words()]\n",
    "vocab = Counter(tokens)\n",
    "sents = list([word[0] for word in sent] for sent in corpus.sents())\n",
    "\n",
    "counter = count_ngrams(3, vocab, sents)\n",
    "knm = KneserNeyModel(counter)\n",
    "\n",
    "\n",
    "def complete(input_text):\n",
    "    tokenized = nltk.word_tokenize(input_text)\n",
    "    if len(tokenized) < 2:\n",
    "        response = \"Say more.\"\n",
    "    else:\n",
    "        completions = {}\n",
    "        for sample in knm.samples():\n",
    "            if (sample[0], sample[1]) == (tokenized[-2], tokenized[-1]):\n",
    "                completions[sample[2]] = knm.prob(sample)\n",
    "        if len(completions) == 0:\n",
    "            response = \"Can we talk about something else?\"\n",
    "        else:\n",
    "            best = max(\n",
    "                completions.keys(), key=(lambda key: completions[key])\n",
    "            )\n",
    "            tokenized += [best]\n",
    "            response = \" \".join(tokenized)\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The President of the United States\n",
      "This election year will be\n"
     ]
    }
   ],
   "source": [
    "print(complete(\"The President of the United\"))\n",
    "print(complete(\"This election year will\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
