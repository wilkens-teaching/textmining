{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed processing\n",
    "\n",
    "## Definitions\n",
    "\n",
    "In a multiprocessing or parallel computing environment, there are multiple workers that handle different aspects of a shared task. The shared task may involve multiple steps, which may (or may not) be separable from one another. Depending on the nature of the task and the bottlenecks (slow steps) it contains, it can be useful to implement **task parallelism** and/or **data parallelism**. \n",
    "\n",
    "### Task parallelism\n",
    "\n",
    "Think of coooking, especially in a restaurant kitchen. Multiple workers perform different parts of the workflow. These tasks are generally independent of one another, though some of them may need to be executed in sequence. The example below, running three different fits over the same data, is an example of task parallelism.\n",
    "\n",
    "### Data parallelism\n",
    "\n",
    "Think of multiple people eating a single dish. Each worker operates on a different part of large dataset. The results of each subset are then combined to produce a single result. \"Split-apply-combine.\"\n",
    "\n",
    "### Should you do it?\n",
    "\n",
    "Note that parallel processing is not always worthwhile. Consider:\n",
    "\n",
    "1. It is generally more difficult to write and debug parallel algorithms.\n",
    "2. Parallel processing always introduces some amount of computational management overhead. In some cases, this overhead can be substantial.\n",
    "3. The computational task itself may not be the slow step in a larger research process. Does it matter if your code takes a day to run? Maybe, but maybe not.\n",
    "\n",
    "## One machine vs. many\n",
    "\n",
    "Multiprocessing generally works on one machine at a time, i.e., the one on which the python interpreter is running. Cluster computing involves (again, generally) many different machines netwroked together. You can scale **a lot** further using a cluster, but this also introduces significant complexity and overhead. Spark is one approach to truly distributed cluster computing.\n",
    "\n",
    "## Multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import multiprocessing as mp\n",
    "import nltk\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import unicodedata\n",
    "\n",
    "from functools import wraps\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Import our libraries\n",
    "sys.path.append(os.path.join('..', 'libraries'))\n",
    "from TMN import PickledCorpusReader\n",
    "\n",
    "# Set path to pickled corpus\n",
    "pickle_dir = os.path.join('..', 'data', 'pickled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Textbook code\n",
    "class TextNormalizer(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, language='english'):\n",
    "        self.stopwords  = set(nltk.corpus.stopwords.words(language))\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    def is_punct(self, token):\n",
    "        return all(\n",
    "            unicodedata.category(char).startswith('P') for char in token\n",
    "        )\n",
    "\n",
    "    def is_stopword(self, token):\n",
    "        return token.lower() in self.stopwords\n",
    "\n",
    "    def normalize(self, document):\n",
    "        return [\n",
    "            self.lemmatize(token, tag).lower()\n",
    "            for paragraph in document\n",
    "            for sentence in paragraph\n",
    "            for (token, tag) in sentence\n",
    "            if not self.is_punct(token) and not self.is_stopword(token)\n",
    "        ]\n",
    "\n",
    "    def lemmatize(self, token, pos_tag):\n",
    "        tag = {\n",
    "            'N': wn.NOUN,\n",
    "            'V': wn.VERB,\n",
    "            'R': wn.ADV,\n",
    "            'J': wn.ADJ\n",
    "        }.get(pos_tag[0], wn.NOUN)\n",
    "\n",
    "        return self.lemmatizer.lemmatize(token, tag)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, documents):\n",
    "        for document in documents:\n",
    "            yield self.normalize(document[0])\n",
    "\n",
    "\n",
    "def identity(doc):\n",
    "    return doc\n",
    "\n",
    "# Logging configuration\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(processName)-10s %(asctime)s %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "\n",
    "def timeit(func):\n",
    "    @wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        return result, time.time() - start\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "def documents(corpus):\n",
    "    return [\n",
    "        list(corpus.docs(fileids=[fileid]))\n",
    "        for fileid in corpus.fileids()\n",
    "    ]\n",
    "\n",
    "\n",
    "def labels(corpus):\n",
    "    return [\n",
    "        corpus.categories(fileids=[fileid])[0]\n",
    "        for fileid in corpus.fileids()\n",
    "    ]\n",
    "\n",
    "\n",
    "@timeit\n",
    "def train_model(path, model, saveto=None, cv=12):\n",
    "    \"\"\"\n",
    "    Trains model from corpus at specified path; constructing cross-validation\n",
    "    scores using the cv parameter, then fitting the model on the full data and\n",
    "    writing it to disk at the saveto path if specified. Returns the scores.\n",
    "    \"\"\"\n",
    "    # Load the corpus data and labels for classification\n",
    "    corpus = PickledCorpusReader(path)\n",
    "    X = documents(corpus)\n",
    "    y = labels(corpus)\n",
    "\n",
    "    # Compute cross validation scores\n",
    "    scores = cross_val_score(model, X, y, cv=3)\n",
    "\n",
    "    # Fit the model on entire data set\n",
    "    model.fit(X, y)\n",
    "\n",
    "    # Write to disk if specified\n",
    "    #if saveto:\n",
    "    #    joblib.dump(model, saveto)\n",
    "\n",
    "    # Return scores as well as training time via decorator\n",
    "    return scores\n",
    "\n",
    "\n",
    "def fit_naive_bayes(path, saveto=None, cv=12):\n",
    "\n",
    "    model = Pipeline([\n",
    "        ('norm', TextNormalizer()),\n",
    "        ('tfidf', TfidfVectorizer(tokenizer=identity, lowercase=False)),\n",
    "        ('clf', MultinomialNB())\n",
    "    ])\n",
    "\n",
    "    if saveto is None:\n",
    "        saveto = \"naive_bayes_{}.pkl\".format(time.time())\n",
    "\n",
    "    scores, delta = train_model(path, model, saveto, cv)\n",
    "    logger.info((\n",
    "        \"naive bayes training took {:0.2f} seconds \"\n",
    "        \"with an average score of {:0.3f}\"\n",
    "    ).format(delta, scores.mean()))\n",
    "\n",
    "\n",
    "def fit_logistic_regression(path, saveto=None, cv=12):\n",
    "    model = Pipeline([\n",
    "        ('norm', TextNormalizer()),\n",
    "        ('tfidf', TfidfVectorizer(tokenizer=identity, lowercase=False)),\n",
    "        ('clf', LogisticRegression(solver='lbfgs'))\n",
    "    ])\n",
    "\n",
    "    if saveto is None:\n",
    "        saveto = \"logistic_regression_{}.pkl\".format(time.time())\n",
    "\n",
    "    scores, delta = train_model(path, model, saveto, cv)\n",
    "    logger.info((\n",
    "        \"logistic regression training took {:0.2f} seconds \"\n",
    "        \"with an average score of {:0.3f}\"\n",
    "    ).format(delta, scores.mean()))\n",
    "\n",
    "\n",
    "def fit_multilayer_perceptron(path, saveto=None, cv=12):\n",
    "    model = Pipeline([\n",
    "        ('norm', TextNormalizer()),\n",
    "        ('tfidf', TfidfVectorizer(tokenizer=identity, lowercase=False)),\n",
    "        ('clf', MLPClassifier(hidden_layer_sizes=(10,10), early_stopping=True))\n",
    "    ])\n",
    "\n",
    "    if saveto is None:\n",
    "        saveto = \"multilayer_perceptron_{}.pkl\".format(time.time())\n",
    "\n",
    "    scores, delta = train_model(path, model, saveto, cv)\n",
    "    logger.info((\n",
    "        \"multilayer perceptron training took {:0.2f} seconds \"\n",
    "        \"with an average score of {:0.3f}\"\n",
    "    ).format(delta, scores.mean()))\n",
    "\n",
    "\n",
    "@timeit\n",
    "def sequential(path):\n",
    "    #Run each fit one after the other\n",
    "    fit_naive_bayes(path)\n",
    "    fit_logistic_regression(path)\n",
    "    fit_multilayer_perceptron(path)\n",
    "\n",
    "\n",
    "@timeit\n",
    "def parallel(path):\n",
    "    tasks = [\n",
    "        fit_naive_bayes, fit_logistic_regression, fit_multilayer_perceptron,\n",
    "    ]\n",
    "\n",
    "    procs = []\n",
    "    for task in tasks:\n",
    "        proc = mp.Process(name=task.__name__, target=task, args=(path,))\n",
    "        procs.append(proc)\n",
    "        proc.start()\n",
    "\n",
    "    for proc in procs:\n",
    "        proc.join() # Tell multiproc that the task is done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MainProcess 2019-04-15 10:26:47 Begin parallel tasks\n",
      "fit_naive_bayes 2019-04-15 10:29:17 naive bayes training took 149.91 seconds with an average score of 0.524\n",
      "fit_logistic_regression 2019-04-15 10:29:17 logistic regression training took 150.03 seconds with an average score of 0.853\n",
      "fit_multilayer_perceptron 2019-04-15 10:29:18 multilayer perceptron training took 151.47 seconds with an average score of 0.500\n",
      "MainProcess 2019-04-15 10:29:18 Total parallel fit time: 151.55 seconds\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"Begin parallel tasks\")\n",
    "_, delta = parallel(pickle_dir)\n",
    "logger.info(\"Total parallel fit time: {:0.2f} seconds\".format(delta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin sequential tasks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MainProcess 2019-04-15 10:31:38 naive bayes training took 93.97 seconds with an average score of 0.524\n",
      "MainProcess 2019-04-15 10:33:09 logistic regression training took 91.02 seconds with an average score of 0.853\n",
      "MainProcess 2019-04-15 10:34:46 multilayer perceptron training took 97.07 seconds with an average score of 0.500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sequential fit time: 282.09 seconds\n"
     ]
    }
   ],
   "source": [
    "print(\"Begin sequential tasks\")\n",
    "_, delta = sequential(pickle_dir)\n",
    "print(\"Total sequential fit time: {:0.2f} seconds\".format(delta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
