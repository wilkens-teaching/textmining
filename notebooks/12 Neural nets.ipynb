{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural networks\n",
    "\n",
    "## Sentiment analysis with Scikit-learn\n",
    "\n",
    "Predict album review sentiment from the [Kaggle Pitchfork dataset](https://www.kaggle.com/nolanbconaway/pitchfork-data/data).\n",
    "\n",
    "Need to install three new libraries:\n",
    "\n",
    "```\n",
    "conda install keras tensorflow python-slugify\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Imports and setup\n",
    "import gensim\n",
    "import multiprocessing as mp\n",
    "import nltk\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import sqlite3\n",
    "import unicodedata\n",
    "\n",
    "from gensim.matutils import sparse2full, full2sparse, full2sparse_clipped, scipy2scipy_clipped\n",
    "from gensim.models.doc2vec import TaggedDocument, Doc2Vec\n",
    "from itertools import groupby\n",
    "from keras.layers import Dense, Dropout, Activation, LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.models import load_model, Sequential\n",
    "from keras.preprocessing import sequence\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus.reader.api import CorpusReader\n",
    "from nltk.chunk import tree2conlltags\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.chunk.regexp import RegexpParser\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neural_network import MLPRegressor, MLPClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from slugify import slugify\n",
    "from unicodedata import category as unicat\n",
    "\n",
    "# Local paths\n",
    "pitchfork_dir = os.path.join('..', 'data', 'pitchfork')\n",
    "sql_file = os.path.join(pitchfork_dir, 'database.sqlite')\n",
    "pickle_dir = os.path.join(pitchfork_dir, 'pickled')\n",
    "pitchfork_model = os.path.join(pitchfork_dir, 'pitchfork_clf.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Readers and utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PKL_PATTERN = r'(?!\\.)[\\w\\s\\d\\-]+\\.pickle'\n",
    "\n",
    "class SqliteCorpusReader(object):\n",
    "\n",
    "    def __init__(self, path):\n",
    "        self._cur = sqlite3.connect(path).cursor()\n",
    "\n",
    "    def scores(self):\n",
    "        \"\"\"\n",
    "        Returns the review score\n",
    "        \"\"\"\n",
    "        self._cur.execute(\"SELECT score FROM reviews\")\n",
    "        scores = self._cur.fetchall()\n",
    "        for score in scores:\n",
    "            yield score\n",
    "\n",
    "    def texts(self):\n",
    "        \"\"\"\n",
    "        Returns the full review texts\n",
    "        \"\"\"\n",
    "        self._cur.execute(\"SELECT content FROM content\")\n",
    "        texts = self._cur.fetchall()\n",
    "        for text in texts:\n",
    "            yield text\n",
    "\n",
    "    def ids(self):\n",
    "        \"\"\"\n",
    "        Returns the review ids\n",
    "        \"\"\"\n",
    "        self._cur.execute(\"SELECT reviewid FROM content\")\n",
    "        ids = self._cur.fetchall()\n",
    "        for idx in ids:\n",
    "            yield idx\n",
    "\n",
    "    def ids_and_texts(self):\n",
    "        \"\"\"\n",
    "        Returns the review ids\n",
    "        \"\"\"\n",
    "        self._cur.execute(\"SELECT * FROM content\")\n",
    "        results = self._cur.fetchall()\n",
    "        for idx,text in results:\n",
    "            yield idx,text\n",
    "\n",
    "    def scores_albums_artists_texts(self):\n",
    "        \"\"\"\n",
    "        Returns a generator with each review represented as a\n",
    "        (score, album name, artist name, review text) tuple\n",
    "        \"\"\"\n",
    "        sql = \"\"\"\n",
    "              SELECT S.score, L.label, A.artist, R.content\n",
    "              FROM [reviews] S\n",
    "              JOIN labels L ON S.reviewid=L.reviewid\n",
    "              JOIN artists A on L.reviewid=A.reviewid\n",
    "              JOIN content R ON A.reviewid=R.reviewid\n",
    "              \"\"\"\n",
    "        self._cur.execute(sql)\n",
    "        results = self._cur.fetchall()\n",
    "        for score,album,band,text in results:\n",
    "            yield (score,album,band,text)\n",
    "\n",
    "    def albums(self):\n",
    "        \"\"\"\n",
    "        Returns the names of albums being reviewed\n",
    "        \"\"\"\n",
    "        self._cur.execute(\"SELECT * FROM labels\")\n",
    "        albums = self._cur.fetchall()\n",
    "        for idx,album in albums:\n",
    "            yield idx,album\n",
    "\n",
    "    def artists(self):\n",
    "        \"\"\"\n",
    "        Returns the name of the artist being reviewed\n",
    "        \"\"\"\n",
    "        self._cur.execute(\"SELECT * FROM artists\")\n",
    "        artists = self._cur.fetchall()\n",
    "        for idx,artist in artists:\n",
    "            yield idx,artist\n",
    "\n",
    "    def genres(self):\n",
    "        \"\"\"\n",
    "        Returns the music genre of each review\n",
    "        \"\"\"\n",
    "        self._cur.execute(\"SELECT * FROM genres\")\n",
    "        genres = self._cur.fetchall()\n",
    "        for idx,genre in genres:\n",
    "            yield idx,genre\n",
    "\n",
    "    def years(self):\n",
    "        \"\"\"\n",
    "        Returns the publication year of each review\n",
    "\n",
    "        Note: There are many missing values\n",
    "        \"\"\"\n",
    "        self._cur.execute(\"SELECT * FROM years\")\n",
    "        years = self._cur.fetchall()\n",
    "        for idx,year in years:\n",
    "            yield idx,year\n",
    "\n",
    "    def paras(self):\n",
    "        \"\"\"\n",
    "        Returns a generator of paragraphs.\n",
    "        \"\"\"\n",
    "        for text in self.texts():\n",
    "            for paragraph in text:\n",
    "                yield paragraph\n",
    "\n",
    "    def sents(self):\n",
    "        \"\"\"\n",
    "        Returns a generator of sentences.\n",
    "        \"\"\"\n",
    "        for para in self.paras():\n",
    "            for sentence in nltk.sent_tokenize(para):\n",
    "                yield sentence\n",
    "\n",
    "    def words(self):\n",
    "        \"\"\"\n",
    "        Returns a generator of words.\n",
    "        \"\"\"\n",
    "        for sent in self.sents():\n",
    "            for word in nltk.wordpunct_tokenize(sent):\n",
    "                yield word\n",
    "\n",
    "    def tagged_tokens(self):\n",
    "        for sent in self.sents():\n",
    "            for word in nltk.wordpunct_tokenize(sent):\n",
    "                yield nltk.pos_tag(word)\n",
    "\n",
    "\n",
    "\n",
    "class PickledReviewsReader(CorpusReader):\n",
    "    def __init__(self, root, fileids=PKL_PATTERN, **kwargs):\n",
    "        \"\"\"\n",
    "        Initialize the corpus reader\n",
    "        \"\"\"\n",
    "        CorpusReader.__init__(self, root, fileids, **kwargs)\n",
    "\n",
    "    def texts_scores(self, fileids=None):\n",
    "        \"\"\"\n",
    "        Returns the document loaded from a pickled object for every file in\n",
    "        the corpus. Similar to the SqliteCorpusReader, this uses a generator\n",
    "        to achieve memory safe iteration.\n",
    "        \"\"\"\n",
    "        # Create a generator, loading one document into memory at a time.\n",
    "        for path, enc, fileid in self.abspaths(fileids, True, True):\n",
    "            with open(path, 'rb') as f:\n",
    "                yield pickle.load(f)\n",
    "\n",
    "    def reviews(self, fileids=None):\n",
    "        \"\"\"\n",
    "        Returns a generator of paragraphs where each paragraph is a list of\n",
    "        sentences, which is in turn a list of (token, tag) tuples.\n",
    "        \"\"\"\n",
    "        for text,score in self.texts_scores(fileids):\n",
    "            yield text\n",
    "\n",
    "    def scores(self, fileids=None):\n",
    "        \"\"\"\n",
    "        Return the scores\n",
    "        \"\"\"\n",
    "        for text,score in self.texts_scores(fileids):\n",
    "            yield score\n",
    "\n",
    "    def paras(self, fileids=None):\n",
    "        \"\"\"\n",
    "        Returns a generator of paragraphs where each paragraph is a list of\n",
    "        sentences, which is in turn a list of (token, tag) tuples.\n",
    "        \"\"\"\n",
    "        for review in self.reviews(fileids):\n",
    "            for paragraph in review:\n",
    "                yield paragraph\n",
    "\n",
    "    def sents(self, fileids=None):\n",
    "        \"\"\"\n",
    "        Returns a generator of sentences where each sentence is a list of\n",
    "        (token, tag) tuples.\n",
    "        \"\"\"\n",
    "        for paragraph in self.paras(fileids):\n",
    "            for sentence in paragraph:\n",
    "                yield sentence\n",
    "\n",
    "    def tagged(self, fileids=None):\n",
    "        for sent in self.sents(fileids):\n",
    "            for token in sent:\n",
    "                yield token\n",
    "\n",
    "    def words(self, fileids=None):\n",
    "        \"\"\"\n",
    "        Returns a generator of (token, tag) tuples.\n",
    "        \"\"\"\n",
    "        for token in self.tagged(fileids):\n",
    "            yield token[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextNormalizer(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, language='english'):\n",
    "        self.stopwords  = set(nltk.corpus.stopwords.words(language))\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    def is_punct(self, token):\n",
    "        return all(\n",
    "            unicodedata.category(char).startswith('P') for char in token\n",
    "        )\n",
    "\n",
    "    def is_stopword(self, token):\n",
    "        return token.lower() in self.stopwords\n",
    "\n",
    "    def normalize(self, document):\n",
    "        return [\n",
    "            self.lemmatize(token, tag).lower()\n",
    "            for sentence in document\n",
    "            for (token, tag) in sentence\n",
    "            if not self.is_punct(token)\n",
    "               and not self.is_stopword(token)\n",
    "        ]\n",
    "\n",
    "    def lemmatize(self, token, pos_tag):\n",
    "        tag = {\n",
    "            'N': wn.NOUN,\n",
    "            'V': wn.VERB,\n",
    "            'R': wn.ADV,\n",
    "            'J': wn.ADJ\n",
    "        }.get(pos_tag[0], wn.NOUN)\n",
    "\n",
    "        return self.lemmatizer.lemmatize(token, tag)\n",
    "\n",
    "    def fit(self, documents, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, documents):\n",
    "        return [\n",
    "            ' '.join(self.normalize(doc)) for doc in documents\n",
    "        ]\n",
    "\n",
    "\n",
    "class GensimDoc2Vectorizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, size=5, min_count=3):\n",
    "        \"\"\"\n",
    "        gensim_doc2vec_vectorize\n",
    "        \"\"\"\n",
    "        self.size = size\n",
    "        self.min_count = min_count\n",
    "\n",
    "    def fit(self, documents, labels=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, documents):\n",
    "        docs = [\n",
    "            TaggedDocument(words, ['d{}'.format(idx)])\n",
    "            for idx, words in enumerate(documents)\n",
    "        ]\n",
    "        model = Doc2Vec(docs, size=self.size, min_count=self.min_count)\n",
    "        return np.array(list(model.docvecs))\n",
    "\n",
    "class GensimTfidfVectorizer(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, nfeatures=100, tofull=False):\n",
    "        \"\"\"\n",
    "        Pass in a directory that holds the lexicon in corpus.dict and the\n",
    "        TFIDF model in tfidf.model (for now).\n",
    "\n",
    "        Set tofull = True if the next thing is a Scikit-Learn estimator\n",
    "        otherwise keep False if the next thing is a Gensim model.\n",
    "        \"\"\"\n",
    "        self._lexicon_path = \"lexigram.dict\"\n",
    "        self._tfidf_path = \"tfidf.model\"\n",
    "        self.nfeatures = nfeatures\n",
    "        self.lexicon = None\n",
    "        self.tfidf = None\n",
    "        self.tofull = tofull\n",
    "\n",
    "        self.load()\n",
    "\n",
    "    def load(self):\n",
    "        if os.path.exists(self._lexicon_path):\n",
    "            self.lexicon = gensim.corpora.Dictionary.load(self._lexicon_path)\n",
    "\n",
    "        if os.path.exists(self._tfidf_path):\n",
    "            self.tfidf = gensim.models.TfidfModel().load(self._tfidf_path)\n",
    "\n",
    "    def save(self):\n",
    "        self.lexicon.save(self._lexicon_path)\n",
    "        self.tfidf.save(self._tfidf_path)\n",
    "\n",
    "    def fit(self, documents, labels=None):\n",
    "        self.lexicon = gensim.corpora.Dictionary(documents, prune_at=self.nfeatures)\n",
    "        self.lexicon.filter_extremes(keep_n=self.nfeatures)\n",
    "        self.lexicon.compactify()\n",
    "        self.tfidf = gensim.models.TfidfModel(\n",
    "            [self.lexicon.doc2bow(doc) for doc in documents],\n",
    "            id2word=self.lexicon\n",
    "        )\n",
    "        self.save()\n",
    "        return self\n",
    "\n",
    "    def transform(self, documents):\n",
    "        def generator():\n",
    "            for document in documents:\n",
    "                vec = self.tfidf[self.lexicon.doc2bow(document)]\n",
    "                if self.tofull:\n",
    "                    yield sparse2full(vec, len(self.lexicon))\n",
    "                else:\n",
    "                    yield vec\n",
    "        return np.array(list(generator()))\n",
    "\n",
    "\n",
    "class KeyphraseExtractor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Extract adverbial and adjective phrases, and transform\n",
    "    documents into lists of these keyphrases, with a total\n",
    "    keyphrase lexicon limited by the nfeatures parameter\n",
    "    and a document length limited/padded to doclen\n",
    "    \"\"\"\n",
    "    def __init__(self, nfeatures=100000, doclen=60):\n",
    "        self.grammar = r'KT: {(<RB.> <JJ.*>|<VB.*>|<RB.*>)|(<JJ> <NN.*>)}'\n",
    "        # self.grammar = r'KT: {(<RB.*> <VB.>|<RB.>|<JJ.> <NN.*>)}'\n",
    "        # self.grammar = r'KT: {<RB.>|<JJ.>}'\n",
    "        self.chunker = RegexpParser(self.grammar)\n",
    "        self.nfeatures = nfeatures\n",
    "        self.doclen = doclen\n",
    "\n",
    "    def normalize(self, sent):\n",
    "        \"\"\"\n",
    "        Removes punctuation from a tokenized/tagged sentence and\n",
    "        lowercases words.\n",
    "        \"\"\"\n",
    "        is_punct = lambda word: all(unicat(c).startswith('P') for c in word)\n",
    "        sent = filter(lambda t: not is_punct(t[0]), sent)\n",
    "        sent = map(lambda t: (t[0].lower(), t[1]), sent)\n",
    "        return list(sent)\n",
    "\n",
    "    def extract_candidate_phrases(self, sents):\n",
    "        \"\"\"\n",
    "        For a document, parse sentences using our chunker created by\n",
    "        our grammar, converting the parse tree into a tagged sequence.\n",
    "        Extract phrases, rejoin with a space, and yield the document\n",
    "        represented as a list of it's keyphrases.\n",
    "        \"\"\"\n",
    "        for sent in sents:\n",
    "            sent = self.normalize(sent)\n",
    "            if not sent: continue\n",
    "            chunks = tree2conlltags(self.chunker.parse(sent))\n",
    "            phrases = [\n",
    "                \" \".join(word for word, pos, chunk in group).lower()\n",
    "                for key, group in groupby(\n",
    "                    chunks, lambda term: term[-1] != 'O'\n",
    "                ) if key\n",
    "            ]\n",
    "            for phrase in phrases:\n",
    "                yield phrase\n",
    "\n",
    "    def fit(self, documents, y=None):\n",
    "        return self\n",
    "\n",
    "    def get_lexicon(self, keydocs):\n",
    "        \"\"\"\n",
    "        Build a lexicon of size nfeatures\n",
    "        \"\"\"\n",
    "        keyphrases = [keyphrase for doc in keydocs for keyphrase in doc]\n",
    "        fdist = FreqDist(keyphrases)\n",
    "        counts = fdist.most_common(self.nfeatures)\n",
    "        lexicon = [phrase for phrase, count in counts]\n",
    "        return {phrase: idx+1 for idx, phrase in enumerate(lexicon)}\n",
    "\n",
    "    def clip(self, keydoc, lexicon):\n",
    "        \"\"\"\n",
    "        Remove keyphrases from documents that aren't in the lexicon\n",
    "        \"\"\"\n",
    "        return [lexicon[keyphrase] for keyphrase in keydoc\n",
    "                if keyphrase in lexicon.keys()]\n",
    "\n",
    "    def transform(self, documents):\n",
    "        docs = [list(self.extract_candidate_phrases(doc)) for doc in documents]\n",
    "        lexicon = self.get_lexicon(docs)\n",
    "        clipped = [list(self.clip(doc, lexicon)) for doc in docs]\n",
    "        return sequence.pad_sequences(clipped, maxlen=self.doclen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessor(object):\n",
    "    \"\"\"\n",
    "    The preprocessor wraps a SqliteCorpusReader and manages the stateful\n",
    "    tokenization and part of speech tagging into a directory that is stored\n",
    "    in a format that can be read by the `PickledCorpusReader`. This format\n",
    "    is more compact and necessarily removes a variety of fields from the\n",
    "    document that are stored in Sqlite database. This format however is more\n",
    "    easily accessed for common parsing activity.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, corpus, target=None, **kwargs):\n",
    "        \"\"\"\n",
    "        The corpus is the `SqliteCorpusReader` to preprocess and pickle.\n",
    "        The target is the directory on disk to output the pickled corpus to.\n",
    "        \"\"\"\n",
    "        self.tasks = mp.cpu_count()\n",
    "        self.corpus = corpus\n",
    "        self.target = target\n",
    "\n",
    "    @property\n",
    "    def target(self):\n",
    "        return self._target\n",
    "\n",
    "    @target.setter\n",
    "    def target(self, path):\n",
    "        if path is not None:\n",
    "            # Normalize the path and make it absolute\n",
    "            path = os.path.expanduser(path)\n",
    "            path = os.path.expandvars(path)\n",
    "            path = os.path.abspath(path)\n",
    "\n",
    "            if os.path.exists(path):\n",
    "                if not os.path.isdir(path):\n",
    "                    raise ValueError(\n",
    "                        \"Please supply a directory to write preprocessed data to.\"\n",
    "                    )\n",
    "\n",
    "        self._target = path\n",
    "\n",
    "    def abspath(self, name):\n",
    "        \"\"\"\n",
    "        Returns the absolute path to the target fileid from the corpus fileid.\n",
    "        \"\"\"\n",
    "        # Create the pickle file extension\n",
    "        fname  = str(name) + '.pickle'\n",
    "\n",
    "        # Return the path to the file relative to the target.\n",
    "        return os.path.normpath(os.path.join(self.target, fname))\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        \"\"\"\n",
    "        Segments, tokenizes, and tags a document in the corpus. Returns a\n",
    "        generator of paragraphs, which are lists of sentences, which in turn\n",
    "        are lists of part of speech tagged words.\n",
    "        \"\"\"\n",
    "        yield [\n",
    "            nltk.pos_tag(nltk.wordpunct_tokenize(sent))\n",
    "            for sent in nltk.sent_tokenize(text)\n",
    "        ]\n",
    "\n",
    "    def process(self, score_album_artist_text):\n",
    "        \"\"\"\n",
    "        For a single file does the following preprocessing work:\n",
    "            1. Checks the location on disk to make sure no errors occur.\n",
    "            2. Gets all paragraphs for the given text.\n",
    "            3. Segments the paragraphs with the sent_tokenizer\n",
    "            4. Tokenizes the sentences with the wordpunct_tokenizer\n",
    "            5. Tags the sentences using the default pos_tagger\n",
    "            6. Writes the document as a pickle to the target location.\n",
    "        This method is called multiple times from the transform runner.\n",
    "        \"\"\"\n",
    "        score, album, artist, text = score_album_artist_text\n",
    "\n",
    "        # Compute the outpath to write the file to.\n",
    "        if album:\n",
    "            name = album+'-'+artist\n",
    "        else:\n",
    "            name = artist\n",
    "        target = self.abspath(slugify(name))\n",
    "        parent = os.path.dirname(target)\n",
    "\n",
    "        # Make sure the directory exists\n",
    "        if not os.path.exists(parent):\n",
    "            os.makedirs(parent)\n",
    "\n",
    "        # Make sure that the parent is a directory and not a file\n",
    "        if not os.path.isdir(parent):\n",
    "            raise ValueError(\n",
    "                \"Please supply a directory to write preprocessed data to.\"\n",
    "            )\n",
    "\n",
    "        # Create a data structure for the pickle\n",
    "        document = list(self.tokenize(text))\n",
    "        document.append(score)\n",
    "\n",
    "        # Open and serialize the pickle to disk\n",
    "        with open(target, 'wb') as f:\n",
    "            pickle.dump(document, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "        # Clean up the document\n",
    "        del document\n",
    "\n",
    "        # Return the target fileid\n",
    "        return target\n",
    "\n",
    "    def transform(self):\n",
    "        \"\"\"\n",
    "        Transform the wrapped corpus, writing out the segmented, tokenized,\n",
    "        and part of speech tagged corpus as a pickle to the target directory.\n",
    "        This method will also directly copy files that are in the corpus.root\n",
    "        directory that are not matched by the corpus.fileids().\n",
    "        \"\"\"\n",
    "        # Make the target directory if it doesn't already exist\n",
    "        if not os.path.exists(self.target):\n",
    "            os.makedirs(self.target)\n",
    "\n",
    "        for score_album_artist_text in self.corpus.scores_albums_artists_texts():\n",
    "            yield self.process(score_album_artist_text)\n",
    "\n",
    "\n",
    "class ParallelPreprocessor(Preprocessor):\n",
    "    \"\"\"\n",
    "    Implements multiprocessing to speed up the preprocessing efforts.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Get parallel-specific arguments and then call super.\n",
    "        \"\"\"\n",
    "        self.tasks = mp.cpu_count()\n",
    "        super(ParallelPreprocessor, self).__init__(*args, **kwargs)\n",
    "\n",
    "    def on_result(self, result):\n",
    "        \"\"\"\n",
    "        Appends the results to the master results list.\n",
    "        \"\"\"\n",
    "        self.results.append(result)\n",
    "\n",
    "    def transform(self):\n",
    "        \"\"\"\n",
    "        Create a pool using the multiprocessing library, passing in\n",
    "        the number of cores available to set the desired number of\n",
    "        processes.\n",
    "        \"\"\"\n",
    "        # Make the target directory if it doesn't already exist\n",
    "        if not os.path.exists(self.target):\n",
    "            os.makedirs(self.target)\n",
    "\n",
    "        # Reset the results\n",
    "        self.results = []\n",
    "\n",
    "        # Create a multiprocessing pool\n",
    "        pool  = mp.Pool(processes=self.tasks)\n",
    "        tasks = [\n",
    "            pool.apply_async(self.process, (score_album_artist_text,), callback=self.on_result)\n",
    "            for score_album_artist_text in self.corpus.scores_albums_artists_texts()\n",
    "        ]\n",
    "\n",
    "        # Close the pool and join\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "\n",
    "        return self.results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## This is slow(ish) - takes c. an hour and a half\n",
    "# Download the data from https://www.kaggle.com/nolanbconaway/pitchfork-data/data\n",
    "# Preprocess the reviews corpus\n",
    "corpus = SqliteCorpusReader(sql_file)\n",
    "transformer = Preprocessor(corpus, pickle_dir)\n",
    "docs = transformer.transform()\n",
    "print(len(list(docs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validate review sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scoring function and utilities\n",
    "def documents(corpus):\n",
    "    return list(corpus.reviews())\n",
    "\n",
    "def continuous(corpus):\n",
    "    return list(corpus.scores())\n",
    "\n",
    "def make_categorical(corpus):\n",
    "    \"\"\"\n",
    "    terrible : 0.0 < y <= 3.0\n",
    "    okay     : 3.0 < y <= 5.0\n",
    "    great    : 5.0 < y <= 7.0\n",
    "    amazing  : 7.0 < y <= 10.1\n",
    "    :param corpus:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    return np.digitize(continuous(corpus), [0.0, 3.0, 5.0, 7.0, 10.1])\n",
    "\n",
    "def train_model(path, model, continuous=True, saveto=None, cv=4):\n",
    "    \"\"\"\n",
    "    Trains model from corpus at specified path; constructing cross-validation\n",
    "    scores using the cv parameter, then fitting the model on the full data and\n",
    "    writing it to disk at the saveto path if specified. Returns the scores.\n",
    "    \"\"\"\n",
    "    # Load the corpus data and labels for classification\n",
    "    corpus = PickledReviewsReader(path)\n",
    "    X = documents(corpus)\n",
    "    if continuous:\n",
    "        y = continuous(corpus)\n",
    "        scoring = 'r2'\n",
    "    else:\n",
    "        y = make_categorical(corpus)\n",
    "        scoring = 'f1_weighted'\n",
    "\n",
    "    # Compute cross validation scores\n",
    "    scores = cross_val_score(model, X, y, cv=cv, scoring=scoring)\n",
    "\n",
    "    # Fit the model on entire data set\n",
    "    model.fit(X, y)\n",
    "\n",
    "    # Write to disk if specified\n",
    "    if saveto:\n",
    "        joblib.dump(model, saveto)\n",
    "\n",
    "    # Return scores as well as training time via decorator\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training discreet model ...\n",
      "Iteration 1, loss = 0.92133579\n",
      "Iteration 2, loss = 0.47235019\n",
      "Iteration 3, loss = 0.11476234\n",
      "Iteration 4, loss = 0.02088891\n",
      "Iteration 5, loss = 0.00650096\n",
      "Iteration 6, loss = 0.00419209\n",
      "Iteration 7, loss = 0.00344165\n",
      "Iteration 8, loss = 0.00301999\n",
      "Iteration 9, loss = 0.00276745\n",
      "Iteration 10, loss = 0.00261010\n",
      "Iteration 11, loss = 0.00249849\n",
      "Iteration 12, loss = 0.00241114\n",
      "Iteration 13, loss = 0.00234038\n",
      "Iteration 14, loss = 0.00227524\n",
      "Iteration 15, loss = 0.00221029\n",
      "Iteration 16, loss = 0.00215361\n",
      "Iteration 17, loss = 0.00210284\n",
      "Iteration 18, loss = 0.00205504\n",
      "Iteration 19, loss = 0.00199417\n",
      "Iteration 20, loss = 0.00197123\n",
      "Iteration 21, loss = 0.00193158\n",
      "Iteration 22, loss = 0.00187751\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.91072408\n",
      "Iteration 2, loss = 0.41439685\n",
      "Iteration 3, loss = 0.10193498\n",
      "Iteration 4, loss = 0.02342571\n",
      "Iteration 5, loss = 0.00644956\n",
      "Iteration 6, loss = 0.00425098\n",
      "Iteration 7, loss = 0.00354700\n",
      "Iteration 8, loss = 0.00317772\n",
      "Iteration 9, loss = 0.00300885\n",
      "Iteration 10, loss = 0.00288031\n",
      "Iteration 11, loss = 0.00278422\n",
      "Iteration 12, loss = 0.00269009\n",
      "Iteration 13, loss = 0.00258639\n",
      "Iteration 14, loss = 0.00254345\n",
      "Iteration 15, loss = 0.00248020\n",
      "Iteration 16, loss = 0.00251114\n",
      "Iteration 17, loss = 0.00236574\n",
      "Iteration 18, loss = 0.00231901\n",
      "Iteration 19, loss = 0.00230223\n",
      "Iteration 20, loss = 0.00221040\n",
      "Iteration 21, loss = 0.00219565\n",
      "Iteration 22, loss = 0.00211100\n",
      "Iteration 23, loss = 0.00207052\n",
      "Iteration 24, loss = 0.00210304\n",
      "Iteration 25, loss = 0.00206702\n",
      "Iteration 26, loss = 0.00198940\n",
      "Iteration 27, loss = 0.00192792\n",
      "Iteration 28, loss = 0.00202424\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.89188134\n",
      "Iteration 2, loss = 0.42873997\n",
      "Iteration 3, loss = 0.08672389\n",
      "Iteration 4, loss = 0.01585487\n",
      "Iteration 5, loss = 0.00591080\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Set up processed review reader\n",
    "reader = PickledReviewsReader(pickle_dir)\n",
    "\n",
    "# Perform sentiment scoring\n",
    "pipeline = Pipeline([\n",
    "    ('norm', TextNormalizer()), # can use KeyphraseExtractor() instead\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('ann', MLPClassifier(hidden_layer_sizes=[500,150], verbose=True))\n",
    "])\n",
    "\n",
    "print(\"Start training discreet model ...\")\n",
    "scores = train_model(pickle_dir, pipeline, continuous=False, saveto=pitchfork_model)\n",
    "\n",
    "print(\"Training complete.\")\n",
    "for idx, score in enumerate(scores):\n",
    "    print(\"Accuracy on slice #{}: {}.\".format((idx+1), score))\n",
    "print(\"Total fit time: {:0.2f} seconds\".format(delta))\n",
    "print(\"Model saved to {}.\".format(pitchfork_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Ditto, continuous (regression) model\n",
    "print()\n",
    "print(\"Starting training regression model ...\")\n",
    "scores, delta = train_model(pickle_dir, pipeline, continuous=True, saveto=None)\n",
    "\n",
    "print(\"Training complete.\")\n",
    "for idx, score in enumerate(scores):\n",
    "    print(\"Accuracy on slice #{}: {}.\".format((idx+1), score))\n",
    "print(\"Total fit time: {:0.2f} seconds\".format(delta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep learning with Keras and TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions and settings\n",
    "N_FEATURES = 10000\n",
    "DOC_LEN = 500\n",
    "N_CLASSES = 4\n",
    "\n",
    "def binarize(corpus):\n",
    "    return np.digitize(continuous(corpus), [0.0, 3.0, 5.1])\n",
    "\n",
    "def build_nn():\n",
    "    \"\"\"\n",
    "    Create a function that returns a compiled neural network\n",
    "    :return: compiled Keras neural network model\n",
    "    \"\"\"\n",
    "    nn = Sequential()\n",
    "    nn.add(Dense(500, activation='relu', input_shape=(N_FEATURES,)))\n",
    "    nn.add(Dense(150, activation='relu'))\n",
    "    nn.add(Dense(N_CLASSES, activation='softmax'))\n",
    "    nn.compile(\n",
    "        loss='categorical_crossentropy',\n",
    "        optimizer='adam',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return nn\n",
    "\n",
    "def build_lstm():\n",
    "    lstm = Sequential()\n",
    "    lstm.add(Embedding(N_FEATURES+1, 128, input_length=DOC_LEN))\n",
    "    lstm.add(Dropout(0.4))\n",
    "    lstm.add(LSTM(units=200, recurrent_dropout=0.2, dropout=0.2))\n",
    "    lstm.add(Dropout(0.2))\n",
    "    lstm.add(Dense(N_CLASSES, activation='sigmoid'))\n",
    "    lstm.compile(\n",
    "        loss='categorical_crossentropy', # b/c target vals are 1 or 2\n",
    "        optimizer='adam',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return lstm\n",
    "\n",
    "def train_nn_model(path, model, reader, saveto=None, cv=12, **kwargs):\n",
    "    \"\"\"\n",
    "    Trains model from corpus at specified path;\n",
    "    fitting the model on the full data and\n",
    "    writing it to disk at the saveto directory if specified.\n",
    "    Returns the scores.\n",
    "    \"\"\"\n",
    "    # Load the corpus data and labels for classification\n",
    "    corpus = reader(path)\n",
    "    X = documents(corpus)\n",
    "    y = make_categorical(corpus) # for Pitchfork\n",
    "    #y = binarize(corpus)\n",
    "\n",
    "    # Compute cross validation scores\n",
    "    # mp note: http://scikit-learn.org/stable/faq.html#why-do-i-sometime-get-a-crash-freeze-with-n-jobs-1-under-osx-or-linux\n",
    "    scores = cross_val_score(model, X, y, cv=cv, scoring='accuracy')\n",
    "\n",
    "    # Fit the model on entire data set\n",
    "    model.fit(X, y)\n",
    "\n",
    "    # Write to disk if specified\n",
    "    if saveto:\n",
    "        # have to save the keras part using keras' save method\n",
    "        model.steps[-1][1].model.save(saveto['keras_model'])\n",
    "        model.steps.pop(-1)\n",
    "        # ... and use joblib to save the rest of the pipeline\n",
    "        joblib.dump(model, saveto['sklearn_pipe'])\n",
    "\n",
    "    # Return scores as well as training time via decorator\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/mwilkens/anaconda3/envs/work/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Users/mwilkens/anaconda3/envs/work/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/200\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Build a Keras Sequential model for the Pitchfork reviews\n",
    "cpath = pickle_dir\n",
    "mpath = {\n",
    "    'keras_model'  : os.path.join(pitchfork_dir, 'keras_nn.h5'),\n",
    "    'sklearn_pipe' : os.path.join(pitchfork_dir, 'nn_pipeline.pkl')\n",
    "}\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('norm', TextNormalizer()),\n",
    "    ('vect', TfidfVectorizer(max_features=N_FEATURES)), # need to control feature count\n",
    "    ('nn', KerasClassifier(build_fn=build_nn, # pass but don't call the function!\n",
    "                           epochs=200,\n",
    "                           batch_size=128))\n",
    "])\n",
    "\n",
    "scores, delta = train_nn_model(cpath, pipeline, PickledReviewsReader, saveto=mpath, cv=12)\n",
    "for idx, score in enumerate(scores):\n",
    "    print('Accuracy on slice #{}: {}.'.format((idx+1), score))\n",
    "print('Total fit time: {:0.2f} seconds'.format(delta))\n",
    "print('Model saved to {}.'.format(list(mpath)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
