{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Imports and setup\n",
    "import gensim\n",
    "import multiprocessing as mp\n",
    "import nltk\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import sqlite3\n",
    "import unicodedata\n",
    "\n",
    "from gensim.matutils import sparse2full, full2sparse, full2sparse_clipped, scipy2scipy_clipped\n",
    "from gensim.models.doc2vec import TaggedDocument, Doc2Vec\n",
    "from itertools import groupby\n",
    "from keras.layers import Dense, Dropout, Activation, LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.models import load_model, Sequential\n",
    "from keras.preprocessing import sequence\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus.reader.api import CorpusReader\n",
    "from nltk.chunk import tree2conlltags\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.chunk.regexp import RegexpParser\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from random import choice\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neural_network import MLPRegressor, MLPClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from slugify import slugify\n",
    "from unicodedata import category as unicat\n",
    "\n",
    "# Local paths\n",
    "pitchfork_dir = os.path.join('..', 'data', 'pitchfork')\n",
    "sql_file = os.path.join(pitchfork_dir, 'database.sqlite')\n",
    "pickle_dir = os.path.join(pitchfork_dir, 'pickled')\n",
    "pitchfork_model = os.path.join(pitchfork_dir, 'pitchfork_clf.pkl')\n",
    "\n",
    "# Settings\n",
    "preproc = False # Perform preprocessing? (takes c. 1.5 hours)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basics: Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions after training\n",
      "[0 0]: 0\n",
      "[0 1]: 0\n",
      "[1 0]: 0\n",
      "[1 1]: 1\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "# Neuron activates if x is positive\n",
    "activation = lambda x: 0 if x < 0 else 1\n",
    "\n",
    "# 3D input data with labels\n",
    "training_data = [(np.array([0,0,1]), 0),\n",
    "                 (np.array([0,1,1]), 0),\n",
    "                 (np.array([1,0,1]), 0),\n",
    "                 (np.array([1,1,1]), 1),\n",
    "                ]\n",
    "\n",
    "# Model parameters\n",
    "learning_rate = 0.2 \n",
    "training_steps = 1000\n",
    "\n",
    "# Initialize weights to random 3D arrays\n",
    "# Values range from zero to one\n",
    "W = np.random.rand(3) \n",
    "\n",
    "# Perform the indicated number of training steps\n",
    "for i in range(training_steps):\n",
    "    \n",
    "    x, y = choice(training_data) # Pick one data element on which to train\n",
    "    \n",
    "    layer1 = np.dot(W, x)        # Compute the output weight\n",
    "    y_pred = activation(layer1)  # Pass output weight to activation, i.e., prediction\n",
    "    \n",
    "    error = y - y_pred           # Calculate amount of error (here, 0 or 1)\n",
    "    update = learning_rate * error * x # Calculate update weight\n",
    "    W += update                  # Update array of weights\n",
    "\n",
    "# Output after training\n",
    "print(\"Predictions after training\")\n",
    "for x, _ in training_data:\n",
    "    y_pred = np.dot(x, W)\n",
    "    print(\"{}: {}\".format(x[:2], activation(y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment analysis with Scikit-learn\n",
    "\n",
    "Predict album review sentiment from the [Kaggle Pitchfork dataset](https://www.kaggle.com/nolanbconaway/pitchfork-data/data).\n",
    "\n",
    "Need to install three new libraries for this section and the following ones:\n",
    "\n",
    "```\n",
    "conda install keras tensorflow python-slugify\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Readers and utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PKL_PATTERN = r'(?!\\.)[\\w\\s\\d\\-]+\\.pickle'\n",
    "\n",
    "class SqliteCorpusReader(object):\n",
    "\n",
    "    def __init__(self, path):\n",
    "        self._cur = sqlite3.connect(path).cursor()\n",
    "\n",
    "    def scores(self):\n",
    "        \"\"\"\n",
    "        Returns the review score\n",
    "        \"\"\"\n",
    "        self._cur.execute(\"SELECT score FROM reviews\")\n",
    "        scores = self._cur.fetchall()\n",
    "        for score in scores:\n",
    "            yield score\n",
    "\n",
    "    def texts(self):\n",
    "        \"\"\"\n",
    "        Returns the full review texts\n",
    "        \"\"\"\n",
    "        self._cur.execute(\"SELECT content FROM content\")\n",
    "        texts = self._cur.fetchall()\n",
    "        for text in texts:\n",
    "            yield text\n",
    "\n",
    "    def ids(self):\n",
    "        \"\"\"\n",
    "        Returns the review ids\n",
    "        \"\"\"\n",
    "        self._cur.execute(\"SELECT reviewid FROM content\")\n",
    "        ids = self._cur.fetchall()\n",
    "        for idx in ids:\n",
    "            yield idx\n",
    "\n",
    "    def ids_and_texts(self):\n",
    "        \"\"\"\n",
    "        Returns the review ids\n",
    "        \"\"\"\n",
    "        self._cur.execute(\"SELECT * FROM content\")\n",
    "        results = self._cur.fetchall()\n",
    "        for idx,text in results:\n",
    "            yield idx,text\n",
    "\n",
    "    def scores_albums_artists_texts(self):\n",
    "        \"\"\"\n",
    "        Returns a generator with each review represented as a\n",
    "        (score, album name, artist name, review text) tuple\n",
    "        \"\"\"\n",
    "        sql = \"\"\"\n",
    "              SELECT S.score, L.label, A.artist, R.content\n",
    "              FROM [reviews] S\n",
    "              JOIN labels L ON S.reviewid=L.reviewid\n",
    "              JOIN artists A on L.reviewid=A.reviewid\n",
    "              JOIN content R ON A.reviewid=R.reviewid\n",
    "              \"\"\"\n",
    "        self._cur.execute(sql)\n",
    "        results = self._cur.fetchall()\n",
    "        for score,album,band,text in results:\n",
    "            yield (score,album,band,text)\n",
    "\n",
    "    def albums(self):\n",
    "        \"\"\"\n",
    "        Returns the names of albums being reviewed\n",
    "        \"\"\"\n",
    "        self._cur.execute(\"SELECT * FROM labels\")\n",
    "        albums = self._cur.fetchall()\n",
    "        for idx,album in albums:\n",
    "            yield idx,album\n",
    "\n",
    "    def artists(self):\n",
    "        \"\"\"\n",
    "        Returns the name of the artist being reviewed\n",
    "        \"\"\"\n",
    "        self._cur.execute(\"SELECT * FROM artists\")\n",
    "        artists = self._cur.fetchall()\n",
    "        for idx,artist in artists:\n",
    "            yield idx,artist\n",
    "\n",
    "    def genres(self):\n",
    "        \"\"\"\n",
    "        Returns the music genre of each review\n",
    "        \"\"\"\n",
    "        self._cur.execute(\"SELECT * FROM genres\")\n",
    "        genres = self._cur.fetchall()\n",
    "        for idx,genre in genres:\n",
    "            yield idx,genre\n",
    "\n",
    "    def years(self):\n",
    "        \"\"\"\n",
    "        Returns the publication year of each review\n",
    "\n",
    "        Note: There are many missing values\n",
    "        \"\"\"\n",
    "        self._cur.execute(\"SELECT * FROM years\")\n",
    "        years = self._cur.fetchall()\n",
    "        for idx,year in years:\n",
    "            yield idx,year\n",
    "\n",
    "    def paras(self):\n",
    "        \"\"\"\n",
    "        Returns a generator of paragraphs.\n",
    "        \"\"\"\n",
    "        for text in self.texts():\n",
    "            for paragraph in text:\n",
    "                yield paragraph\n",
    "\n",
    "    def sents(self):\n",
    "        \"\"\"\n",
    "        Returns a generator of sentences.\n",
    "        \"\"\"\n",
    "        for para in self.paras():\n",
    "            for sentence in nltk.sent_tokenize(para):\n",
    "                yield sentence\n",
    "\n",
    "    def words(self):\n",
    "        \"\"\"\n",
    "        Returns a generator of words.\n",
    "        \"\"\"\n",
    "        for sent in self.sents():\n",
    "            for word in nltk.wordpunct_tokenize(sent):\n",
    "                yield word\n",
    "\n",
    "    def tagged_tokens(self):\n",
    "        for sent in self.sents():\n",
    "            for word in nltk.wordpunct_tokenize(sent):\n",
    "                yield nltk.pos_tag(word)\n",
    "\n",
    "\n",
    "\n",
    "class PickledReviewsReader(CorpusReader):\n",
    "    def __init__(self, root, fileids=PKL_PATTERN, **kwargs):\n",
    "        \"\"\"\n",
    "        Initialize the corpus reader\n",
    "        \"\"\"\n",
    "        CorpusReader.__init__(self, root, fileids, **kwargs)\n",
    "\n",
    "    def texts_scores(self, fileids=None):\n",
    "        \"\"\"\n",
    "        Returns the document loaded from a pickled object for every file in\n",
    "        the corpus. Similar to the SqliteCorpusReader, this uses a generator\n",
    "        to achieve memory safe iteration.\n",
    "        \"\"\"\n",
    "        # Create a generator, loading one document into memory at a time.\n",
    "        for path, enc, fileid in self.abspaths(fileids, True, True):\n",
    "            with open(path, 'rb') as f:\n",
    "                yield pickle.load(f)\n",
    "\n",
    "    def reviews(self, fileids=None):\n",
    "        \"\"\"\n",
    "        Returns a generator of paragraphs where each paragraph is a list of\n",
    "        sentences, which is in turn a list of (token, tag) tuples.\n",
    "        \"\"\"\n",
    "        for text,score in self.texts_scores(fileids):\n",
    "            yield text\n",
    "\n",
    "    def scores(self, fileids=None):\n",
    "        \"\"\"\n",
    "        Return the scores\n",
    "        \"\"\"\n",
    "        for text,score in self.texts_scores(fileids):\n",
    "            yield score\n",
    "\n",
    "    def paras(self, fileids=None):\n",
    "        \"\"\"\n",
    "        Returns a generator of paragraphs where each paragraph is a list of\n",
    "        sentences, which is in turn a list of (token, tag) tuples.\n",
    "        \"\"\"\n",
    "        for review in self.reviews(fileids):\n",
    "            for paragraph in review:\n",
    "                yield paragraph\n",
    "\n",
    "    def sents(self, fileids=None):\n",
    "        \"\"\"\n",
    "        Returns a generator of sentences where each sentence is a list of\n",
    "        (token, tag) tuples.\n",
    "        \"\"\"\n",
    "        for paragraph in self.paras(fileids):\n",
    "            for sentence in paragraph:\n",
    "                yield sentence\n",
    "\n",
    "    def tagged(self, fileids=None):\n",
    "        for sent in self.sents(fileids):\n",
    "            for token in sent:\n",
    "                yield token\n",
    "\n",
    "    def words(self, fileids=None):\n",
    "        \"\"\"\n",
    "        Returns a generator of (token, tag) tuples.\n",
    "        \"\"\"\n",
    "        for token in self.tagged(fileids):\n",
    "            yield token[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextNormalizer(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, language='english'):\n",
    "        self.stopwords  = set(nltk.corpus.stopwords.words(language))\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    def is_punct(self, token):\n",
    "        return all(\n",
    "            unicodedata.category(char).startswith('P') for char in token\n",
    "        )\n",
    "\n",
    "    def is_stopword(self, token):\n",
    "        return token.lower() in self.stopwords\n",
    "\n",
    "    def normalize(self, document):\n",
    "        return [\n",
    "            self.lemmatize(token, tag).lower()\n",
    "            for sentence in document\n",
    "            for (token, tag) in sentence\n",
    "            if not self.is_punct(token)\n",
    "               and not self.is_stopword(token)\n",
    "        ]\n",
    "\n",
    "    def lemmatize(self, token, pos_tag):\n",
    "        tag = {\n",
    "            'N': wn.NOUN,\n",
    "            'V': wn.VERB,\n",
    "            'R': wn.ADV,\n",
    "            'J': wn.ADJ\n",
    "        }.get(pos_tag[0], wn.NOUN)\n",
    "\n",
    "        return self.lemmatizer.lemmatize(token, tag)\n",
    "\n",
    "    def fit(self, documents, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, documents):\n",
    "        return [\n",
    "            ' '.join(self.normalize(doc)) for doc in documents\n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessor(object):\n",
    "    \"\"\"\n",
    "    The preprocessor wraps a SqliteCorpusReader and manages the stateful\n",
    "    tokenization and part of speech tagging into a directory that is stored\n",
    "    in a format that can be read by the `PickledCorpusReader`. This format\n",
    "    is more compact and necessarily removes a variety of fields from the\n",
    "    document that are stored in Sqlite database. This format however is more\n",
    "    easily accessed for common parsing activity.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, corpus, target=None, **kwargs):\n",
    "        \"\"\"\n",
    "        The corpus is the `SqliteCorpusReader` to preprocess and pickle.\n",
    "        The target is the directory on disk to output the pickled corpus to.\n",
    "        \"\"\"\n",
    "        self.tasks = mp.cpu_count()\n",
    "        self.corpus = corpus\n",
    "        self.target = target\n",
    "\n",
    "    @property\n",
    "    def target(self):\n",
    "        return self._target\n",
    "\n",
    "    @target.setter\n",
    "    def target(self, path):\n",
    "        if path is not None:\n",
    "            # Normalize the path and make it absolute\n",
    "            path = os.path.expanduser(path)\n",
    "            path = os.path.expandvars(path)\n",
    "            path = os.path.abspath(path)\n",
    "\n",
    "            if os.path.exists(path):\n",
    "                if not os.path.isdir(path):\n",
    "                    raise ValueError(\n",
    "                        \"Please supply a directory to write preprocessed data to.\"\n",
    "                    )\n",
    "\n",
    "        self._target = path\n",
    "\n",
    "    def abspath(self, name):\n",
    "        \"\"\"\n",
    "        Returns the absolute path to the target fileid from the corpus fileid.\n",
    "        \"\"\"\n",
    "        # Create the pickle file extension\n",
    "        fname  = str(name) + '.pickle'\n",
    "\n",
    "        # Return the path to the file relative to the target.\n",
    "        return os.path.normpath(os.path.join(self.target, fname))\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        \"\"\"\n",
    "        Segments, tokenizes, and tags a document in the corpus. Returns a\n",
    "        generator of paragraphs, which are lists of sentences, which in turn\n",
    "        are lists of part of speech tagged words.\n",
    "        \"\"\"\n",
    "        yield [\n",
    "            nltk.pos_tag(nltk.wordpunct_tokenize(sent))\n",
    "            for sent in nltk.sent_tokenize(text)\n",
    "        ]\n",
    "\n",
    "    def process(self, score_album_artist_text):\n",
    "        \"\"\"\n",
    "        For a single file does the following preprocessing work:\n",
    "            1. Checks the location on disk to make sure no errors occur.\n",
    "            2. Gets all paragraphs for the given text.\n",
    "            3. Segments the paragraphs with the sent_tokenizer\n",
    "            4. Tokenizes the sentences with the wordpunct_tokenizer\n",
    "            5. Tags the sentences using the default pos_tagger\n",
    "            6. Writes the document as a pickle to the target location.\n",
    "        This method is called multiple times from the transform runner.\n",
    "        \"\"\"\n",
    "        score, album, artist, text = score_album_artist_text\n",
    "\n",
    "        # Compute the outpath to write the file to.\n",
    "        if album:\n",
    "            name = album+'-'+artist\n",
    "        else:\n",
    "            name = artist\n",
    "        target = self.abspath(slugify(name))\n",
    "        parent = os.path.dirname(target)\n",
    "\n",
    "        # Make sure the directory exists\n",
    "        if not os.path.exists(parent):\n",
    "            os.makedirs(parent)\n",
    "\n",
    "        # Make sure that the parent is a directory and not a file\n",
    "        if not os.path.isdir(parent):\n",
    "            raise ValueError(\n",
    "                \"Please supply a directory to write preprocessed data to.\"\n",
    "            )\n",
    "\n",
    "        # Create a data structure for the pickle\n",
    "        document = list(self.tokenize(text))\n",
    "        document.append(score)\n",
    "\n",
    "        # Open and serialize the pickle to disk\n",
    "        with open(target, 'wb') as f:\n",
    "            pickle.dump(document, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "        # Clean up the document\n",
    "        del document\n",
    "\n",
    "        # Return the target fileid\n",
    "        return target\n",
    "\n",
    "    def transform(self):\n",
    "        \"\"\"\n",
    "        Transform the wrapped corpus, writing out the segmented, tokenized,\n",
    "        and part of speech tagged corpus as a pickle to the target directory.\n",
    "        This method will also directly copy files that are in the corpus.root\n",
    "        directory that are not matched by the corpus.fileids().\n",
    "        \"\"\"\n",
    "        # Make the target directory if it doesn't already exist\n",
    "        if not os.path.exists(self.target):\n",
    "            os.makedirs(self.target)\n",
    "\n",
    "        for score_album_artist_text in self.corpus.scores_albums_artists_texts():\n",
    "            yield self.process(score_album_artist_text)\n",
    "\n",
    "\n",
    "class ParallelPreprocessor(Preprocessor):\n",
    "    \"\"\"\n",
    "    Implements multiprocessing to speed up the preprocessing efforts.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Get parallel-specific arguments and then call super.\n",
    "        \"\"\"\n",
    "        self.tasks = mp.cpu_count()\n",
    "        super(ParallelPreprocessor, self).__init__(*args, **kwargs)\n",
    "\n",
    "    def on_result(self, result):\n",
    "        \"\"\"\n",
    "        Appends the results to the master results list.\n",
    "        \"\"\"\n",
    "        self.results.append(result)\n",
    "\n",
    "    def transform(self):\n",
    "        \"\"\"\n",
    "        Create a pool using the multiprocessing library, passing in\n",
    "        the number of cores available to set the desired number of\n",
    "        processes.\n",
    "        \"\"\"\n",
    "        # Make the target directory if it doesn't already exist\n",
    "        if not os.path.exists(self.target):\n",
    "            os.makedirs(self.target)\n",
    "\n",
    "        # Reset the results\n",
    "        self.results = []\n",
    "\n",
    "        # Create a multiprocessing pool\n",
    "        pool  = mp.Pool(processes=self.tasks)\n",
    "        tasks = [\n",
    "            pool.apply_async(self.process, (score_album_artist_text,), callback=self.on_result)\n",
    "            for score_album_artist_text in self.corpus.scores_albums_artists_texts()\n",
    "        ]\n",
    "\n",
    "        # Close the pool and join\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "\n",
    "        return self.results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 µs, sys: 1 µs, total: 5 µs\n",
      "Wall time: 7.87 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## This is slow(ish) - takes c. an hour and a half\n",
    "# Download the data from https://www.kaggle.com/nolanbconaway/pitchfork-data/data\n",
    "# Preprocess the reviews corpus\n",
    "if preproc:\n",
    "    corpus = SqliteCorpusReader(sql_file)\n",
    "    transformer = Preprocessor(corpus, pickle_dir)\n",
    "    docs = transformer.transform()\n",
    "    print(len(list(docs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validate review sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scoring function and utilities\n",
    "def documents(corpus):\n",
    "    return list(corpus.reviews())\n",
    "\n",
    "def make_continuous(corpus):\n",
    "    return list(corpus.scores())\n",
    "\n",
    "def make_categorical(corpus):\n",
    "    \"\"\"\n",
    "    terrible : 0.0 < y <= 3.0\n",
    "    okay     : 3.0 < y <= 5.0\n",
    "    great    : 5.0 < y <= 7.0\n",
    "    amazing  : 7.0 < y <= 10.1\n",
    "    :param corpus:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    return np.digitize(make_continuous(corpus), [0.0, 3.0, 5.0, 7.0, 10.1])\n",
    "\n",
    "def train_model(path, model, continuous=True, saveto=None, cv=4):\n",
    "    \"\"\"\n",
    "    Trains model from corpus at specified path; constructing cross-validation\n",
    "    scores using the cv parameter, then fitting the model on the full data and\n",
    "    writing it to disk at the saveto path if specified. Returns the scores.\n",
    "    \"\"\"\n",
    "    # Load the corpus data and labels for classification\n",
    "    corpus = PickledReviewsReader(path)\n",
    "    X = documents(corpus)\n",
    "    if continuous:\n",
    "        y = make_continuous(corpus)\n",
    "        scoring = 'r2'\n",
    "    else:\n",
    "        y = make_categorical(corpus)\n",
    "        scoring = 'f1_weighted'\n",
    "\n",
    "    # Compute cross validation scores\n",
    "    scores = cross_val_score(model, X, y, cv=cv, scoring=scoring)\n",
    "\n",
    "    # Write to disk if specified\n",
    "    if saveto:\n",
    "        # Fit the model on entire data set\n",
    "        model.fit(X, y)\n",
    "        joblib.dump(model, saveto)\n",
    "\n",
    "    # Return scores as well as training time via decorator\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training discreet model ...\n",
      "Iteration 1, loss = 0.93580851\n",
      "Iteration 2, loss = 0.49681042\n",
      "Iteration 3, loss = 0.13560381\n",
      "Iteration 4, loss = 0.02530439\n",
      "Iteration 5, loss = 0.00604412\n",
      "Iteration 6, loss = 0.00383449\n",
      "Iteration 7, loss = 0.00323782\n",
      "Iteration 8, loss = 0.00289548\n",
      "Iteration 9, loss = 0.00268886\n",
      "Iteration 10, loss = 0.00253897\n",
      "Iteration 11, loss = 0.00245177\n",
      "Iteration 12, loss = 0.00235821\n",
      "Iteration 13, loss = 0.00230173\n",
      "Iteration 14, loss = 0.00226554\n",
      "Iteration 15, loss = 0.00217869\n",
      "Iteration 16, loss = 0.00212844\n",
      "Iteration 17, loss = 0.00208369\n",
      "Iteration 18, loss = 0.00203649\n",
      "Iteration 19, loss = 0.00198492\n",
      "Iteration 20, loss = 0.00193895\n",
      "Iteration 21, loss = 0.00190184\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.89090957\n",
      "Iteration 2, loss = 0.42506844\n",
      "Iteration 3, loss = 0.08429308\n",
      "Iteration 4, loss = 0.01512366\n",
      "Iteration 5, loss = 0.00512944\n",
      "Iteration 6, loss = 0.00368741\n",
      "Iteration 7, loss = 0.00317153\n",
      "Iteration 8, loss = 0.00294297\n",
      "Iteration 9, loss = 0.00280175\n",
      "Iteration 10, loss = 0.00274446\n",
      "Iteration 11, loss = 0.00264398\n",
      "Iteration 12, loss = 0.00255434\n",
      "Iteration 13, loss = 0.00250793\n",
      "Iteration 14, loss = 0.00244126\n",
      "Iteration 15, loss = 0.00237029\n",
      "Iteration 16, loss = 0.00234937\n",
      "Iteration 17, loss = 0.00240384\n",
      "Iteration 18, loss = 0.00225222\n",
      "Iteration 19, loss = 0.00226362\n",
      "Iteration 20, loss = 0.00216994\n",
      "Iteration 21, loss = 0.00215454\n",
      "Iteration 22, loss = 0.00210262\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.89304305\n",
      "Iteration 2, loss = 0.44467397\n",
      "Iteration 3, loss = 0.11163544\n",
      "Iteration 4, loss = 0.02297057\n",
      "Iteration 5, loss = 0.00670036\n",
      "Iteration 6, loss = 0.00450066\n",
      "Iteration 7, loss = 0.00367288\n",
      "Iteration 8, loss = 0.00332910\n",
      "Iteration 9, loss = 0.00308892\n",
      "Iteration 10, loss = 0.00298834\n",
      "Iteration 11, loss = 0.00283855\n",
      "Iteration 12, loss = 0.00273944\n",
      "Iteration 13, loss = 0.00269766\n",
      "Iteration 14, loss = 0.00257996\n",
      "Iteration 15, loss = 0.00251207\n",
      "Iteration 16, loss = 0.00249491\n",
      "Iteration 17, loss = 0.00241078\n",
      "Iteration 18, loss = 0.00240432\n",
      "Iteration 19, loss = 0.00240400\n",
      "Iteration 20, loss = 0.00226450\n",
      "Iteration 21, loss = 0.00223256\n",
      "Iteration 22, loss = 0.00221881\n",
      "Iteration 23, loss = 0.00222665\n",
      "Iteration 24, loss = 0.00221506\n",
      "Iteration 25, loss = 0.00209555\n",
      "Iteration 26, loss = 0.00204699\n",
      "Iteration 27, loss = 0.00209135\n",
      "Iteration 28, loss = 0.00194987\n",
      "Iteration 29, loss = 0.00204757\n",
      "Iteration 30, loss = 0.00200017\n",
      "Iteration 31, loss = 0.00190085\n",
      "Iteration 32, loss = 0.00186193\n",
      "Iteration 33, loss = 0.00180368\n",
      "Iteration 34, loss = 0.00181606\n",
      "Iteration 35, loss = 0.00182364\n",
      "Iteration 36, loss = 0.00172377\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.89029797\n",
      "Iteration 2, loss = 0.44059284\n",
      "Iteration 3, loss = 0.11439647\n",
      "Iteration 4, loss = 0.02527888\n",
      "Iteration 5, loss = 0.00707954\n",
      "Iteration 6, loss = 0.00404788\n",
      "Iteration 7, loss = 0.00336068\n",
      "Iteration 8, loss = 0.00304189\n",
      "Iteration 9, loss = 0.00287262\n",
      "Iteration 10, loss = 0.00276127\n",
      "Iteration 11, loss = 0.00266604\n",
      "Iteration 12, loss = 0.00259106\n",
      "Iteration 13, loss = 0.00251757\n",
      "Iteration 14, loss = 0.00247456\n",
      "Iteration 15, loss = 0.00241325\n",
      "Iteration 16, loss = 0.00234178\n",
      "Iteration 17, loss = 0.00227123\n",
      "Iteration 18, loss = 0.00223783\n",
      "Iteration 19, loss = 0.00219732\n",
      "Iteration 20, loss = 0.00215152\n",
      "Iteration 21, loss = 0.00211957\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.86602343\n",
      "Iteration 2, loss = 0.36986581\n",
      "Iteration 3, loss = 0.07541835\n",
      "Iteration 4, loss = 0.01439183\n",
      "Iteration 5, loss = 0.00529521\n",
      "Iteration 6, loss = 0.00404606\n",
      "Iteration 7, loss = 0.00350362\n",
      "Iteration 8, loss = 0.00322018\n",
      "Iteration 9, loss = 0.00308122\n",
      "Iteration 10, loss = 0.00295479\n",
      "Iteration 11, loss = 0.00285480\n",
      "Iteration 12, loss = 0.00277327\n",
      "Iteration 13, loss = 0.00266739\n",
      "Iteration 14, loss = 0.00261770\n",
      "Iteration 15, loss = 0.00252261\n",
      "Iteration 16, loss = 0.00246731\n",
      "Iteration 17, loss = 0.00242486\n",
      "Iteration 18, loss = 0.00236802\n",
      "Iteration 19, loss = 0.00235705\n",
      "Iteration 20, loss = 0.00223447\n",
      "Iteration 21, loss = 0.00217718\n",
      "Iteration 22, loss = 0.00213157\n",
      "Iteration 23, loss = 0.00209048\n",
      "Iteration 24, loss = 0.00204104\n",
      "Iteration 25, loss = 0.00198063\n",
      "Iteration 26, loss = 0.00193630\n",
      "Iteration 27, loss = 0.00195163\n",
      "Iteration 28, loss = 0.00188261\n",
      "Iteration 29, loss = 0.00185498\n",
      "Iteration 30, loss = 0.00184254\n",
      "Iteration 31, loss = 0.00186920\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training complete.\n",
      "Accuracy on slice #1: 0.6825566868905647.\n",
      "Accuracy on slice #2: 0.6805832791681988.\n",
      "Accuracy on slice #3: 0.6780520620501489.\n",
      "Accuracy on slice #4: 0.6689786415749301.\n",
      "Model saved to ../data/pitchfork/pitchfork_clf.pkl.\n",
      "CPU times: user 4h 27min 35s, sys: 3h 22min 12s, total: 7h 49min 47s\n",
      "Wall time: 4h 39min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Set up processed review reader\n",
    "reader = PickledReviewsReader(pickle_dir)\n",
    "\n",
    "# Perform sentiment scoring\n",
    "pipeline = Pipeline([\n",
    "    ('norm', TextNormalizer()),\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('ann', MLPClassifier(hidden_layer_sizes=[500,150], verbose=True))\n",
    "])\n",
    "\n",
    "print(\"Start training discreet model ...\")\n",
    "scores = train_model(pickle_dir, pipeline, continuous=False, saveto=pitchfork_model)\n",
    "\n",
    "print(\"Training complete.\")\n",
    "for idx, score in enumerate(scores):\n",
    "    print(\"Accuracy on slice #{}: {}.\".format((idx+1), score))\n",
    "print(\"Model saved to {}.\".format(pitchfork_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training regression model ...\n",
      "Iteration 1, loss = 7.29345703\n",
      "Iteration 2, loss = 0.82019703\n",
      "Iteration 3, loss = 0.42778504\n",
      "Iteration 4, loss = 0.19981920\n",
      "Iteration 5, loss = 0.08335059\n",
      "Iteration 6, loss = 0.03419112\n",
      "Iteration 7, loss = 0.01846326\n",
      "Iteration 8, loss = 0.01442684\n",
      "Iteration 9, loss = 0.01388839\n",
      "Iteration 10, loss = 0.01423167\n",
      "Iteration 11, loss = 0.01470336\n",
      "Iteration 12, loss = 0.01510508\n",
      "Iteration 13, loss = 0.01523300\n",
      "Iteration 14, loss = 0.01516401\n",
      "Iteration 15, loss = 0.01513121\n",
      "Iteration 16, loss = 0.01534073\n",
      "Iteration 17, loss = 0.01543284\n",
      "Iteration 18, loss = 0.01519092\n",
      "Iteration 19, loss = 0.01506917\n",
      "Iteration 20, loss = 0.01520778\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 5.96033903\n",
      "Iteration 2, loss = 0.74651895\n",
      "Iteration 3, loss = 0.37564046\n",
      "Iteration 4, loss = 0.17460223\n",
      "Iteration 5, loss = 0.07300537\n",
      "Iteration 6, loss = 0.03271803\n",
      "Iteration 7, loss = 0.02076787\n",
      "Iteration 8, loss = 0.01781107\n",
      "Iteration 9, loss = 0.01758007\n",
      "Iteration 10, loss = 0.01771487\n",
      "Iteration 11, loss = 0.01792449\n",
      "Iteration 12, loss = 0.01704071\n",
      "Iteration 13, loss = 0.01603231\n",
      "Iteration 14, loss = 0.01526822\n",
      "Iteration 15, loss = 0.01522691\n",
      "Iteration 16, loss = 0.01469357\n",
      "Iteration 17, loss = 0.01446169\n",
      "Iteration 18, loss = 0.01443718\n",
      "Iteration 19, loss = 0.01475949\n",
      "Iteration 20, loss = 0.01475393\n",
      "Iteration 21, loss = 0.01479736\n",
      "Iteration 22, loss = 0.01475281\n",
      "Iteration 23, loss = 0.01478787\n",
      "Iteration 24, loss = 0.01486316\n",
      "Iteration 25, loss = 0.01463555\n",
      "Iteration 26, loss = 0.01402522\n",
      "Iteration 27, loss = 0.01566602\n",
      "Iteration 28, loss = 0.01644519\n",
      "Iteration 29, loss = 0.01560039\n",
      "Iteration 30, loss = 0.01485659\n",
      "Iteration 31, loss = 0.01401161\n",
      "Iteration 32, loss = 0.01442327\n",
      "Iteration 33, loss = 0.01452401\n",
      "Iteration 34, loss = 0.01400107\n",
      "Iteration 35, loss = 0.01361682\n",
      "Iteration 36, loss = 0.01297449\n",
      "Iteration 37, loss = 0.01278789\n",
      "Iteration 38, loss = 0.01281109\n",
      "Iteration 39, loss = 0.01254502\n",
      "Iteration 40, loss = 0.01231672\n",
      "Iteration 41, loss = 0.01214242\n",
      "Iteration 42, loss = 0.01230932\n",
      "Iteration 43, loss = 0.01232255\n",
      "Iteration 44, loss = 0.01265050\n",
      "Iteration 45, loss = 0.01272532\n",
      "Iteration 46, loss = 0.01322614\n",
      "Iteration 47, loss = 0.01318143\n",
      "Iteration 48, loss = 0.01291264\n",
      "Iteration 49, loss = 0.01301652\n",
      "Iteration 50, loss = 0.01350145\n",
      "Iteration 51, loss = 0.01385910\n",
      "Iteration 52, loss = 0.01343136\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 5.95468133\n",
      "Iteration 2, loss = 0.76106030\n",
      "Iteration 3, loss = 0.37612113\n",
      "Iteration 4, loss = 0.15461668\n",
      "Iteration 5, loss = 0.05718054\n",
      "Iteration 6, loss = 0.02578676\n",
      "Iteration 7, loss = 0.01696027\n",
      "Iteration 8, loss = 0.01446284\n",
      "Iteration 9, loss = 0.01399751\n",
      "Iteration 10, loss = 0.01446986\n",
      "Iteration 11, loss = 0.01534867\n",
      "Iteration 12, loss = 0.01574405\n",
      "Iteration 13, loss = 0.01602504\n",
      "Iteration 14, loss = 0.01591637\n",
      "Iteration 15, loss = 0.01536291\n",
      "Iteration 16, loss = 0.01511314\n",
      "Iteration 17, loss = 0.01480679\n",
      "Iteration 18, loss = 0.01486018\n",
      "Iteration 19, loss = 0.01482889\n",
      "Iteration 20, loss = 0.01438835\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 6.40720825\n",
      "Iteration 2, loss = 0.78050800\n",
      "Iteration 3, loss = 0.38700666\n",
      "Iteration 4, loss = 0.14767659\n",
      "Iteration 5, loss = 0.05669950\n",
      "Iteration 6, loss = 0.02624397\n",
      "Iteration 7, loss = 0.01710131\n",
      "Iteration 8, loss = 0.01404383\n",
      "Iteration 9, loss = 0.01344943\n",
      "Iteration 10, loss = 0.01383106\n",
      "Iteration 11, loss = 0.01450719\n",
      "Iteration 12, loss = 0.01490427\n",
      "Iteration 13, loss = 0.01527706\n",
      "Iteration 14, loss = 0.01550293\n",
      "Iteration 15, loss = 0.01537901\n",
      "Iteration 16, loss = 0.01472481\n",
      "Iteration 17, loss = 0.01365305\n",
      "Iteration 18, loss = 0.01342719\n",
      "Iteration 19, loss = 0.01339720\n",
      "Iteration 20, loss = 0.01285730\n",
      "Iteration 21, loss = 0.01298087\n",
      "Iteration 22, loss = 0.01325625\n",
      "Iteration 23, loss = 0.01316852\n",
      "Iteration 24, loss = 0.01318920\n",
      "Iteration 25, loss = 0.01295883\n",
      "Iteration 26, loss = 0.01315346\n",
      "Iteration 27, loss = 0.01315893\n",
      "Iteration 28, loss = 0.01340111\n",
      "Iteration 29, loss = 0.01392216\n",
      "Iteration 30, loss = 0.01439437\n",
      "Iteration 31, loss = 0.01409155\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training complete.\n",
      "Accuracy on slice #1: 0.28648122315246605.\n",
      "Accuracy on slice #2: 0.2923417665404572.\n",
      "Accuracy on slice #3: 0.2269290132361863.\n",
      "Accuracy on slice #4: 0.262246929402256.\n",
      "CPU times: user 3h 34min 27s, sys: 2h 43min 37s, total: 6h 18min 5s\n",
      "Wall time: 3h 36min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Ditto, continuous (regression) model\n",
    "\n",
    "pipeline_reg = Pipeline([\n",
    "    ('norm', TextNormalizer()),\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('ann', MLPRegressor(hidden_layer_sizes=[500,150], verbose=True))\n",
    "])\n",
    "\n",
    "print(\"Start training regression model ...\")\n",
    "scores = train_model(pickle_dir, pipeline_reg, continuous=True, saveto=None)\n",
    "\n",
    "print(\"Training complete.\")\n",
    "for idx, score in enumerate(scores):\n",
    "    print(\"Accuracy on slice #{}: {}.\".format((idx+1), score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep learning with Keras and TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions and settings\n",
    "N_FEATURES = 10000\n",
    "DOC_LEN = 500\n",
    "N_CLASSES = 4\n",
    "\n",
    "def build_nn():\n",
    "    \"\"\"\n",
    "    Create a function that returns a compiled neural network\n",
    "    :return: compiled Keras neural network model\n",
    "    \"\"\"\n",
    "    nn = Sequential()\n",
    "    nn.add(Dense(500, activation='relu', input_shape=(N_FEATURES,)))\n",
    "    nn.add(Dense(150, activation='relu'))\n",
    "    nn.add(Dense(N_CLASSES, activation='softmax'))\n",
    "    nn.compile(\n",
    "        loss='categorical_crossentropy',\n",
    "        optimizer='adam',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return nn\n",
    "\n",
    "def build_lstm():\n",
    "    \"\"\"\n",
    "    LSTM alternative\n",
    "    \"\"\"\n",
    "    lstm = Sequential()\n",
    "    lstm.add(Embedding(N_FEATURES+1, 128, input_length=DOC_LEN))\n",
    "    lstm.add(Dropout(0.4))\n",
    "    lstm.add(LSTM(units=200, recurrent_dropout=0.2, dropout=0.2))\n",
    "    lstm.add(Dropout(0.2))\n",
    "    lstm.add(Dense(N_CLASSES, activation='sigmoid'))\n",
    "    lstm.compile(\n",
    "        loss='categorical_crossentropy', # b/c target vals are 1 or 2\n",
    "        optimizer='adam',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return lstm\n",
    "\n",
    "def train_nn_model(path, model, reader, saveto=None, cv=12, **kwargs):\n",
    "    \"\"\"\n",
    "    Trains model from corpus at specified path;\n",
    "    fitting the model on the full data and\n",
    "    writing it to disk at the saveto directory if specified.\n",
    "    Returns the scores.\n",
    "    \"\"\"\n",
    "    # Load the corpus data and labels for classification\n",
    "    corpus = reader(path)\n",
    "    X = documents(corpus)\n",
    "    y = make_categorical(corpus) # for Pitchfork\n",
    "\n",
    "    # Compute cross validation scores\n",
    "    # mp note: http://scikit-learn.org/stable/faq.html#why-do-i-sometime-get-a-crash-freeze-with-n-jobs-1-under-osx-or-linux\n",
    "    scores = cross_val_score(model, X, y, cv=cv, scoring='accuracy')\n",
    "\n",
    "    # Write to disk if specified\n",
    "    if saveto:\n",
    "        # Fit the model on entire data set\n",
    "        model.fit(X, y)\n",
    "        # have to save the keras part using keras' save method\n",
    "        model.steps[-1][1].model.save(saveto['keras_model'])\n",
    "        model.steps.pop(-1)\n",
    "        # ... and use joblib to save the rest of the pipeline\n",
    "        joblib.dump(model, saveto['sklearn_pipe'])\n",
    "\n",
    "    # Return scores as well as training time via decorator\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/mwilkens/anaconda3/envs/work/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Users/mwilkens/anaconda3/envs/work/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/10\n",
      "10789/10789 [==============================] - 22s 2ms/step - loss: 0.8210 - acc: 0.6506\n",
      "Epoch 2/10\n",
      "10789/10789 [==============================] - 16s 1ms/step - loss: 0.4618 - acc: 0.8145\n",
      "Epoch 3/10\n",
      "10789/10789 [==============================] - 16s 2ms/step - loss: 0.1986 - acc: 0.9321\n",
      "Epoch 4/10\n",
      "10789/10789 [==============================] - 17s 2ms/step - loss: 0.0536 - acc: 0.9892\n",
      "Epoch 5/10\n",
      "10789/10789 [==============================] - 16s 1ms/step - loss: 0.0105 - acc: 0.9986\n",
      "Epoch 6/10\n",
      "10789/10789 [==============================] - 20s 2ms/step - loss: 0.0035 - acc: 0.9994\n",
      "Epoch 7/10\n",
      "10789/10789 [==============================] - 21s 2ms/step - loss: 0.0022 - acc: 0.9994\n",
      "Epoch 8/10\n",
      "10789/10789 [==============================] - 17s 2ms/step - loss: 0.0015 - acc: 0.9995\n",
      "Epoch 9/10\n",
      "10789/10789 [==============================] - 16s 2ms/step - loss: 0.0011 - acc: 0.9999\n",
      "Epoch 10/10\n",
      "10789/10789 [==============================] - 16s 2ms/step - loss: 7.6011e-04 - acc: 0.9999\n",
      "Epoch 1/10\n",
      "10789/10789 [==============================] - 18s 2ms/step - loss: 0.8309 - acc: 0.6510\n",
      "Epoch 2/10\n",
      "10789/10789 [==============================] - 16s 1ms/step - loss: 0.4694 - acc: 0.8085\n",
      "Epoch 3/10\n",
      "10789/10789 [==============================] - 16s 1ms/step - loss: 0.2066 - acc: 0.9328\n",
      "Epoch 4/10\n",
      "10789/10789 [==============================] - 16s 2ms/step - loss: 0.0581 - acc: 0.9847\n",
      "Epoch 5/10\n",
      "10789/10789 [==============================] - 17s 2ms/step - loss: 0.0117 - acc: 0.9984\n",
      "Epoch 6/10\n",
      "10789/10789 [==============================] - 16s 1ms/step - loss: 0.0038 - acc: 0.9993\n",
      "Epoch 7/10\n",
      "10789/10789 [==============================] - 16s 2ms/step - loss: 0.0021 - acc: 0.9995\n",
      "Epoch 8/10\n",
      "10789/10789 [==============================] - 17s 2ms/step - loss: 0.0015 - acc: 0.9997\n",
      "Epoch 9/10\n",
      "10789/10789 [==============================] - 17s 2ms/step - loss: 0.0013 - acc: 0.9997\n",
      "Epoch 10/10\n",
      "10789/10789 [==============================] - 16s 2ms/step - loss: 0.0011 - acc: 0.9997\n",
      "Epoch 1/10\n",
      "10790/10790 [==============================] - 19s 2ms/step - loss: 0.8459 - acc: 0.6336\n",
      "Epoch 2/10\n",
      "10790/10790 [==============================] - 17s 2ms/step - loss: 0.4904 - acc: 0.7971\n",
      "Epoch 3/10\n",
      "10790/10790 [==============================] - 17s 2ms/step - loss: 0.2308 - acc: 0.9207\n",
      "Epoch 4/10\n",
      "10790/10790 [==============================] - 17s 2ms/step - loss: 0.0646 - acc: 0.9858\n",
      "Epoch 5/10\n",
      "10790/10790 [==============================] - 16s 2ms/step - loss: 0.0122 - acc: 0.9983\n",
      "Epoch 6/10\n",
      "10790/10790 [==============================] - 16s 2ms/step - loss: 0.0040 - acc: 0.9992\n",
      "Epoch 7/10\n",
      "10790/10790 [==============================] - 16s 2ms/step - loss: 0.0024 - acc: 0.9994\n",
      "Epoch 8/10\n",
      "10790/10790 [==============================] - 16s 1ms/step - loss: 0.0017 - acc: 0.9997\n",
      "Epoch 9/10\n",
      "10790/10790 [==============================] - 17s 2ms/step - loss: 0.0013 - acc: 0.9997\n",
      "Epoch 10/10\n",
      "10790/10790 [==============================] - 17s 2ms/step - loss: 0.0011 - acc: 0.9997\n",
      "Epoch 1/10\n",
      "10790/10790 [==============================] - 20s 2ms/step - loss: 0.8411 - acc: 0.6460\n",
      "Epoch 2/10\n",
      "10790/10790 [==============================] - 21s 2ms/step - loss: 0.4886 - acc: 0.8027\n",
      "Epoch 3/10\n",
      "10790/10790 [==============================] - 21s 2ms/step - loss: 0.2343 - acc: 0.9178\n",
      "Epoch 4/10\n",
      "10790/10790 [==============================] - 20s 2ms/step - loss: 0.0704 - acc: 0.9823\n",
      "Epoch 5/10\n",
      "10790/10790 [==============================] - 17s 2ms/step - loss: 0.0152 - acc: 0.9979\n",
      "Epoch 6/10\n",
      "10790/10790 [==============================] - 17s 2ms/step - loss: 0.0049 - acc: 0.9990\n",
      "Epoch 7/10\n",
      "10790/10790 [==============================] - 17s 2ms/step - loss: 0.0026 - acc: 0.9992\n",
      "Epoch 8/10\n",
      "10790/10790 [==============================] - 17s 2ms/step - loss: 0.0017 - acc: 0.9998\n",
      "Epoch 9/10\n",
      "10790/10790 [==============================] - 17s 2ms/step - loss: 0.0013 - acc: 0.9998\n",
      "Epoch 10/10\n",
      "10790/10790 [==============================] - 17s 2ms/step - loss: 9.8341e-04 - acc: 0.9998\n",
      "Epoch 1/10\n",
      "14386/14386 [==============================] - 25s 2ms/step - loss: 0.7871 - acc: 0.6613\n",
      "Epoch 2/10\n",
      "14386/14386 [==============================] - 22s 2ms/step - loss: 0.4640 - acc: 0.8108\n",
      "Epoch 3/10\n",
      "14386/14386 [==============================] - 27s 2ms/step - loss: 0.2411 - acc: 0.9132\n",
      "Epoch 4/10\n",
      "14386/14386 [==============================] - 27s 2ms/step - loss: 0.0699 - acc: 0.9834\n",
      "Epoch 5/10\n",
      "14386/14386 [==============================] - 23s 2ms/step - loss: 0.0136 - acc: 0.9979\n",
      "Epoch 6/10\n",
      "14386/14386 [==============================] - 22s 2ms/step - loss: 0.0041 - acc: 0.9990\n",
      "Epoch 7/10\n",
      "14386/14386 [==============================] - 23s 2ms/step - loss: 0.0022 - acc: 0.9997\n",
      "Epoch 8/10\n",
      "14386/14386 [==============================] - 23s 2ms/step - loss: 0.0014 - acc: 0.9998\n",
      "Epoch 9/10\n",
      "14386/14386 [==============================] - 23s 2ms/step - loss: 0.0011 - acc: 0.9998\n",
      "Epoch 10/10\n",
      "14386/14386 [==============================] - 23s 2ms/step - loss: 8.8284e-04 - acc: 0.9998\n",
      "Accuracy on slice #1: 0.6983597442313039.\n",
      "Accuracy on slice #2: 0.6864053377814846.\n",
      "Accuracy on slice #3: 0.7063403781979978.\n",
      "Accuracy on slice #4: 0.7013348164627363.\n",
      "Model saved to ['keras_model', 'sklearn_pipe'].\n",
      "CPU times: user 46min 8s, sys: 41.4 s, total: 46min 50s\n",
      "Wall time: 21min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Work around buggy OpenMP linking on MacOS\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "# Build a Keras Sequential model for the Pitchfork reviews\n",
    "cpath = pickle_dir\n",
    "mpath = {\n",
    "    'keras_model'  : os.path.join(pitchfork_dir, 'keras_nn.h5'),\n",
    "    'sklearn_pipe' : os.path.join(pitchfork_dir, 'nn_pipeline.pkl')\n",
    "}\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('norm', TextNormalizer()),\n",
    "    ('vect', TfidfVectorizer(max_features=N_FEATURES)), # need to control feature count\n",
    "    ('nn', KerasClassifier(build_fn=build_nn, # pass but don't call the function!\n",
    "                           epochs=10,\n",
    "                           batch_size=128))\n",
    "])\n",
    "\n",
    "scores = train_nn_model(cpath, pipeline, PickledReviewsReader, saveto=mpath, cv=4)\n",
    "for idx, score in enumerate(scores):\n",
    "    print('Accuracy on slice #{}: {}.'.format((idx+1), score))\n",
    "print('Model saved to {}.'.format(list(mpath)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
