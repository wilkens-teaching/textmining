{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural networks\n",
    "\n",
    "## Sentiment analysis with Scikit-learn\n",
    "\n",
    "Predict album review sentiment from the [Kaggle Pitchfork dataset](https://www.kaggle.com/nolanbconaway/pitchfork-data/data).\n",
    "\n",
    "Need to install three new libraries:\n",
    "\n",
    "```\n",
    "conda install keras tensorflow python-slugify\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Imports and setup\n",
    "import gensim\n",
    "import multiprocessing as mp\n",
    "import nltk\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import sqlite3\n",
    "import unicodedata\n",
    "\n",
    "from gensim.matutils import sparse2full, full2sparse, full2sparse_clipped, scipy2scipy_clipped\n",
    "from gensim.models.doc2vec import TaggedDocument, Doc2Vec\n",
    "from itertools import groupby\n",
    "from keras.layers import Dense, Dropout, Activation, LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.models import load_model, Sequential\n",
    "from keras.preprocessing import sequence\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus.reader.api import CorpusReader\n",
    "from nltk.chunk import tree2conlltags\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.chunk.regexp import RegexpParser\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neural_network import MLPRegressor, MLPClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from slugify import slugify\n",
    "from unicodedata import category as unicat\n",
    "\n",
    "# Local paths\n",
    "pitchfork_dir = os.path.join('..', 'data', 'pitchfork')\n",
    "sql_file = os.path.join(pitchfork_dir, 'database.sqlite')\n",
    "pickle_dir = os.path.join(pitchfork_dir, 'pickled')\n",
    "pitchfork_model = os.path.join(pitchfork_dir, 'pitchfork_clf.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Readers and utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PKL_PATTERN = r'(?!\\.)[\\w\\s\\d\\-]+\\.pickle'\n",
    "\n",
    "class PickledReviewsReader(CorpusReader):\n",
    "    def __init__(self, root, fileids=PKL_PATTERN, **kwargs):\n",
    "        \"\"\"\n",
    "        Initialize the corpus reader\n",
    "        \"\"\"\n",
    "        CorpusReader.__init__(self, root, fileids, **kwargs)\n",
    "\n",
    "    def texts_scores(self, fileids=None):\n",
    "        \"\"\"\n",
    "        Returns the document loaded from a pickled object for every file in\n",
    "        the corpus. Similar to the SqliteCorpusReader, this uses a generator\n",
    "        to achieve memory safe iteration.\n",
    "        \"\"\"\n",
    "        # Create a generator, loading one document into memory at a time.\n",
    "        for path, enc, fileid in self.abspaths(fileids, True, True):\n",
    "            with open(path, 'rb') as f:\n",
    "                yield pickle.load(f)\n",
    "\n",
    "    def reviews(self, fileids=None):\n",
    "        \"\"\"\n",
    "        Returns a generator of paragraphs where each paragraph is a list of\n",
    "        sentences, which is in turn a list of (token, tag) tuples.\n",
    "        \"\"\"\n",
    "        for text,score in self.texts_scores(fileids):\n",
    "            yield text\n",
    "\n",
    "    def scores(self, fileids=None):\n",
    "        \"\"\"\n",
    "        Return the scores\n",
    "        \"\"\"\n",
    "        for text,score in self.texts_scores(fileids):\n",
    "            yield score\n",
    "\n",
    "    def paras(self, fileids=None):\n",
    "        \"\"\"\n",
    "        Returns a generator of paragraphs where each paragraph is a list of\n",
    "        sentences, which is in turn a list of (token, tag) tuples.\n",
    "        \"\"\"\n",
    "        for review in self.reviews(fileids):\n",
    "            for paragraph in review:\n",
    "                yield paragraph\n",
    "\n",
    "    def sents(self, fileids=None):\n",
    "        \"\"\"\n",
    "        Returns a generator of sentences where each sentence is a list of\n",
    "        (token, tag) tuples.\n",
    "        \"\"\"\n",
    "        for paragraph in self.paras(fileids):\n",
    "            for sentence in paragraph:\n",
    "                yield sentence\n",
    "\n",
    "    def tagged(self, fileids=None):\n",
    "        for sent in self.sents(fileids):\n",
    "            for token in sent:\n",
    "                yield token\n",
    "\n",
    "    def words(self, fileids=None):\n",
    "        \"\"\"\n",
    "        Returns a generator of (token, tag) tuples.\n",
    "        \"\"\"\n",
    "        for token in self.tagged(fileids):\n",
    "            yield token[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextNormalizer(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, language='english'):\n",
    "        self.stopwords  = set(nltk.corpus.stopwords.words(language))\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    def is_punct(self, token):\n",
    "        return all(\n",
    "            unicodedata.category(char).startswith('P') for char in token\n",
    "        )\n",
    "\n",
    "    def is_stopword(self, token):\n",
    "        return token.lower() in self.stopwords\n",
    "\n",
    "    def normalize(self, document):\n",
    "        return [\n",
    "            self.lemmatize(token, tag).lower()\n",
    "            for sentence in document\n",
    "            for (token, tag) in sentence\n",
    "            if not self.is_punct(token)\n",
    "               and not self.is_stopword(token)\n",
    "        ]\n",
    "\n",
    "    def lemmatize(self, token, pos_tag):\n",
    "        tag = {\n",
    "            'N': wn.NOUN,\n",
    "            'V': wn.VERB,\n",
    "            'R': wn.ADV,\n",
    "            'J': wn.ADJ\n",
    "        }.get(pos_tag[0], wn.NOUN)\n",
    "\n",
    "        return self.lemmatizer.lemmatize(token, tag)\n",
    "\n",
    "    def fit(self, documents, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, documents):\n",
    "        return [\n",
    "            ' '.join(self.normalize(doc)) for doc in documents\n",
    "        ]\n",
    "\n",
    "\n",
    "class GensimDoc2Vectorizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, size=5, min_count=3):\n",
    "        \"\"\"\n",
    "        gensim_doc2vec_vectorize\n",
    "        \"\"\"\n",
    "        self.size = size\n",
    "        self.min_count = min_count\n",
    "\n",
    "    def fit(self, documents, labels=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, documents):\n",
    "        docs = [\n",
    "            TaggedDocument(words, ['d{}'.format(idx)])\n",
    "            for idx, words in enumerate(documents)\n",
    "        ]\n",
    "        model = Doc2Vec(docs, size=self.size, min_count=self.min_count)\n",
    "        return np.array(list(model.docvecs))\n",
    "\n",
    "class GensimTfidfVectorizer(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, nfeatures=100, tofull=False):\n",
    "        \"\"\"\n",
    "        Pass in a directory that holds the lexicon in corpus.dict and the\n",
    "        TFIDF model in tfidf.model (for now).\n",
    "\n",
    "        Set tofull = True if the next thing is a Scikit-Learn estimator\n",
    "        otherwise keep False if the next thing is a Gensim model.\n",
    "        \"\"\"\n",
    "        self._lexicon_path = \"lexigram.dict\"\n",
    "        self._tfidf_path = \"tfidf.model\"\n",
    "        self.nfeatures = nfeatures\n",
    "        self.lexicon = None\n",
    "        self.tfidf = None\n",
    "        self.tofull = tofull\n",
    "\n",
    "        self.load()\n",
    "\n",
    "    def load(self):\n",
    "        if os.path.exists(self._lexicon_path):\n",
    "            self.lexicon = gensim.corpora.Dictionary.load(self._lexicon_path)\n",
    "\n",
    "        if os.path.exists(self._tfidf_path):\n",
    "            self.tfidf = gensim.models.TfidfModel().load(self._tfidf_path)\n",
    "\n",
    "    def save(self):\n",
    "        self.lexicon.save(self._lexicon_path)\n",
    "        self.tfidf.save(self._tfidf_path)\n",
    "\n",
    "    def fit(self, documents, labels=None):\n",
    "        self.lexicon = gensim.corpora.Dictionary(documents, prune_at=self.nfeatures)\n",
    "        self.lexicon.filter_extremes(keep_n=self.nfeatures)\n",
    "        self.lexicon.compactify()\n",
    "        self.tfidf = gensim.models.TfidfModel(\n",
    "            [self.lexicon.doc2bow(doc) for doc in documents],\n",
    "            id2word=self.lexicon\n",
    "        )\n",
    "        self.save()\n",
    "        return self\n",
    "\n",
    "    def transform(self, documents):\n",
    "        def generator():\n",
    "            for document in documents:\n",
    "                vec = self.tfidf[self.lexicon.doc2bow(document)]\n",
    "                if self.tofull:\n",
    "                    yield sparse2full(vec, len(self.lexicon))\n",
    "                else:\n",
    "                    yield vec\n",
    "        return np.array(list(generator()))\n",
    "\n",
    "\n",
    "class KeyphraseExtractor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Extract adverbial and adjective phrases, and transform\n",
    "    documents into lists of these keyphrases, with a total\n",
    "    keyphrase lexicon limited by the nfeatures parameter\n",
    "    and a document length limited/padded to doclen\n",
    "    \"\"\"\n",
    "    def __init__(self, nfeatures=100000, doclen=60):\n",
    "        self.grammar = r'KT: {(<RB.> <JJ.*>|<VB.*>|<RB.*>)|(<JJ> <NN.*>)}'\n",
    "        # self.grammar = r'KT: {(<RB.*> <VB.>|<RB.>|<JJ.> <NN.*>)}'\n",
    "        # self.grammar = r'KT: {<RB.>|<JJ.>}'\n",
    "        self.chunker = RegexpParser(self.grammar)\n",
    "        self.nfeatures = nfeatures\n",
    "        self.doclen = doclen\n",
    "\n",
    "    def normalize(self, sent):\n",
    "        \"\"\"\n",
    "        Removes punctuation from a tokenized/tagged sentence and\n",
    "        lowercases words.\n",
    "        \"\"\"\n",
    "        is_punct = lambda word: all(unicat(c).startswith('P') for c in word)\n",
    "        sent = filter(lambda t: not is_punct(t[0]), sent)\n",
    "        sent = map(lambda t: (t[0].lower(), t[1]), sent)\n",
    "        return list(sent)\n",
    "\n",
    "    def extract_candidate_phrases(self, sents):\n",
    "        \"\"\"\n",
    "        For a document, parse sentences using our chunker created by\n",
    "        our grammar, converting the parse tree into a tagged sequence.\n",
    "        Extract phrases, rejoin with a space, and yield the document\n",
    "        represented as a list of it's keyphrases.\n",
    "        \"\"\"\n",
    "        for sent in sents:\n",
    "            sent = self.normalize(sent)\n",
    "            if not sent: continue\n",
    "            chunks = tree2conlltags(self.chunker.parse(sent))\n",
    "            phrases = [\n",
    "                \" \".join(word for word, pos, chunk in group).lower()\n",
    "                for key, group in groupby(\n",
    "                    chunks, lambda term: term[-1] != 'O'\n",
    "                ) if key\n",
    "            ]\n",
    "            for phrase in phrases:\n",
    "                yield phrase\n",
    "\n",
    "    def fit(self, documents, y=None):\n",
    "        return self\n",
    "\n",
    "    def get_lexicon(self, keydocs):\n",
    "        \"\"\"\n",
    "        Build a lexicon of size nfeatures\n",
    "        \"\"\"\n",
    "        keyphrases = [keyphrase for doc in keydocs for keyphrase in doc]\n",
    "        fdist = FreqDist(keyphrases)\n",
    "        counts = fdist.most_common(self.nfeatures)\n",
    "        lexicon = [phrase for phrase, count in counts]\n",
    "        return {phrase: idx+1 for idx, phrase in enumerate(lexicon)}\n",
    "\n",
    "    def clip(self, keydoc, lexicon):\n",
    "        \"\"\"\n",
    "        Remove keyphrases from documents that aren't in the lexicon\n",
    "        \"\"\"\n",
    "        return [lexicon[keyphrase] for keyphrase in keydoc\n",
    "                if keyphrase in lexicon.keys()]\n",
    "\n",
    "    def transform(self, documents):\n",
    "        docs = [list(self.extract_candidate_phrases(doc)) for doc in documents]\n",
    "        lexicon = self.get_lexicon(docs)\n",
    "        clipped = [list(self.clip(doc, lexicon)) for doc in docs]\n",
    "        return sequence.pad_sequences(clipped, maxlen=self.doclen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validate review sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scoring function and utilities\n",
    "def documents(corpus):\n",
    "    return list(corpus.reviews())\n",
    "\n",
    "def continuous(corpus):\n",
    "    return list(corpus.scores())\n",
    "\n",
    "def make_categorical(corpus):\n",
    "    \"\"\"\n",
    "    terrible : 0.0 < y <= 3.0\n",
    "    okay     : 3.0 < y <= 5.0\n",
    "    great    : 5.0 < y <= 7.0\n",
    "    amazing  : 7.0 < y <= 10.1\n",
    "    :param corpus:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    return np.digitize(continuous(corpus), [0.0, 3.0, 5.0, 7.0, 10.1])\n",
    "\n",
    "def train_model(path, model, continuous=True, saveto=None, cv=12):\n",
    "    \"\"\"\n",
    "    Trains model from corpus at specified path; constructing cross-validation\n",
    "    scores using the cv parameter, then fitting the model on the full data and\n",
    "    writing it to disk at the saveto path if specified. Returns the scores.\n",
    "    \"\"\"\n",
    "    # Load the corpus data and labels for classification\n",
    "    corpus = PickledReviewsReader(path)\n",
    "    X = documents(corpus)\n",
    "    if continuous:\n",
    "        y = continuous(corpus)\n",
    "        scoring = 'r2'\n",
    "    else:\n",
    "        y = make_categorical(corpus)\n",
    "        scoring = 'f1_weighted'\n",
    "\n",
    "    # Compute cross validation scores\n",
    "    scores = cross_val_score(model, X, y, cv=cv, scoring=scoring)\n",
    "\n",
    "    # Fit the model on entire data set\n",
    "    model.fit(X, y)\n",
    "\n",
    "    # Write to disk if specified\n",
    "    if saveto:\n",
    "        joblib.dump(model, saveto)\n",
    "\n",
    "    # Return scores as well as training time via decorator\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep learning with Keras and TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions and settings\n",
    "N_FEATURES = 10000\n",
    "DOC_LEN = 500\n",
    "N_CLASSES = 4\n",
    "\n",
    "def build_nn():\n",
    "    \"\"\"\n",
    "    Create a function that returns a compiled neural network\n",
    "    :return: compiled Keras neural network model\n",
    "    \"\"\"\n",
    "    nn = Sequential()\n",
    "    nn.add(Dense(500, activation='relu', input_shape=(N_FEATURES,)))\n",
    "    nn.add(Dense(150, activation='relu'))\n",
    "    nn.add(Dense(N_CLASSES, activation='softmax'))\n",
    "    nn.compile(\n",
    "        loss='categorical_crossentropy',\n",
    "        optimizer='adam',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return nn\n",
    "\n",
    "def build_lstm():\n",
    "    lstm = Sequential()\n",
    "    lstm.add(Embedding(N_FEATURES+1, 128, input_length=DOC_LEN))\n",
    "    lstm.add(Dropout(0.4))\n",
    "    lstm.add(LSTM(units=200, recurrent_dropout=0.2, dropout=0.2))\n",
    "    lstm.add(Dropout(0.2))\n",
    "    lstm.add(Dense(N_CLASSES, activation='sigmoid'))\n",
    "    lstm.compile(\n",
    "        loss='categorical_crossentropy', # b/c target vals are 1 or 2\n",
    "        optimizer='adam',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return lstm\n",
    "\n",
    "def train_nn_model(path, model, reader, saveto=None, cv=12, **kwargs):\n",
    "    \"\"\"\n",
    "    Trains model from corpus at specified path;\n",
    "    fitting the model on the full data and\n",
    "    writing it to disk at the saveto directory if specified.\n",
    "    Returns the scores.\n",
    "    \"\"\"\n",
    "    # Load the corpus data and labels for classification\n",
    "    corpus = reader(path)\n",
    "    X = documents(corpus)\n",
    "    y = make_categorical(corpus) # for Pitchfork\n",
    "\n",
    "    # Compute cross validation scores\n",
    "    # mp note: http://scikit-learn.org/stable/faq.html#why-do-i-sometime-get-a-crash-freeze-with-n-jobs-1-under-osx-or-linux\n",
    "    scores = cross_val_score(model, X, y, cv=cv, scoring='accuracy')\n",
    "\n",
    "    # Fit the model on entire data set\n",
    "    model.fit(X, y)\n",
    "\n",
    "    # Write to disk if specified\n",
    "    if saveto:\n",
    "        # have to save the keras part using keras' save method\n",
    "        model.steps[-1][1].model.save(saveto['keras_model'])\n",
    "        model.steps.pop(-1)\n",
    "        # ... and use joblib to save the rest of the pipeline\n",
    "        joblib.dump(model, saveto['sklearn_pipe'])\n",
    "\n",
    "    # Return scores as well as training time via decorator\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "10789/10789 [==============================] - 21s 2ms/step - loss: 0.8210 - acc: 0.6483\n",
      "Epoch 2/10\n",
      "10789/10789 [==============================] - 19s 2ms/step - loss: 0.4713 - acc: 0.8052\n",
      "Epoch 3/10\n",
      "10789/10789 [==============================] - 18s 2ms/step - loss: 0.2218 - acc: 0.9202\n",
      "Epoch 4/10\n",
      "10789/10789 [==============================] - 17s 2ms/step - loss: 0.0676 - acc: 0.9803\n",
      "Epoch 5/10\n",
      "10789/10789 [==============================] - 17s 2ms/step - loss: 0.0133 - acc: 0.9988\n",
      "Epoch 6/10\n",
      "10789/10789 [==============================] - 17s 2ms/step - loss: 0.0038 - acc: 0.9994\n",
      "Epoch 7/10\n",
      "10789/10789 [==============================] - 17s 2ms/step - loss: 0.0023 - acc: 0.9994\n",
      "Epoch 8/10\n",
      "10789/10789 [==============================] - 21s 2ms/step - loss: 0.0017 - acc: 0.9994\n",
      "Epoch 9/10\n",
      "10789/10789 [==============================] - 17s 2ms/step - loss: 0.0011 - acc: 0.9998\n",
      "Epoch 10/10\n",
      "10789/10789 [==============================] - 17s 2ms/step - loss: 8.0822e-04 - acc: 0.9999\n",
      "Epoch 1/10\n",
      "10789/10789 [==============================] - 22s 2ms/step - loss: 0.8420 - acc: 0.6331\n",
      "Epoch 2/10\n",
      "10789/10789 [==============================] - 17s 2ms/step - loss: 0.5027 - acc: 0.7925\n",
      "Epoch 3/10\n",
      "10789/10789 [==============================] - 18s 2ms/step - loss: 0.2428 - acc: 0.9175\n",
      "Epoch 4/10\n",
      "10789/10789 [==============================] - 17s 2ms/step - loss: 0.0845 - acc: 0.9766\n",
      "Epoch 5/10\n",
      "10789/10789 [==============================] - 17s 2ms/step - loss: 0.0205 - acc: 0.9968\n",
      "Epoch 6/10\n",
      "10789/10789 [==============================] - 17s 2ms/step - loss: 0.0056 - acc: 0.9989\n",
      "Epoch 7/10\n",
      "10789/10789 [==============================] - 17s 2ms/step - loss: 0.0027 - acc: 0.9994\n",
      "Epoch 8/10\n",
      "10789/10789 [==============================] - 17s 2ms/step - loss: 0.0019 - acc: 0.9994\n",
      "Epoch 9/10\n",
      "10789/10789 [==============================] - 17s 2ms/step - loss: 0.0014 - acc: 0.9997\n",
      "Epoch 10/10\n",
      "10789/10789 [==============================] - 17s 2ms/step - loss: 0.0012 - acc: 0.9997\n",
      "Epoch 1/10\n",
      "10790/10790 [==============================] - 19s 2ms/step - loss: 0.8464 - acc: 0.6343\n",
      "Epoch 2/10\n",
      "10790/10790 [==============================] - 18s 2ms/step - loss: 0.4941 - acc: 0.7956\n",
      "Epoch 3/10\n",
      "10790/10790 [==============================] - 17s 2ms/step - loss: 0.2295 - acc: 0.9204\n",
      "Epoch 4/10\n",
      "10790/10790 [==============================] - 17s 2ms/step - loss: 0.0674 - acc: 0.9846\n",
      "Epoch 5/10\n",
      "10790/10790 [==============================] - 17s 2ms/step - loss: 0.0156 - acc: 0.9981\n",
      "Epoch 6/10\n",
      "10790/10790 [==============================] - 17s 2ms/step - loss: 0.0046 - acc: 0.9991\n",
      "Epoch 7/10\n",
      "10790/10790 [==============================] - 18s 2ms/step - loss: 0.0025 - acc: 0.9994\n",
      "Epoch 8/10\n",
      "10790/10790 [==============================] - 17s 2ms/step - loss: 0.0018 - acc: 0.9996\n",
      "Epoch 9/10\n",
      "10790/10790 [==============================] - 19s 2ms/step - loss: 0.0014 - acc: 0.9997\n",
      "Epoch 10/10\n",
      "10790/10790 [==============================] - 18s 2ms/step - loss: 0.0012 - acc: 0.9997\n",
      "Epoch 1/10\n",
      "10790/10790 [==============================] - 19s 2ms/step - loss: 0.8417 - acc: 0.6401\n",
      "Epoch 2/10\n",
      "10790/10790 [==============================] - 17s 2ms/step - loss: 0.4857 - acc: 0.8041\n",
      "Epoch 3/10\n",
      "10790/10790 [==============================] - 17s 2ms/step - loss: 0.2235 - acc: 0.9234\n",
      "Epoch 4/10\n",
      "10790/10790 [==============================] - 17s 2ms/step - loss: 0.0634 - acc: 0.9840\n",
      "Epoch 5/10\n",
      "10790/10790 [==============================] - 17s 2ms/step - loss: 0.0136 - acc: 0.9983\n",
      "Epoch 6/10\n",
      "10790/10790 [==============================] - 17s 2ms/step - loss: 0.0041 - acc: 0.9992\n",
      "Epoch 7/10\n",
      "10790/10790 [==============================] - 20s 2ms/step - loss: 0.0025 - acc: 0.9992\n",
      "Epoch 8/10\n",
      "10790/10790 [==============================] - 18s 2ms/step - loss: 0.0017 - acc: 0.9995\n",
      "Epoch 9/10\n",
      "10790/10790 [==============================] - 18s 2ms/step - loss: 0.0012 - acc: 0.9998\n",
      "Epoch 10/10\n",
      "10790/10790 [==============================] - 18s 2ms/step - loss: 9.5731e-04 - acc: 0.9998\n",
      "Epoch 1/10\n",
      "14386/14386 [==============================] - 26s 2ms/step - loss: 0.8010 - acc: 0.6581\n",
      "Epoch 2/10\n",
      "14386/14386 [==============================] - 24s 2ms/step - loss: 0.4665 - acc: 0.8036\n",
      "Epoch 3/10\n",
      "14386/14386 [==============================] - 23s 2ms/step - loss: 0.2256 - acc: 0.9187\n",
      "Epoch 4/10\n",
      "14386/14386 [==============================] - 23s 2ms/step - loss: 0.0578 - acc: 0.9876\n",
      "Epoch 5/10\n",
      "14386/14386 [==============================] - 23s 2ms/step - loss: 0.0115 - acc: 0.9983\n",
      "Epoch 6/10\n",
      "14386/14386 [==============================] - 27s 2ms/step - loss: 0.0036 - acc: 0.9990\n",
      "Epoch 7/10\n",
      "14386/14386 [==============================] - 25s 2ms/step - loss: 0.0019 - acc: 0.9997\n",
      "Epoch 8/10\n",
      "14386/14386 [==============================] - 24s 2ms/step - loss: 0.0013 - acc: 0.9998\n",
      "Epoch 9/10\n",
      "14386/14386 [==============================] - 24s 2ms/step - loss: 9.7082e-04 - acc: 0.9998\n",
      "Epoch 10/10\n",
      "14386/14386 [==============================] - 25s 2ms/step - loss: 8.1618e-04 - acc: 0.9998\n",
      "Accuracy on slice #1: 0.6922435362802335.\n",
      "Accuracy on slice #2: 0.6894634417570198.\n",
      "Accuracy on slice #3: 0.699666295884316.\n",
      "Accuracy on slice #4: 0.696329254727475.\n",
      "Model saved to ['keras_model', 'sklearn_pipe'].\n",
      "CPU times: user 50min 10s, sys: 1min 9s, total: 51min 20s\n",
      "Wall time: 21min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Build a Keras Sequential model for the Pitchfork reviews\n",
    "cpath = pickle_dir\n",
    "mpath = {\n",
    "    'keras_model'  : os.path.join(pitchfork_dir, 'keras_nn.h5'),\n",
    "    'sklearn_pipe' : os.path.join(pitchfork_dir, 'nn_pipeline.pkl')\n",
    "}\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('norm', TextNormalizer()),\n",
    "    ('vect', TfidfVectorizer(max_features=N_FEATURES)), # need to control feature count\n",
    "    ('nn', KerasClassifier(build_fn=build_nn, # pass but don't call the function!\n",
    "                           epochs=10,\n",
    "                           batch_size=128))\n",
    "])\n",
    "\n",
    "scores = train_nn_model(cpath, pipeline, PickledReviewsReader, saveto=mpath, cv=4)\n",
    "for idx, score in enumerate(scores):\n",
    "    print('Accuracy on slice #{}: {}.'.format((idx+1), score))\n",
    "print('Model saved to {}.'.format(list(mpath)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
