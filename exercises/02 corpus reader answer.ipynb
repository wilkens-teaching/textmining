{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK corpus reader\n",
    "\n",
    "## Background\n",
    "\n",
    "In this exercise, we'll modify the custom NLTK corpus reader introduced in the textbook to accommodate multiple categories for each novel and to work with plaintext documents rather than HTML. But first, let's see how we would use the stock `PlaintextCorpusReader` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "from   nltk.corpus.reader import PlaintextCorpusReader\n",
    "\n",
    "# Where are the corpus texts on your system\n",
    "text_dir = os.path.join('..', 'data', 'texts')\n",
    "\n",
    "# Regex to identify text files\n",
    "text_pattern = '.+\\.txt'\n",
    "\n",
    "# Initialize the corpus reader\n",
    "plain_corpus = PlaintextCorpusReader(text_dir, text_pattern)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Couple of things to notice here:\n",
    "\n",
    "* We've once again used `os.path.join()` to create a file path that's portable across Mac, Windows, etc. Make sure the directories in question are the right ones for wherever you're keeping your copy of the corpus. Note, too, that `'..'` indicates the parent directory of the current directory (wherever the notebook lives). `'.'` (single period rather than double) indicates the current directory.\n",
    "* We use a regular expression to identify the file names that should be included in the corpus. This isn't especially relevant yet, but it will allow us to have files in the corpus directory that *aren't* included in the corpus itself.\n",
    "* We'll say a bit about regular expressions in class. A full treatment is beyond the scope of the course, but there are [many](https://developers.google.com/edu/python/regular-expressions) [overviews](https://docs.python.org/3/howto/regex.html) [online](https://www.datacamp.com/community/tutorials/python-regular-expression-tutorial). In the present case, the `text_pattern` regex will match any string (here, a file name) that ends in `.txt` and contains one or more characters before that suffix.\n",
    "\n",
    "NLTK corpus reader objects have some useful convenience methods, like `.fileids()`..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A-Alcott-Little_Women-1868-F.txt',\n",
       " 'A-Cather-Antonia-1918-F.txt',\n",
       " 'A-Chesnutt-Marrow-1901-M.txt',\n",
       " 'A-Chopin-Awakening-1899-F.txt',\n",
       " 'A-Crane-Maggie-1893-M.txt',\n",
       " 'A-Davis-Life_Iron_mills-1861-F.txt',\n",
       " 'A-Dreiser-Sister_Carrie-1900-M.txt',\n",
       " 'A-Freeman-Pembroke-1894-F.txt',\n",
       " 'A-Gilman-Herland-1915-F.txt',\n",
       " 'A-Harper-Iola_Leroy-1892-F.txt',\n",
       " 'A-Hawthorne-Scarlet_Letter-1850-M.txt',\n",
       " 'A-Howells-Silas_Lapham-1885-M.txt',\n",
       " 'A-James-Golden_Bowl-1904-M.txt',\n",
       " 'A-Jewett-Pointed_Firs-1896-F.txt',\n",
       " 'A-London-Call_Wild-1903-M.txt',\n",
       " 'A-Melville-Moby_Dick-1851-M.txt',\n",
       " 'A-Norris-Pit-1903-M.txt',\n",
       " 'A-Stowe-Uncle_Tom-1852-F.txt',\n",
       " 'A-Twain-Huck_Finn-1885-M.txt',\n",
       " 'A-Wharton-Age_Innocence-1920-F.txt',\n",
       " 'B-Austen-Pride_Prejudice-1813-F.txt',\n",
       " 'B-Bronte_C-Jane_Eyre-1847-F.txt',\n",
       " 'B-Bronte_E-Wuthering_Heights-1847-F.txt',\n",
       " 'B-Burney-Evelina-1778-F.txt',\n",
       " 'B-Conrad-Heart_Darkness-1902-M.txt',\n",
       " 'B-Dickens-Bleak_House-1853-M.txt',\n",
       " 'B-Disraeli-Sybil-1845-M.txt',\n",
       " 'B-Eliot-Middlemarch-1869-F.txt',\n",
       " 'B-Forster-Room_View-1908-M.txt',\n",
       " 'B-Gaskell-North_South-1855-F.txt',\n",
       " 'B-Gissing-Grub_Street-1893-M.txt',\n",
       " 'B-Hardy-Tess-1891-M.txt',\n",
       " 'B-Mitford-Our_Village-1826-F.txt',\n",
       " 'B-Radcliffe-Mysteries_Udolpho-1794-F.txt',\n",
       " 'B-Shelley-Frankenstein-1818-F.txt',\n",
       " 'B-Stevenson-Treasure_Island-1883-M.txt',\n",
       " 'B-Thackeray-Vanity_Fair-1848-M.txt',\n",
       " 'B-Trollope-Live_Now-1875-M.txt',\n",
       " 'B-Wells-Time_Machine-1895-M.txt',\n",
       " 'B-Woolf-Mrs_Dalloway-1925-F.txt']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plain_corpus.fileids()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and `.paras()`. Here, we print the second sentence of the 101st paragraph of *Mrs Dalloway*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['No', 'vulgar', 'jealousy', 'could', 'separate', 'her', 'from', 'Richard', '.']\n"
     ]
    }
   ],
   "source": [
    "print(plain_corpus.paras(fileids=['B-Woolf-Mrs_Dalloway-1925-F.txt'])[100][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the text has been fully parsed into paragraphs, sentences, and words, which are returned as nested lists.\n",
    "\n",
    "NLTK also contains a `CategorizedPlaintextCorpusReader` class, which is similar to `PlaintextCorpusReader`, but handles categories for us as well. Categorized corpus readers allow users to specify category labels in place of the `fileids=` argument, which is useful for subsetting the corpus according to divisions of interest.\n",
    "\n",
    "In many cases, the NLTK built-in corpus readers will be all you need. But we're going to use a modified version of the textbook's `HTMLCorpusReader` class, both because it's instructive to see how `CorpusReader` methods are implemented and because we'll need some non-standard features in future work.\n",
    "\n",
    "## Exercise\n",
    "\n",
    "**NB. Look for '`# YOUR CODE HERE`' comments in the code blocks below to find the spots where you need to make changes or to create code of your own.** In every case, the amount of code for you to add is no more than a few lines.\n",
    "\n",
    "Using the examples in chapter two and the (extensive) starter code below, write a custom NLTK-based corpus reader that ingests our course corpus (40 novels) and labels each of them by the author's gender and national origin. Name your corpus reader `TMNCorpusReader`. When complete, your corpus reader should successfully execute the code examples given further down in the notebook.\n",
    "\n",
    "NLTK corpora support multiple category labels for each text, but there's a limitation to the way it treats these labels: they can only be combined via logical OR, not AND. In other words, if you select multiple categories, you get all the texts that belong any one of them. So we need to label our texts both restrictively (\"American female\", 10 texts) and at higher levels (\"American\", 20 texts).\n",
    "\n",
    "The example in the textbook uses directory structure to assign categories. That specific approach won't work for us, because the same text can belong to multiple categories (we're not going to explore cumbersome work-arounds like symlinks or multiple copies of each file). But our files *are* named in ways that indicate the categories to which they belong. For example:\n",
    "\n",
    "```\n",
    "A-Alcott-Little_Women-1868-F.txt\n",
    "```\n",
    "\n",
    "The format is:\n",
    "\n",
    "```\n",
    "nationality-author-title-year-gender.txt\n",
    "```\n",
    "\n",
    "Multi-word titles or author names are joined with an underscore; fields are separated with a hyphen.\n",
    "\n",
    "So ... we'll use a dictionary to associate multiple labels with each input text, then pass that dictionary to the corpus reader using the `cat_map` keyword."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category labels for _Little Women_: ['A', 'F', 'AF']\n"
     ]
    }
   ],
   "source": [
    "# More imports, in addition to those above\n",
    "from   glob import glob\n",
    "\n",
    "# We're going to read just the file names to create the category map\n",
    "file_paths = glob(os.path.join(text_dir, '*.txt')) # glob lets us use wildcards in paths\n",
    "file_names = [os.path.split(i)[1] for i in file_paths] # split filenames from paths\n",
    "\n",
    "category_map = {} # Dict to hold filename:[categories] mappings\n",
    "\n",
    "for file in file_names:\n",
    "    parsed = file.rstrip('.txt').split('-') # strip extension and split on hyphens\n",
    "    nation = parsed[0]\n",
    "    gender = parsed[4]\n",
    "    category_map[file] = [nation, gender, nation+gender]\n",
    "\n",
    "print(\"Category labels for _Little Women_:\", \n",
    "      category_map['A-Alcott-Little_Women-1868-F.txt'])\n",
    "assert(category_map['A-Alcott-Little_Women-1868-F.txt']==['A', 'F', 'AF'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that each text in the corpus has three labels:\n",
    "\n",
    "* Author nationality\n",
    "* Author gender\n",
    "* Combined nationality and gender\n",
    "\n",
    "This will let us subset the corpus at any desired level of granularity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOC_PATTERN = '.+\\.txt'         # Documents are just files that end in '.txt'\n",
    "CAT_PATTERN = r'([a-z_\\s]+)/.*' # We won't use this, but fall back to directory-based labels\n",
    "                                # if no other labels are supplied\n",
    "\n",
    "import codecs\n",
    "import nltk.data\n",
    "from   nltk.tokenize import *\n",
    "from   nltk.corpus.reader.util import *\n",
    "from   nltk.corpus.reader.api import *\n",
    "from   nltk.corpus.reader import PlaintextCorpusReader\n",
    "\n",
    "class TMNCorpusReader(CategorizedCorpusReader, PlaintextCorpusReader):\n",
    "    \"\"\"\n",
    "    A corpus reader for categorized text documents to enable preprocessing.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        root, \n",
    "        fileids=DOC_PATTERN,\n",
    "        word_tokenizer=WordPunctTokenizer(),\n",
    "        sent_tokenizer=nltk.data.LazyLoader('tokenizers/punkt/english.pickle'),\n",
    "        para_block_reader=read_blankline_block,\n",
    "        encoding='utf8', \n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the corpus reader.  Categorization arguments\n",
    "        (``cat_pattern``, ``cat_map``, and ``cat_file``) are passed to\n",
    "        the ``CategorizedCorpusReader`` constructor.  The remaining\n",
    "        arguments are passed to the ``CorpusReader`` constructor.\n",
    "        \"\"\"\n",
    "        # Add the default category pattern if not passed into the class.\n",
    "        if not any(key.startswith('cat_') for key in kwargs.keys()):\n",
    "            kwargs['cat_pattern'] = CAT_PATTERN\n",
    "\n",
    "        # Initialize the NLTK corpus reader objects\n",
    "        CategorizedCorpusReader.__init__(self, kwargs)\n",
    "        PlaintextCorpusReader.__init__(self, root, fileids, encoding)\n",
    "        self._word_tokenizer = word_tokenizer\n",
    "        self._sent_tokenizer = sent_tokenizer\n",
    "        self._para_block_reader = para_block_reader\n",
    "\n",
    "    def resolve(self, fileids, categories):\n",
    "        \"\"\"\n",
    "        Returns a list of fileids or categories depending on what is passed\n",
    "        to each internal corpus reader function. Implemented similarly to\n",
    "        the NLTK ``CategorizedPlaintextCorpusReader``.\n",
    "        \"\"\"\n",
    "        if fileids is not None and categories is not None:\n",
    "            raise ValueError(\"Specify fileids or categories, not both\")\n",
    "\n",
    "        if categories is not None:\n",
    "            return self.fileids(categories)\n",
    "        return fileids\n",
    "\n",
    "    def docs(self, fileids=None, categories=None):\n",
    "        \"\"\"\n",
    "        Returns the complete text of a document, closing the document\n",
    "        after we are done reading it and yielding it in a memory safe fashion.\n",
    "        \"\"\"\n",
    "        # Resolve the fileids and the categories\n",
    "        fileids = self.resolve(fileids, categories)\n",
    "\n",
    "        # Create a generator, loading one document into memory at a time.\n",
    "        for path, encoding in self.abspaths(fileids, include_encoding=True):\n",
    "            with codecs.open(path, 'r', encoding=encoding) as f:\n",
    "                yield f.read()\n",
    "\n",
    "    def sizes(self, fileids=None, categories=None):\n",
    "        \"\"\"\n",
    "        Returns a list of tuples, the fileid and size on disk of the file.\n",
    "        This function is used to detect oddly large files in the corpus.\n",
    "        \"\"\"\n",
    "        # Resolve the fileids and the categories\n",
    "        fileids = self.resolve(fileids, categories)\n",
    "\n",
    "        # Create a generator, getting every path and computing filesize\n",
    "        for path in self.abspaths(fileids):\n",
    "            yield os.path.getsize(path)\n",
    "            \n",
    "    # Code below this line is extra, not (yet) covered in the textbook.\n",
    "    # You can leave it as-is. It provides some standard corpus methods.\n",
    "    # We're using PlaintextCorpusReader methods, but providing category resolution\n",
    "    def raw(self, fileids=None, categories=None):\n",
    "        \"\"\"\n",
    "        Returns raw text as a string.\n",
    "        \"\"\"\n",
    "        return PlaintextCorpusReader.raw(self, self.resolve(fileids, categories))\n",
    "\n",
    "    def words(self, fileids=None, categories=None):\n",
    "        \"\"\"\n",
    "        Returns a list of words.\n",
    "        \"\"\"\n",
    "        return PlaintextCorpusReader.words(self, self.resolve(fileids, categories))\n",
    "\n",
    "    def sents(self, fileids=None, categories=None):\n",
    "        \"\"\"\n",
    "        Returns a list of tokenized sentences.\n",
    "        \"\"\"\n",
    "        return PlaintextCorpusReader.sents(self, self.resolve(fileids, categories))\n",
    "\n",
    "    def paras(self, fileids=None, categories=None):\n",
    "        \"\"\"\n",
    "        Returns a list of tokenized sentences.\n",
    "        \"\"\"\n",
    "        return PlaintextCorpusReader.paras(self, self.resolve(fileids, categories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categories in the corpus:\n",
      " ['A', 'AF', 'AM', 'B', 'BF', 'BM', 'F', 'M']\n",
      "\n",
      "The first five fileids:\n",
      " ['A-Alcott-Little_Women-1868-F.txt', 'A-Cather-Antonia-1918-F.txt', 'A-Chesnutt-Marrow-1901-M.txt', 'A-Chopin-Awakening-1899-F.txt', 'A-Crane-Maggie-1893-M.txt']\n",
      "\n",
      "The 101st paragraph of Mrs Dalloway:\n",
      " [['Millicent', 'Bruton', ',', 'whose', 'lunch', 'parties', 'were', 'said', 'to', 'be', 'extraordinarily', 'amusing', ',', 'had', 'not', 'asked', 'her', '.'], ['No', 'vulgar', 'jealousy', 'could', 'separate', 'her', 'from', 'Richard', '.'], ['But', 'she', 'feared', 'time', 'itself', ',', 'and', 'read', 'on', 'Lady', 'Bruton', \"'\", 's', 'face', ',', 'as', 'if', 'it', 'had', 'been', 'a', 'dial', 'cut', 'in', 'impassive', 'stone', ',', 'the', 'dwindling', 'of', 'life', ';', 'how', 'year', 'by', 'year', 'her', 'share', 'was', 'sliced', ';', 'how', 'little', 'the', 'margin', 'that', 'remained', 'was', 'capable', 'any', 'longer', 'of', 'stretching', ',', 'of', 'absorbing', ',', 'as', 'in', 'the', 'youthful', 'years', ',', 'the', 'colours', ',', 'salts', ',', 'tones', 'of', 'existence', ',', 'so', 'that', 'she', 'filled', 'the', 'room', 'she', 'entered', ',', 'and', 'felt', 'often', 'as', 'she', 'stood', 'hesitating', 'one', 'moment', 'on', 'the', 'threshold', 'of', 'her', 'drawing', '-', 'room', ',', 'an', 'exquisite', 'suspense', ',', 'such', 'as', 'might', 'stay', 'a', 'diver', 'before', 'plunging', 'while', 'the', 'sea', 'darkens', 'and', 'brightens', 'beneath', 'him', ',', 'and', 'the', 'waves', 'which', 'threaten', 'to', 'break', ',', 'but', 'only', 'gently', 'split', 'their', 'surface', ',', 'roll', 'and', 'conceal', 'and', 'encrust', 'as', 'they', 'just', 'turn', 'over', 'the', 'weeds', 'with', 'pearl', '.']]\n",
      "\n",
      "The second sentence of that paragraph:\n",
      " ['No', 'vulgar', 'jealousy', 'could', 'separate', 'her', 'from', 'Richard', '.']\n",
      "\n",
      "American-female subcorpus file sizes in bytes:\n",
      " [1015333, 440138, 362678, 81705, 439228, 303622, 400630, 226008, 1038920, 580020]\n",
      "\n",
      "The first 300 characters of the British-male subcorpus:\n",
      "I\n",
      "\n",
      "The Nellie, a cruising yawl, swung to her anchor without a flutter of the sails, and was at rest. The flood had made, the wind was nearly calm, and being bound down the river, the only thing for it was to come to and wait for the turn of the tide.\n",
      "\n",
      "The sea-reach of the Thames stretched before us \n",
      "\n",
      "The 1,000,001-1,000,020th characters of the corpus:\n",
      "unusually up-lifted \n",
      "\n",
      "Total words in the corpus:\n",
      "6487940\n"
     ]
    }
   ],
   "source": [
    "corpus = TMNCorpusReader(text_dir, cat_map=category_map)\n",
    "\n",
    "print(\"Categories in the corpus:\\n\", corpus.categories())\n",
    "print(\"\\nThe first five fileids:\\n\", corpus.fileids()[:5])\n",
    "\n",
    "woolf = corpus.paras(fileids=['B-Woolf-Mrs_Dalloway-1925-F.txt'])\n",
    "print(\"\\nThe 101st paragraph of Mrs Dalloway:\\n\", woolf[100])\n",
    "print(\"\\nThe second sentence of that paragraph:\\n\", woolf[100][1])\n",
    "\n",
    "sizes = [i for i in corpus.sizes(categories=['AF'])]\n",
    "print(\"\\nAmerican-female subcorpus file sizes in bytes:\\n\", sizes)\n",
    "\n",
    "print(\"\\nThe first 300 characters of the British-male subcorpus:\")\n",
    "for doc in corpus.docs(categories=['BM']):\n",
    "    print(doc[:300])\n",
    "    break\n",
    "\n",
    "print(\"\\nThe 1,000,001-1,000,020th characters of the corpus:\")\n",
    "print(corpus.raw()[1000000:1000020])\n",
    "\n",
    "print(\"\\nTotal words in the corpus:\")\n",
    "print(len(corpus.words()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the only truly slow part of the code above is the bit that counts all the words. If this were production code, you'd want to calculate that value once and store the result.\n",
    "\n",
    "## Mini problems\n",
    "\n",
    "What fraction of the total corpus words are contained in the British subcorpus?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraction British words: 0.609\n"
     ]
    }
   ],
   "source": [
    "total_words = len(corpus.words())\n",
    "british_words = len(corpus.words(categories=['B']))\n",
    "print(f\"Fraction British words: {round(british_words/total_words,3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many books in the corpus are written by female authors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Female-authored books: 20\n"
     ]
    }
   ],
   "source": [
    "print(\"Female-authored books:\", len(corpus.fileids(categories=['F'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print words 2-4 of the second sentence of the fifth paragraph of _Middlemarch_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indicated words from Middlemarch: ['hand', 'and', 'wrist']\n"
     ]
    }
   ],
   "source": [
    "print(\"Indicated words from Middlemarch:\", \n",
    "      corpus.paras(fileids=['B-Eliot-Middlemarch-1869-F.txt'])[4][1][1:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List the file names of books by American men."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files by American men:\n",
      " ['A-Chesnutt-Marrow-1901-M.txt', 'A-Crane-Maggie-1893-M.txt', 'A-Dreiser-Sister_Carrie-1900-M.txt', 'A-Hawthorne-Scarlet_Letter-1850-M.txt', 'A-Howells-Silas_Lapham-1885-M.txt', 'A-James-Golden_Bowl-1904-M.txt', 'A-London-Call_Wild-1903-M.txt', 'A-Melville-Moby_Dick-1851-M.txt', 'A-Norris-Pit-1903-M.txt', 'A-Twain-Huck_Finn-1885-M.txt']\n"
     ]
    }
   ],
   "source": [
    "print(\"Files by American men:\\n\", corpus.fileids(categories=['AM']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Volumes by American men:\n",
      "\n",
      "A-Chesnutt-Marrow-1901-M.txt\n",
      "A-Crane-Maggie-1893-M.txt\n",
      "A-Dreiser-Sister_Carrie-1900-M.txt\n",
      "A-Hawthorne-Scarlet_Letter-1850-M.txt\n",
      "A-Howells-Silas_Lapham-1885-M.txt\n",
      "A-James-Golden_Bowl-1904-M.txt\n",
      "A-London-Call_Wild-1903-M.txt\n",
      "A-Melville-Moby_Dick-1851-M.txt\n",
      "A-Norris-Pit-1903-M.txt\n",
      "A-Twain-Huck_Finn-1885-M.txt\n"
     ]
    }
   ],
   "source": [
    "# Or, more legibly:\n",
    "print(\"Volumes by American men:\\n\")\n",
    "for file in corpus.fileids(categories=['AM']):\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
