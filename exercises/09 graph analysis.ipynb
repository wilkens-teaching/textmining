{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph analysis\n",
    "\n",
    "Your task in this exercise is straightforward: produce a graph of the named entities in the literary corpus and calculate a handful of centrality measures on it. This follows notebook 09 very closely.\n",
    "\n",
    "Your notebook should:\n",
    "\n",
    "1. Perform entity recognition on the full corpus using Spacy.\n",
    "2. Produce a `networkx` graph object of the extracted entites, where nodes are entities and edges represent weighted coöccurrence in the same text. (For a challenge, use the chunked version of the corpus to get better in-document coöccurrence resolution.) **NB.** You can use all the Spacy entity types if you want, but this produces a very large graph, which in turn makes some of the centrality measures very computationaly expensive. I'd suggest limiting the retained entity types to just a few (ideally, broadly related, like 'GPE', 'LOC', and 'FACILITY', for example).\n",
    "3. Produce a list of the top 20 entities in your graph according to each of at least three different centrality measures.\n",
    "4. Produce a visualization of your graph. In a perfect world, make this visualization as legible as possible ;).\n",
    "\n",
    "There's starter code from the textbook below. Modify it as needed and add your own code to perform the tasks listed above. Upload your completed notebook to Sakai."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import heapq\n",
    "import itertools\n",
    "import networkx as nx\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "from operator import itemgetter\n",
    "\n",
    "# Import our libraries\n",
    "sys.path.append(os.path.join('..', 'libraries'))\n",
    "from TMN import PickledCorpusReader\n",
    "\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "GOOD_ENTS = ['PERSON', 'NORP', 'FACILITY', 'ORG', 'GPE', 'LOC',\n",
    "             'PRODUCT', 'EVENT', 'WORK_OF_ART', 'LANGUAGE']\n",
    "\n",
    "def entities(sent):\n",
    "    doc = nlp(sent)\n",
    "    for ent in doc.ents:\n",
    "        #  filter out non-target entities\n",
    "        if ent.label_ in GOOD_ENTS:\n",
    "            return ent.text, ent.label_\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "def pairs(doc):\n",
    "    candidates = [\n",
    "        entities(' '.join(word for word, tag in sent))\n",
    "        for para in doc for sent in para\n",
    "    ]\n",
    "\n",
    "    doc_entities = [\n",
    "        entity for entity in candidates if entity is not None\n",
    "    ]\n",
    "\n",
    "    return list(itertools.permutations(set(doc_entities), 2))\n",
    "\n",
    "def graph(docs):\n",
    "    G = nx.Graph()\n",
    "    completed = 0\n",
    "    for doc in docs:\n",
    "        if completed % 500 == 0:\n",
    "            print(f'{time.ctime()} :: {completed} chunks complete')\n",
    "        for pair in pairs(doc):\n",
    "            if (pair[0][0], pair[1][0]) in G.edges():\n",
    "                G.edges[(pair[0][0], pair[1][0])]['weight'] += 1\n",
    "            else:\n",
    "                G.add_edge(pair[0][0], pair[1][0], weight=1)\n",
    "        completed+=1\n",
    "    return G\n",
    "\n",
    "def nbest_centrality(G, metric, n=10, attr=\"centrality\", **kwargs):\n",
    "    # Compute the centrality scores for each vertex\n",
    "    scores = metric(G, **kwargs)\n",
    "\n",
    "    # Set the score as a property on each node\n",
    "    nx.set_node_attributes(G, name=attr, values=scores)\n",
    "\n",
    "    # Find the top n scores and print them along with their index\n",
    "    topn = heapq.nlargest(n, scores.items(), key=itemgetter(1))\n",
    "    for idx, item in enumerate(topn):\n",
    "        print(\"{}. {}: {:0.4f}\".format(idx + 1, *item))\n",
    "\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
