{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with corpus data\n",
    "\n",
    "## Background\n",
    "\n",
    "In this exercise, we'll make use of the (further) modified NLTK corpus reader from the previous exercise and of a new version that deals with saved (pickled) corpora. Your tasks -- outlined below, will be to put these readers to use to begin characterizing the corpora.\n",
    "\n",
    "First, you'll need to update the file locations below to match your own system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Where are the corpus texts on your system\n",
    "text_dir = os.path.join('..', 'data', 'texts')\n",
    "pickle_dir = os.path.join('..', 'data', 'pickled')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Existing code\n",
    "\n",
    "Now, review the code in the next (long) cell to make sure you understand it. I'm not going to make you mess with it just for the sake of messing with it, but you should know how you would, for instance, change the `tokenize()` or `process()` methods to return lowercase tokens or remove punctuation, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOC_PATTERN = r'.+\\.txt'        # Documents are just files that end in '.txt'\n",
    "PKL_PATTERN = r'.+\\.pickle'     # Pickled files end in .pickle\n",
    "CAT_PATTERN = r'([a-z_\\s]+)/.*' # We won't use this, but fall back to directory-based labels\n",
    "                                # if no other labels are supplied\n",
    "\n",
    "import codecs\n",
    "import time\n",
    "import nltk\n",
    "import os\n",
    "import pickle\n",
    "from   glob import glob\n",
    "from   nltk.corpus.reader.api import CorpusReader\n",
    "from   nltk.corpus.reader.api import CategorizedCorpusReader\n",
    "from   nltk import pos_tag, sent_tokenize, wordpunct_tokenize\n",
    "\n",
    "def make_cat_map(path, extension):\n",
    "    \"\"\"\n",
    "    Takes a directory path and file extension (e.g., 'txt').\n",
    "    Returns a dictionary of file:category mappings from standard file names:\n",
    "      nation-author-title-year-gender\n",
    "    \"\"\"\n",
    "    file_paths = glob(os.path.join(path, f'*.{extension}'))\n",
    "    file_names = [os.path.split(i)[1] for i in file_paths]\n",
    "    category_map = {} # Dict to hold filename:[categories] mappings\n",
    "    for file in file_names:\n",
    "        parsed = file.rstrip(f'.{extension}').split('-') # strip extension and split on hyphens\n",
    "        nation = parsed[0]\n",
    "        gender = parsed[4]\n",
    "        category_map[file] = [nation, gender, nation+gender]\n",
    "    return category_map\n",
    "\n",
    "class TMNCorpusReader(CategorizedCorpusReader, CorpusReader):\n",
    "    \"\"\"\n",
    "    A corpus reader for categorized text documents to enable preprocessing.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        root, \n",
    "        fileids=DOC_PATTERN,\n",
    "        encoding='utf8', \n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the corpus reader.  Categorization arguments\n",
    "        (``cat_pattern``, ``cat_map``, and ``cat_file``) are passed to\n",
    "        the ``CategorizedCorpusReader`` constructor.  The remaining\n",
    "        arguments are passed to the ``CorpusReader`` constructor.\n",
    "        \"\"\"\n",
    "        # Add the default category pattern if not passed into the class.\n",
    "        if not any(key.startswith('cat_') for key in kwargs.keys()):\n",
    "            # First, try to build a cat_map from standard-style filenames\n",
    "            try: \n",
    "                kwargs['cat_map'] = make_cat_map(root, 'txt')\n",
    "            # On error, fall back to dir names for categories    \n",
    "            except Exception as e:\n",
    "                print(type(e), e, \"\\nUnable to build category map from file names.\\nFalling back to categories by directory name.\")\n",
    "                kwargs['cat_pattern'] = CAT_PATTERN\n",
    "\n",
    "        # Initialize the NLTK corpus reader objects\n",
    "        CategorizedCorpusReader.__init__(self, kwargs)\n",
    "        CorpusReader.__init__(self, root, fileids, encoding)\n",
    "        \n",
    "    def resolve(self, fileids, categories):\n",
    "            \"\"\"\n",
    "            Returns a list of fileids or categories depending on what is passed\n",
    "            to each internal corpus reader function. Implemented similarly to\n",
    "            the NLTK ``CategorizedPlaintextCorpusReader``.\n",
    "            \"\"\"\n",
    "            if fileids is not None and categories is not None:\n",
    "                raise ValueError(\"Specify fileids or categories, not both\")\n",
    "\n",
    "            if categories is not None:\n",
    "                return self.fileids(categories)\n",
    "            return fileids\n",
    "\n",
    "    def docs(self, fileids=None, categories=None):\n",
    "        \"\"\"\n",
    "        Returns the complete text of a document, closing the document\n",
    "        after we are done reading it and yielding it in a memory safe fashion.\n",
    "        \"\"\"\n",
    "        # Resolve the fileids and the categories\n",
    "        fileids = self.resolve(fileids, categories)\n",
    "\n",
    "        # Create a generator, loading one document into memory at a time.\n",
    "        for path, encoding in self.abspaths(fileids, include_encoding=True):\n",
    "            with codecs.open(path, 'r', encoding=encoding) as f:\n",
    "                yield f.read()\n",
    "\n",
    "    def sizes(self, fileids=None, categories=None):\n",
    "        \"\"\"\n",
    "        Returns a list of tuples, the fileid and size on disk of the file.\n",
    "        This function is used to detect oddly large files in the corpus.\n",
    "        \"\"\"\n",
    "        # Resolve the fileids and the categories\n",
    "        fileids = self.resolve(fileids, categories)\n",
    "\n",
    "        # Create a generator, getting every path and computing filesize\n",
    "        for path in self.abspaths(fileids):\n",
    "            yield os.path.getsize(path)\n",
    "            \n",
    "    def paras(self, fileids=None, categories=None):\n",
    "        \"\"\"\n",
    "        Uses splitlines() to parse the paragraphs from plain text.\n",
    "        \"\"\"\n",
    "        # Resolve the fileids and the categories\n",
    "        fileids = self.resolve(fileids, categories)\n",
    "        \n",
    "        for doc in self.docs(fileids):\n",
    "            for par in doc.splitlines():\n",
    "                if len(par) > 0:\n",
    "                    yield par\n",
    "\n",
    "    def sents(self, fileids=None, categories=None):\n",
    "        \"\"\"\n",
    "        Uses the built in sentence tokenizer to extract sentences from the\n",
    "        paragraphs. Note that this method uses BeautifulSoup to parse HTML.\n",
    "        \"\"\"\n",
    "        # Resolve the fileids and the categories\n",
    "        fileids = self.resolve(fileids, categories)\n",
    "        \n",
    "        for paragraph in self.paras(fileids):\n",
    "            for sentence in sent_tokenize(paragraph):\n",
    "                yield sentence\n",
    "\n",
    "    def words(self, fileids=None, categories=None):\n",
    "        \"\"\"\n",
    "        Uses the built in word tokenizer to extract tokens from sentences.\n",
    "        Note that this method uses BeautifulSoup to parse HTML content.\n",
    "        \"\"\"\n",
    "        # Resolve the fileids and the categories\n",
    "        fileids = self.resolve(fileids, categories)\n",
    "        \n",
    "        for sentence in self.sents(fileids):\n",
    "            for token in wordpunct_tokenize(sentence):\n",
    "                yield token\n",
    "\n",
    "    def describe(self, fileids=None, categories=None):\n",
    "        \"\"\"\n",
    "        Performs a single pass of the corpus and\n",
    "        returns a dictionary with a variety of metrics\n",
    "        concerning the state of the corpus.\n",
    "        \"\"\"\n",
    "        started = time.time()\n",
    "\n",
    "        # Structures to perform counting.\n",
    "        counts  = nltk.FreqDist()\n",
    "        tokens  = nltk.FreqDist()\n",
    "\n",
    "        # Perform single pass over paragraphs, tokenize and count\n",
    "        for para in self.paras(fileids, categories):\n",
    "            counts['paras'] += 1\n",
    "\n",
    "            for sent in sent_tokenize(para):\n",
    "                counts['sents'] += 1\n",
    "\n",
    "                for word in wordpunct_tokenize(sent):\n",
    "                    counts['words'] += 1\n",
    "                    tokens[word] += 1\n",
    "\n",
    "        # Compute the number of files and categories in the corpus\n",
    "        n_fileids = len(self.resolve(fileids, categories) or self.fileids())\n",
    "        n_topics  = len(self.categories(self.resolve(fileids, categories)))\n",
    "\n",
    "        # Return data structure with information\n",
    "        return {\n",
    "            'files':  n_fileids,\n",
    "            'categories': n_topics,\n",
    "            'paragraphs':  counts['paras'],\n",
    "            'sentences':  counts['sents'],\n",
    "            'words':  counts['words'],\n",
    "            'vocabulary_size':  len(tokens),\n",
    "            'lexical_diversity': float(counts['words']) / float(len(tokens)),\n",
    "            'paras_per_doc':  float(counts['paras']) / float(n_fileids),\n",
    "            'words_per_doc':  float(counts['words']) / float(n_fileids),\n",
    "            'sents_per_para':  float(counts['sents']) / float(counts['paras']),\n",
    "            'secs':   time.time() - started,\n",
    "        }\n",
    "    \n",
    "class PickledCorpusReader(CategorizedCorpusReader, CorpusReader):\n",
    "\n",
    "    def __init__(self, root, fileids=PKL_PATTERN, **kwargs):\n",
    "        \"\"\"\n",
    "        Initialize the corpus reader.  Categorization arguments\n",
    "        (``cat_pattern``, ``cat_map``, and ``cat_file``) are passed to\n",
    "        the ``CategorizedCorpusReader`` constructor.  The remaining arguments\n",
    "        are passed to the ``CorpusReader`` constructor.\n",
    "        \"\"\"\n",
    "        # Add the default category pattern if not passed into the class.\n",
    "        if not any(key.startswith('cat_') for key in kwargs.keys()):\n",
    "            # First, try to build a cat_map from standard-style filenames\n",
    "            try: \n",
    "                kwargs['cat_map'] = make_cat_map(root, 'pickle')\n",
    "            # On error, fall back to dir names for categories    \n",
    "            except Exception as e:\n",
    "                print(type(e), e, \"\\nUnable to build category map from file names.\\nFalling back to categories by directory name.\")\n",
    "                kwargs['cat_pattern'] = CAT_PATTERN\n",
    "\n",
    "        CategorizedCorpusReader.__init__(self, kwargs)\n",
    "        CorpusReader.__init__(self, root, fileids)\n",
    "\n",
    "    def resolve(self, fileids, categories):\n",
    "        \"\"\"\n",
    "        Returns a list of fileids or categories depending on what is passed\n",
    "        to each internal corpus reader function. This primarily bubbles up to\n",
    "        the high level ``docs`` method, but is implemented here similar to\n",
    "        the nltk ``CategorizedPlaintextCorpusReader``.\n",
    "        \"\"\"\n",
    "        if fileids is not None and categories is not None:\n",
    "            raise ValueError(\"Specify fileids or categories, not both\")\n",
    "\n",
    "        if categories is not None:\n",
    "            return self.fileids(categories)\n",
    "        return fileids\n",
    "\n",
    "    def docs(self, fileids=None, categories=None):\n",
    "        \"\"\"\n",
    "        Returns the document loaded from a pickled object for every file in\n",
    "        the corpus. Similar to the BaleenCorpusReader, this uses a generator\n",
    "        to acheive memory safe iteration.\n",
    "        \"\"\"\n",
    "        # Resolve the fileids and the categories\n",
    "        fileids = self.resolve(fileids, categories)\n",
    "\n",
    "        # Create a generator, loading one document into memory at a time.\n",
    "        for path, enc, fileid in self.abspaths(fileids, True, True):\n",
    "            with open(path, 'rb') as f:\n",
    "                yield pickle.load(f)\n",
    "\n",
    "    def paras(self, fileids=None, categories=None):\n",
    "        \"\"\"\n",
    "        Returns a generator of paragraphs where each paragraph is a list of\n",
    "        sentences, which is in turn a list of (token, tag) tuples.\n",
    "        \"\"\"\n",
    "        for doc in self.docs(fileids, categories):\n",
    "            for paragraph in doc:\n",
    "                yield paragraph\n",
    "\n",
    "    def sents(self, fileids=None, categories=None):\n",
    "        \"\"\"\n",
    "        Returns a generator of sentences where each sentence is a list of\n",
    "        (token, tag) tuples.\n",
    "        \"\"\"\n",
    "        for paragraph in self.paras(fileids, categories):\n",
    "            for sentence in paragraph:\n",
    "                yield sentence\n",
    "\n",
    "    def tagged(self, fileids=None, categories=None):\n",
    "        for sent in self.sents(fileids, categories):\n",
    "            for token in sent:\n",
    "                yield token\n",
    "\n",
    "    def words(self, fileids=None, categories=None):\n",
    "        \"\"\"\n",
    "        Returns a generator of (token, tag) tuples.\n",
    "        \"\"\"\n",
    "        for token in self.tagged(fileids, categories):\n",
    "            yield token[0]\n",
    "            \n",
    "class Preprocessor(object):\n",
    "    \"\"\"\n",
    "    The preprocessor wraps a corpus object (usually a `TMNCorpusReader`)\n",
    "    and manages the stateful tokenization and part of speech tagging into a\n",
    "    directory that is stored in a format that can be read by the\n",
    "    `PickledCorpusReader`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, corpus, target=None, **kwargs):\n",
    "        \"\"\"\n",
    "        The corpus is the `TMNCorpusReader` to preprocess and pickle.\n",
    "        The target is the directory on disk to output the pickled corpus to.\n",
    "        \"\"\"\n",
    "        self.corpus = corpus\n",
    "        self.target = target\n",
    "\n",
    "    def fileids(self, fileids=None, categories=None):\n",
    "        \"\"\"\n",
    "        Helper function access the fileids of the corpus\n",
    "        \"\"\"\n",
    "        fileids = self.corpus.resolve(fileids, categories)\n",
    "        if fileids:\n",
    "            return fileids\n",
    "        return self.corpus.fileids()\n",
    "\n",
    "    def abspath(self, fileid):\n",
    "        \"\"\"\n",
    "        Returns the absolute path to the target fileid from the corpus fileid.\n",
    "        \"\"\"\n",
    "        # Find the directory, relative from the corpus root.\n",
    "        parent = os.path.relpath(\n",
    "            os.path.dirname(self.corpus.abspath(fileid)), self.corpus.root\n",
    "        )\n",
    "\n",
    "        # Compute the name parts to reconstruct\n",
    "        basename  = os.path.basename(fileid)\n",
    "        name, ext = os.path.splitext(basename)\n",
    "\n",
    "        # Create the pickle file extension\n",
    "        basename  = name + '.pickle'\n",
    "\n",
    "        # Return the path to the file relative to the target.\n",
    "        return os.path.normpath(os.path.join(self.target, parent, basename))\n",
    "\n",
    "    def tokenize(self, fileid):\n",
    "        \"\"\"\n",
    "        Segments, tokenizes, and tags a document in the corpus. Returns a\n",
    "        generator of paragraphs, which are lists of sentences, which in turn\n",
    "        are lists of part of speech tagged words.\n",
    "        \"\"\"\n",
    "        for paragraph in self.corpus.paras(fileids=fileid):\n",
    "            yield [\n",
    "                pos_tag(wordpunct_tokenize(sent))\n",
    "                for sent in sent_tokenize(paragraph)\n",
    "            ]\n",
    "\n",
    "    def process(self, fileid):\n",
    "        \"\"\"\n",
    "        For a single file does the following preprocessing work:\n",
    "            1. Checks the location on disk to make sure no errors occur.\n",
    "            2. Gets all paragraphs for the given text.\n",
    "            3. Segements the paragraphs with the sent_tokenizer\n",
    "            4. Tokenizes the sentences with the wordpunct_tokenizer\n",
    "            5. Tags the sentences using the default pos_tagger\n",
    "            6. Writes the document as a pickle to the target location.\n",
    "        This method is called multiple times from the transform runner.\n",
    "        \"\"\"\n",
    "        # Compute the outpath to write the file to.\n",
    "        target = self.abspath(fileid)\n",
    "        parent = os.path.dirname(target)\n",
    "\n",
    "        # Make sure the directory exists\n",
    "        if not os.path.exists(parent):\n",
    "            os.makedirs(parent)\n",
    "\n",
    "        # Make sure that the parent is a directory and not a file\n",
    "        if not os.path.isdir(parent):\n",
    "            raise ValueError(\n",
    "                \"Please supply a directory to write preprocessed data to.\"\n",
    "            )\n",
    "\n",
    "        # Create a data structure for the pickle\n",
    "        document = list(self.tokenize(fileid))\n",
    "\n",
    "        # Open and serialize the pickle to disk\n",
    "        with open(target, 'wb') as f:\n",
    "            pickle.dump(document, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "        # Clean up the document\n",
    "        del document\n",
    "\n",
    "        # Return the target fileid\n",
    "        return target\n",
    "\n",
    "    def transform(self, fileids=None, categories=None):\n",
    "        \"\"\"\n",
    "        Transform the wrapped corpus, writing out the segmented, tokenized,\n",
    "        and part of speech tagged corpus as a pickle to the target directory.\n",
    "        This method will also directly copy files that are in the corpus.root\n",
    "        directory that are not matched by the corpus.fileids().\n",
    "        \"\"\"\n",
    "        # Make the target directory if it doesn't already exist\n",
    "        if not os.path.exists(self.target):\n",
    "            os.makedirs(self.target)\n",
    "\n",
    "        # Resolve the fileids to start processing and return the list of \n",
    "        # target file ids to pass to downstream transformers. \n",
    "        return [\n",
    "            self.process(fileid)\n",
    "            for fileid in self.fileids(fileids, categories)\n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load, preprocess, and pickle to disk\n",
    "\n",
    "Execute this code. Again, be sure you understand what it's doing ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize our corpus reader\n",
    "corpus = TMNCorpusReader(text_dir, r'.+\\.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "files 40\n",
      "categories 8\n",
      "paragraphs 98477\n",
      "sentences 295138\n",
      "words 6488322\n",
      "vocabulary_size 64215\n",
      "lexical_diversity 101.04059799112358\n",
      "paras_per_doc 2461.925\n",
      "words_per_doc 162208.05\n",
      "sents_per_para 2.9970246859672818\n",
      "secs 26.308953046798706\n"
     ]
    }
   ],
   "source": [
    "# Get descriptive stats for full corpus\n",
    "description = corpus.describe()\n",
    "for key in description:\n",
    "    print(key, description[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "American/British comparison\n",
      "files 1.0\n",
      "categories 1.0\n",
      "paragraphs 0.71\n",
      "sentences 0.67\n",
      "words 0.64\n",
      "vocabulary_size 0.96\n",
      "lexical_diversity 0.67\n",
      "paras_per_doc 0.71\n",
      "words_per_doc 0.64\n",
      "sents_per_para 0.96\n",
      "secs 0.61\n",
      "\n",
      "Male/female comparison\n",
      "files 1.0\n",
      "categories 1.0\n",
      "paragraphs 1.25\n",
      "sentences 1.37\n",
      "words 1.19\n",
      "vocabulary_size 1.2\n",
      "lexical_diversity 0.99\n",
      "paras_per_doc 1.25\n",
      "words_per_doc 1.19\n",
      "sents_per_para 1.1\n",
      "secs 1.23\n",
      "CPU times: user 51.2 s, sys: 167 ms, total: 51.4 s\n",
      "Wall time: 51.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Compare stats for A/B and M/F categories\n",
    "a_desc = corpus.describe(categories=['A'])\n",
    "b_desc = corpus.describe(categories=['B'])\n",
    "f_desc = corpus.describe(categories=['F'])\n",
    "m_desc = corpus.describe(categories=['M'])\n",
    "\n",
    "print(\"American/British comparison\")\n",
    "for key in a_desc.keys():\n",
    "    print(key, round(a_desc[key]/b_desc[key], 2))\n",
    "\n",
    "print(\"\\nMale/female comparison\")\n",
    "for key in m_desc.keys():\n",
    "    print(key, round(m_desc[key]/f_desc[key], 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize preprocessor\n",
    "preproc = Preprocessor(corpus, pickle_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5min 32s, sys: 9.14 s, total: 5min 41s\n",
      "Wall time: 6min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Perform preprocessing and save output to disk\n",
    "processed = preproc.transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categories in the corpus:\n",
      " ['A', 'AF', 'AM', 'B', 'BF', 'BM', 'F', 'M']\n",
      "\n",
      "British-female-authored files:\n",
      " ['B-Austen-Pride_Prejudice-1813-F.pickle', 'B-Bronte_C-Jane_Eyre-1847-F.pickle', 'B-Bronte_E-Wuthering_Heights-1847-F.pickle', 'B-Burney-Evelina-1778-F.pickle', 'B-Eliot-Middlemarch-1869-F.pickle', 'B-Gaskell-North_South-1855-F.pickle', 'B-Mitford-Our_Village-1826-F.pickle', 'B-Radcliffe-Mysteries_Udolpho-1794-F.pickle', 'B-Shelley-Frankenstein-1818-F.pickle', 'B-Woolf-Mrs_Dalloway-1925-F.pickle']\n",
      "\n",
      "A bit of one pickled text\n",
      "[[('“', 'JJ'), ('No', 'NNP'), (';', ':'), ('I', 'PRP'), ('mean', 'VBP'), (',', ','), ('really', 'RB'), (',', ','), ('Tom', 'NNP'), ('is', 'VBZ'), ('a', 'DT'), ('good', 'JJ'), (',', ','), ('steady', 'JJ'), (',', ','), ('sensible', 'JJ'), (',', ','), ('pious', 'JJ'), ('fellow', 'NN'), ('.', '.')], [('He', 'PRP'), ('got', 'VBD'), ('religion', 'NN'), ('at', 'IN'), ('a', 'DT'), ('camp', 'NN'), ('-', ':'), ('meeting', 'NN'), (',', ','), ('four', 'CD'), ('years', 'NNS'), ('ago', 'RB'), (';', ':'), ('and', 'CC'), ('I', 'PRP'), ('believe', 'VBP'), ('he', 'PRP'), ('really', 'RB'), ('_did_', 'VBZ'), ('get', 'VB'), ('it', 'PRP'), ('.', '.')], [('I', 'PRP'), ('’', 'VBP'), ('ve', 'RB'), ('trusted', 'VBD'), ('him', 'PRP'), (',', ','), ('since', 'IN'), ('then', 'RB'), (',', ','), ('with', 'IN'), ('everything', 'NN'), ('I', 'PRP'), ('have', 'VBP'), (',--', 'JJ'), ('money', 'NN'), (',', ','), ('house', 'NN'), (',', ','), ('horses', 'VBZ'), (',--', 'JJ'), ('and', 'CC'), ('let', 'VB'), ('him', 'PRP'), ('come', 'VB'), ('and', 'CC'), ('go', 'VB'), ('round', 'IN'), ('the', 'DT'), ('country', 'NN'), (';', ':'), ('and', 'CC'), ('I', 'PRP'), ('always', 'RB'), ('found', 'VBD'), ('him', 'PRP'), ('true', 'JJ'), ('and', 'CC'), ('square', 'JJ'), ('in', 'IN'), ('everything', 'NN'), ('.”', 'NN')]]\n"
     ]
    }
   ],
   "source": [
    "# Show that we can work with the pickled versions\n",
    "pcorpus = PickledCorpusReader(pickle_dir)\n",
    "print(\"Categories in the corpus:\\n\", pcorpus.categories())\n",
    "print(\"\\nBritish-female-authored files:\\n\", pcorpus.fileids(categories=['BF']))\n",
    "print(\"\\nA bit of one pickled text\")\n",
    "for doc in pcorpus.docs(fileids=['A-Stowe-Uncle_Tom-1852-F.pickle']):\n",
    "    print(doc[11])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finally, your code!\n",
    "\n",
    "So, now we've read the corpus, preprocessed it, saved the tokenized and part-of-speech-tagged version to disk, and loaded the processed version (as `pcorpus`) for investigation.\n",
    "\n",
    "Your tasks:\n",
    "\n",
    "1. Print a list of the 20 most frequently occurring words in the British female ('BF') corpus.\n",
    "2. Print a list of the 10 most frequently occurring parts of speech in the full corpus. Note that the part-of-speech tags are from the [Penn Treebank tagset](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html). You can get more info about the tags via `nltk.help.upenn_tagset()`.\n",
    "3. Devise a method to assess which parts of speech are under- and over-represented in the American corpus relative to the British corpus. This could be simple, or it might be more complex (especially if you know a bit about statistics). Simple is OK. Remember that the subcorpora *do not* contain the same number of words (this may or may not matter, depending on houw you approach the problem). Print a list of the 5 parts of speech that are most overrepresented in the American corpus according to your metric.\n",
    "4. Print the same list as in task 1, but using lemmatized, lowercase versions of the words and removing stopwords and punctuation. You do not need to reprocess the corpus; you can transform the tokenized output that we're already using. To lemmatize, use `nltk.stem.wordnet.WordNetLemmatizer()`. You'll need to translate between Penn Treebank tags and WordNet tags; there's a function for this included below. You can get a list of English stopwords (high-frequency words like 'the' and 'an' that are not generally informative on their own) from `nltk.corpus.stopwords.words('english')` and a list of punctuation from `string.punctuation`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 words in BF corpus:\n",
      ", \t 129573\n",
      "the \t 66885\n",
      ". \t 57577\n",
      "to \t 47804\n",
      "and \t 47011\n",
      "of \t 41668\n",
      "I \t 34279\n",
      "a \t 29904\n",
      "' \t 24965\n",
      "in \t 21795\n",
      "her \t 20872\n",
      "was \t 20242\n",
      "; \t 19381\n",
      "that \t 19251\n",
      "he \t 16491\n",
      "she \t 15132\n",
      "you \t 14959\n",
      "it \t 14804\n",
      "had \t 14703\n",
      "his \t 13635\n"
     ]
    }
   ],
   "source": [
    "# 1. Top 20 words in BF corpus\n",
    "from collections import Counter\n",
    "words_bf = Counter()\n",
    "for token in pcorpus.tagged(categories='BF'):\n",
    "    words_bf[token[0]] += 1\n",
    "print(\"Top 20 words in BF corpus:\")\n",
    "for word in words_bf.most_common(n=20):\n",
    "    print(word[0], '\\t', word[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 parts of speech in full corpus:\n",
      "NN \t 873979\n",
      "IN \t 653926\n",
      "DT \t 501505\n",
      "PRP \t 495040\n",
      ", \t 402550\n",
      "JJ \t 384182\n",
      "VBD \t 374716\n",
      "RB \t 328877\n",
      "NNP \t 288752\n",
      ". \t 251017\n"
     ]
    }
   ],
   "source": [
    "# 2. Top 10 PoS in full corpus\n",
    "pos_all = Counter()\n",
    "for token in pcorpus.tagged():\n",
    "    pos_all[token[1]] += 1\n",
    "print(\"Top 10 parts of speech in full corpus:\")\n",
    "for pos in pos_all.most_common(n=10):\n",
    "    print(pos[0], '\\t', pos[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 PoS enriched in A corpus relative to B:\n",
      "$ \t 5.83\n",
      "FW \t 2.58\n",
      "'' \t 1.51\n",
      "RP \t 1.48\n",
      "SYM \t 1.25\n",
      "CD \t 1.16\n",
      "NNPS \t 1.1\n",
      "JJ \t 1.09\n",
      "EX \t 1.09\n",
      "NNS \t 1.08\n",
      "\n",
      "Top 10 PoS deficient in A corpus relative to B:\n",
      ") \t 0.4\n",
      "( \t 0.4\n",
      "WP$ \t 0.62\n",
      "MD \t 0.72\n",
      "VBZ \t 0.85\n",
      "WP \t 0.85\n",
      "VBN \t 0.85\n",
      "WDT \t 0.85\n",
      "TO \t 0.87\n",
      "PRP$ \t 0.87\n"
     ]
    }
   ],
   "source": [
    "# 3. Under/overrepresentation of PoS in A corpus vs. B corpus.\n",
    "import numpy as np\n",
    "pos_a = Counter()\n",
    "pos_b = Counter()\n",
    "for token in pcorpus.tagged(categories='A'):\n",
    "    pos_a[token[1]] += 1\n",
    "for token in pcorpus.tagged(categories='B'):\n",
    "    pos_b[token[1]] += 1\n",
    "wc_a = np.sum(list(pos_a.values()))\n",
    "wc_b = np.sum(list(pos_b.values()))\n",
    "\n",
    "pos_ratios = Counter()\n",
    "wordcount_ratio = wc_b/wc_a\n",
    "for pos in pos_b.items():\n",
    "    try:\n",
    "        tag = pos[0]\n",
    "        pos_ratios[tag] = pos_a[tag]/pos_b[tag] * wordcount_ratio\n",
    "    except ZeroDivisionError:\n",
    "        pass\n",
    "print(\"Top 10 PoS enriched in A corpus relative to B:\")\n",
    "for tag in pos_ratios.most_common(10):\n",
    "    print(tag[0], '\\t', round(tag[1], 2))\n",
    "print(\"\\nTop 10 PoS deficient in A corpus relative to B:\")\n",
    "for tag in pos_ratios.most_common()[:-11:-1]:\n",
    "    print(tag[0], '\\t', round(tag[1], 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>$</th>\n",
       "      <th>''</th>\n",
       "      <th>(</th>\n",
       "      <th>)</th>\n",
       "      <th>,</th>\n",
       "      <th>.</th>\n",
       "      <th>:</th>\n",
       "      <th>CC</th>\n",
       "      <th>CD</th>\n",
       "      <th>DT</th>\n",
       "      <th>...</th>\n",
       "      <th>VBD</th>\n",
       "      <th>VBG</th>\n",
       "      <th>VBN</th>\n",
       "      <th>VBP</th>\n",
       "      <th>VBZ</th>\n",
       "      <th>WDT</th>\n",
       "      <th>WP</th>\n",
       "      <th>WP$</th>\n",
       "      <th>WRB</th>\n",
       "      <th>``</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>A-Alcott-Little_Women-1868-F.pickle</th>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.015878</td>\n",
       "      <td>0.000118</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.077302</td>\n",
       "      <td>0.034055</td>\n",
       "      <td>0.003976</td>\n",
       "      <td>0.043827</td>\n",
       "      <td>0.004395</td>\n",
       "      <td>0.068154</td>\n",
       "      <td>...</td>\n",
       "      <td>0.060621</td>\n",
       "      <td>0.016620</td>\n",
       "      <td>0.016327</td>\n",
       "      <td>0.021927</td>\n",
       "      <td>0.009619</td>\n",
       "      <td>0.003893</td>\n",
       "      <td>0.004286</td>\n",
       "      <td>0.000157</td>\n",
       "      <td>0.005709</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A-Cather-Antonia-1918-F.pickle</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014392</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.052536</td>\n",
       "      <td>0.047359</td>\n",
       "      <td>0.012598</td>\n",
       "      <td>0.037385</td>\n",
       "      <td>0.005115</td>\n",
       "      <td>0.078603</td>\n",
       "      <td>...</td>\n",
       "      <td>0.080377</td>\n",
       "      <td>0.015120</td>\n",
       "      <td>0.017406</td>\n",
       "      <td>0.014290</td>\n",
       "      <td>0.005197</td>\n",
       "      <td>0.003250</td>\n",
       "      <td>0.003434</td>\n",
       "      <td>0.000082</td>\n",
       "      <td>0.007237</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A-Chesnutt-Marrow-1901-M.pickle</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018660</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.061304</td>\n",
       "      <td>0.039306</td>\n",
       "      <td>0.008001</td>\n",
       "      <td>0.026679</td>\n",
       "      <td>0.004998</td>\n",
       "      <td>0.090023</td>\n",
       "      <td>...</td>\n",
       "      <td>0.053366</td>\n",
       "      <td>0.011067</td>\n",
       "      <td>0.028030</td>\n",
       "      <td>0.016800</td>\n",
       "      <td>0.009561</td>\n",
       "      <td>0.006014</td>\n",
       "      <td>0.004617</td>\n",
       "      <td>0.000399</td>\n",
       "      <td>0.004073</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A-Chopin-Awakening-1899-F.pickle</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005470</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.045042</td>\n",
       "      <td>0.052318</td>\n",
       "      <td>0.013518</td>\n",
       "      <td>0.034914</td>\n",
       "      <td>0.004868</td>\n",
       "      <td>0.080440</td>\n",
       "      <td>...</td>\n",
       "      <td>0.075847</td>\n",
       "      <td>0.020467</td>\n",
       "      <td>0.021893</td>\n",
       "      <td>0.012144</td>\n",
       "      <td>0.007158</td>\n",
       "      <td>0.007394</td>\n",
       "      <td>0.004345</td>\n",
       "      <td>0.000209</td>\n",
       "      <td>0.006478</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A-Crane-Maggie-1893-M.pickle</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018938</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.046577</td>\n",
       "      <td>0.059296</td>\n",
       "      <td>0.007757</td>\n",
       "      <td>0.026591</td>\n",
       "      <td>0.003774</td>\n",
       "      <td>0.087669</td>\n",
       "      <td>...</td>\n",
       "      <td>0.074251</td>\n",
       "      <td>0.016842</td>\n",
       "      <td>0.018100</td>\n",
       "      <td>0.011775</td>\n",
       "      <td>0.008002</td>\n",
       "      <td>0.002306</td>\n",
       "      <td>0.004682</td>\n",
       "      <td>0.000280</td>\n",
       "      <td>0.004612</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            $        ''         (         )  \\\n",
       "A-Alcott-Little_Women-1868-F.pickle  0.000004  0.015878  0.000118  0.000039   \n",
       "A-Cather-Antonia-1918-F.pickle       0.000000  0.014392  0.000051  0.000000   \n",
       "A-Chesnutt-Marrow-1901-M.pickle      0.000000  0.018660  0.000000  0.000000   \n",
       "A-Chopin-Awakening-1899-F.pickle     0.000000  0.005470  0.000013  0.000000   \n",
       "A-Crane-Maggie-1893-M.pickle         0.000000  0.018938  0.000070  0.000000   \n",
       "\n",
       "                                            ,         .         :        CC  \\\n",
       "A-Alcott-Little_Women-1868-F.pickle  0.077302  0.034055  0.003976  0.043827   \n",
       "A-Cather-Antonia-1918-F.pickle       0.052536  0.047359  0.012598  0.037385   \n",
       "A-Chesnutt-Marrow-1901-M.pickle      0.061304  0.039306  0.008001  0.026679   \n",
       "A-Chopin-Awakening-1899-F.pickle     0.045042  0.052318  0.013518  0.034914   \n",
       "A-Crane-Maggie-1893-M.pickle         0.046577  0.059296  0.007757  0.026591   \n",
       "\n",
       "                                           CD        DT ...        VBD  \\\n",
       "A-Alcott-Little_Women-1868-F.pickle  0.004395  0.068154 ...   0.060621   \n",
       "A-Cather-Antonia-1918-F.pickle       0.005115  0.078603 ...   0.080377   \n",
       "A-Chesnutt-Marrow-1901-M.pickle      0.004998  0.090023 ...   0.053366   \n",
       "A-Chopin-Awakening-1899-F.pickle     0.004868  0.080440 ...   0.075847   \n",
       "A-Crane-Maggie-1893-M.pickle         0.003774  0.087669 ...   0.074251   \n",
       "\n",
       "                                          VBG       VBN       VBP       VBZ  \\\n",
       "A-Alcott-Little_Women-1868-F.pickle  0.016620  0.016327  0.021927  0.009619   \n",
       "A-Cather-Antonia-1918-F.pickle       0.015120  0.017406  0.014290  0.005197   \n",
       "A-Chesnutt-Marrow-1901-M.pickle      0.011067  0.028030  0.016800  0.009561   \n",
       "A-Chopin-Awakening-1899-F.pickle     0.020467  0.021893  0.012144  0.007158   \n",
       "A-Crane-Maggie-1893-M.pickle         0.016842  0.018100  0.011775  0.008002   \n",
       "\n",
       "                                          WDT        WP       WP$       WRB  \\\n",
       "A-Alcott-Little_Women-1868-F.pickle  0.003893  0.004286  0.000157  0.005709   \n",
       "A-Cather-Antonia-1918-F.pickle       0.003250  0.003434  0.000082  0.007237   \n",
       "A-Chesnutt-Marrow-1901-M.pickle      0.006014  0.004617  0.000399  0.004073   \n",
       "A-Chopin-Awakening-1899-F.pickle     0.007394  0.004345  0.000209  0.006478   \n",
       "A-Crane-Maggie-1893-M.pickle         0.002306  0.004682  0.000280  0.004612   \n",
       "\n",
       "                                      ``  \n",
       "A-Alcott-Little_Women-1868-F.pickle  0.0  \n",
       "A-Cather-Antonia-1918-F.pickle       0.0  \n",
       "A-Chesnutt-Marrow-1901-M.pickle      0.0  \n",
       "A-Chopin-Awakening-1899-F.pickle     0.0  \n",
       "A-Crane-Maggie-1893-M.pickle         0.0  \n",
       "\n",
       "[5 rows x 44 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using chi-squared statistic\n",
    "import pandas as pd\n",
    "from sklearn.feature_selection import chi2\n",
    "from collections import Counter\n",
    "\n",
    "dpm = pd.DataFrame(index=sorted(pos_all.keys()))\n",
    "labels = []\n",
    "for nation in ['A', 'B']:\n",
    "    for fileid in pcorpus.fileids(categories=nation):\n",
    "        pos = Counter()\n",
    "        for token in pcorpus.tagged(fileids=fileid):\n",
    "            pos[token[1]] += 1\n",
    "        df = pd.Series(pos, name=fileid)\n",
    "        df.sort_index(inplace=True)\n",
    "        df = df/df.sum()\n",
    "        df = pd.DataFrame(df)\n",
    "        labels.append(nation)\n",
    "        dpm = dpm.join(df)\n",
    "dpm.fillna(0, inplace=True)\n",
    "dpm = dpm.T\n",
    "dpm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most distinctive PoS tags\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chi-sq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>''</th>\n",
       "      <td>0.021468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MD</th>\n",
       "      <td>0.013908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>:</th>\n",
       "      <td>0.007900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RP</th>\n",
       "      <td>0.006745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NN</th>\n",
       "      <td>0.005711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VBD</th>\n",
       "      <td>0.005057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FW</th>\n",
       "      <td>0.004629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VB</th>\n",
       "      <td>0.003920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(</th>\n",
       "      <td>0.003910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>JJ</th>\n",
       "      <td>0.003515</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       chi-sq\n",
       "''   0.021468\n",
       "MD   0.013908\n",
       ":    0.007900\n",
       "RP   0.006745\n",
       "NN   0.005711\n",
       "VBD  0.005057\n",
       "FW   0.004629\n",
       "VB   0.003920\n",
       "(    0.003910\n",
       "JJ   0.003515"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chisq, _ = chi2(dpm, labels)\n",
    "chisq = pd.DataFrame(chisq, index=dpm.columns, columns=['chi-sq'])\n",
    "print(\"Most distinctive PoS tags\")\n",
    "display(chisq.sort_values(by='chi-sq', ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$: dollar\n",
      "    $ -$ --$ A$ C$ HK$ M$ NZ$ S$ U.S.$ US$\n",
      "'': closing quotation mark\n",
      "    ' ''\n",
      "(: opening parenthesis\n",
      "    ( [ {\n",
      "): closing parenthesis\n",
      "    ) ] }\n",
      ",: comma\n",
      "    ,\n",
      "--: dash\n",
      "    --\n",
      ".: sentence terminator\n",
      "    . ! ?\n",
      ":: colon or ellipsis\n",
      "    : ; ...\n",
      "CC: conjunction, coordinating\n",
      "    & 'n and both but either et for less minus neither nor or plus so\n",
      "    therefore times v. versus vs. whether yet\n",
      "CD: numeral, cardinal\n",
      "    mid-1890 nine-thirty forty-two one-tenth ten million 0.5 one forty-\n",
      "    seven 1987 twenty '79 zero two 78-degrees eighty-four IX '60s .025\n",
      "    fifteen 271,124 dozen quintillion DM2,000 ...\n",
      "DT: determiner\n",
      "    all an another any both del each either every half la many much nary\n",
      "    neither no some such that the them these this those\n",
      "EX: existential there\n",
      "    there\n",
      "FW: foreign word\n",
      "    gemeinschaft hund ich jeux habeas Haementeria Herr K'ang-si vous\n",
      "    lutihaw alai je jour objets salutaris fille quibusdam pas trop Monte\n",
      "    terram fiche oui corporis ...\n",
      "IN: preposition or conjunction, subordinating\n",
      "    astride among uppon whether out inside pro despite on by throughout\n",
      "    below within for towards near behind atop around if like until below\n",
      "    next into if beside ...\n",
      "JJ: adjective or numeral, ordinal\n",
      "    third ill-mannered pre-war regrettable oiled calamitous first separable\n",
      "    ectoplasmic battery-powered participatory fourth still-to-be-named\n",
      "    multilingual multi-disciplinary ...\n",
      "JJR: adjective, comparative\n",
      "    bleaker braver breezier briefer brighter brisker broader bumper busier\n",
      "    calmer cheaper choosier cleaner clearer closer colder commoner costlier\n",
      "    cozier creamier crunchier cuter ...\n",
      "JJS: adjective, superlative\n",
      "    calmest cheapest choicest classiest cleanest clearest closest commonest\n",
      "    corniest costliest crassest creepiest crudest cutest darkest deadliest\n",
      "    dearest deepest densest dinkiest ...\n",
      "LS: list item marker\n",
      "    A A. B B. C C. D E F First G H I J K One SP-44001 SP-44002 SP-44005\n",
      "    SP-44007 Second Third Three Two * a b c d first five four one six three\n",
      "    two\n",
      "MD: modal auxiliary\n",
      "    can cannot could couldn't dare may might must need ought shall should\n",
      "    shouldn't will would\n",
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n",
      "NNP: noun, proper, singular\n",
      "    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n",
      "    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n",
      "    Shannon A.K.C. Meltex Liverpool ...\n",
      "NNPS: noun, proper, plural\n",
      "    Americans Americas Amharas Amityvilles Amusements Anarcho-Syndicalists\n",
      "    Andalusians Andes Andruses Angels Animals Anthony Antilles Antiques\n",
      "    Apache Apaches Apocrypha ...\n",
      "NNS: noun, common, plural\n",
      "    undergraduates scotches bric-a-brac products bodyguards facets coasts\n",
      "    divestitures storehouses designs clubs fragrances averages\n",
      "    subjectivists apprehensions muses factory-jobs ...\n",
      "PDT: pre-determiner\n",
      "    all both half many quite such sure this\n",
      "POS: genitive marker\n",
      "    ' 's\n",
      "PRP: pronoun, personal\n",
      "    hers herself him himself hisself it itself me myself one oneself ours\n",
      "    ourselves ownself self she thee theirs them themselves they thou thy us\n",
      "PRP$: pronoun, possessive\n",
      "    her his mine my our ours their thy your\n",
      "RB: adverb\n",
      "    occasionally unabatingly maddeningly adventurously professedly\n",
      "    stirringly prominently technologically magisterially predominately\n",
      "    swiftly fiscally pitilessly ...\n",
      "RBR: adverb, comparative\n",
      "    further gloomier grander graver greater grimmer harder harsher\n",
      "    healthier heavier higher however larger later leaner lengthier less-\n",
      "    perfectly lesser lonelier longer louder lower more ...\n",
      "RBS: adverb, superlative\n",
      "    best biggest bluntest earliest farthest first furthest hardest\n",
      "    heartiest highest largest least less most nearest second tightest worst\n",
      "RP: particle\n",
      "    aboard about across along apart around aside at away back before behind\n",
      "    by crop down ever fast for forth from go high i.e. in into just later\n",
      "    low more off on open out over per pie raising start teeth that through\n",
      "    under unto up up-pp upon whole with you\n",
      "SYM: symbol\n",
      "    % & ' '' ''. ) ). * + ,. < = > @ A[fj] U.S U.S.S.R * ** ***\n",
      "TO: \"to\" as preposition or infinitive marker\n",
      "    to\n",
      "UH: interjection\n",
      "    Goodbye Goody Gosh Wow Jeepers Jee-sus Hubba Hey Kee-reist Oops amen\n",
      "    huh howdy uh dammit whammo shucks heck anyways whodunnit honey golly\n",
      "    man baby diddle hush sonuvabitch ...\n",
      "VB: verb, base form\n",
      "    ask assemble assess assign assume atone attention avoid bake balkanize\n",
      "    bank begin behold believe bend benefit bevel beware bless boil bomb\n",
      "    boost brace break bring broil brush build ...\n",
      "VBD: verb, past tense\n",
      "    dipped pleaded swiped regummed soaked tidied convened halted registered\n",
      "    cushioned exacted snubbed strode aimed adopted belied figgered\n",
      "    speculated wore appreciated contemplated ...\n",
      "VBG: verb, present participle or gerund\n",
      "    telegraphing stirring focusing angering judging stalling lactating\n",
      "    hankerin' alleging veering capping approaching traveling besieging\n",
      "    encrypting interrupting erasing wincing ...\n",
      "VBN: verb, past participle\n",
      "    multihulled dilapidated aerosolized chaired languished panelized used\n",
      "    experimented flourished imitated reunifed factored condensed sheared\n",
      "    unsettled primed dubbed desired ...\n",
      "VBP: verb, present tense, not 3rd person singular\n",
      "    predominate wrap resort sue twist spill cure lengthen brush terminate\n",
      "    appear tend stray glisten obtain comprise detest tease attract\n",
      "    emphasize mold postpone sever return wag ...\n",
      "VBZ: verb, present tense, 3rd person singular\n",
      "    bases reconstructs marks mixes displeases seals carps weaves snatches\n",
      "    slumps stretches authorizes smolders pictures emerges stockpiles\n",
      "    seduces fizzes uses bolsters slaps speaks pleads ...\n",
      "WDT: WH-determiner\n",
      "    that what whatever which whichever\n",
      "WP: WH-pronoun\n",
      "    that what whatever whatsoever which who whom whosoever\n",
      "WP$: WH-pronoun, possessive\n",
      "    whose\n",
      "WRB: Wh-adverb\n",
      "    how however whence whenever where whereby whereever wherein whereof why\n",
      "``: opening quotation mark\n",
      "    ` ``\n"
     ]
    }
   ],
   "source": [
    "nltk.help.upenn_tagset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the list of over-represented parts of speech. Do you feel you've found a good metric? Why or why not? What else might you try? Enter a sentence or two of reflection here ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 lowercased, non-stopword lemmas in the BF corpus:\n",
      "say \t 9625\n",
      "mr \t 7660\n",
      "-- \t 6979\n",
      "would \t 6072\n",
      ".\" \t 5072\n",
      "go \t 4789\n",
      "could \t 4430\n",
      "one \t 4185\n",
      "know \t 4026\n",
      "make \t 3922\n",
      "come \t 3902\n",
      "think \t 3884\n",
      "see \t 3532\n",
      ",\" \t 3520\n",
      "look \t 3383\n",
      ".' \t 3146\n",
      "take \t 2873\n",
      "like \t 2751\n",
      "good \t 2673\n",
      "time \t 2593\n"
     ]
    }
   ],
   "source": [
    "# 4. Top 20 lemmatized, lowercase, non-punctuation, non-stopwords in BF corpus\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet, stopwords\n",
    "import string\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop = set(stopwords.words('english')).union(set(string.punctuation))\n",
    "\n",
    "def wordnet_tag(penn_tag):\n",
    "    \"\"\"\n",
    "    For an input Penn Treebank PoS tag, return a WordNet PoS tag or None.\n",
    "    \"\"\"\n",
    "    if penn_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif penn_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif penn_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif penn_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None # input tag is other or not recognized\n",
    "\n",
    "def get_lemma(token, pos):\n",
    "    '''\n",
    "    Return the lemmatized version of an English word.\n",
    "    '''\n",
    "    pos = wordnet_tag(pos)\n",
    "    if pos:\n",
    "        lemma = lemmatizer.lemmatize(token, pos)\n",
    "    else:\n",
    "        lemma = lemmatizer.lemmatize(token)\n",
    "    return lemma\n",
    "\n",
    "lemmas_bf = Counter()\n",
    "for token in pcorpus.tagged(categories='BF'):\n",
    "    word = token[0].lower()\n",
    "    pos = token[1]\n",
    "    if word not in stop:\n",
    "        lemma = get_lemma(word, pos)\n",
    "        lemmas_bf[lemma] += 1\n",
    "        \n",
    "print(\"Top 20 lowercased, non-stopword lemmas in the BF corpus:\")\n",
    "for lem in lemmas_bf.most_common(n=20):\n",
    "    print(lem[0], '\\t', lem[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exmine this list. Are there other words or symbols we might want to add to our list of stopwords? No need to write this up, but we'll discuss it in class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
